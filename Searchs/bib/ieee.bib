@INPROCEEDINGS{8342497, 
author={S. Kalra and S. Jain and A. Agarwal}, 
booktitle={2017 6th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)}, 
title={Fixed DO Solfège based object detection and positional analysis for the visually impaired}, 
year={2017}, 
volume={}, 
number={}, 
pages={594-598}, 
abstract={This paper proposes a novel approach towards object detection and its subsequent positional analysis for a visually impaired subject, using Fixed DO Solfège, basically sounds that we have heard multiple times, and we are aware of them and also their sequence is thus naturally perceived by us viz. DO-RE-MI-FA-SO-LA-TI This approach utilizes a concept of virtual zones superimposed on the viewport of a small wearable CMOS camera module mounted on the eyeglasses of the subject, communicating to a computing device with attached earplugs for sonic feedbacks in the Fixed DO Solfège notation. By implantation of a HAAR cascade based classifier trained system, several need to know objects are trained and fed into the recognition pool, which are thus detected in the viewport overlaid by the virtual harmonic zones further linked to the Fixed DO Solfège notations, and are mapped throughout the viewport in an incremental/sequential manner. As the subject moves his hand through the viewport, and his hands overlap the recognized objects, an audible beep basis on the sound of the zone is played on the earplugs. This not only enables the subject to know that in which direction a particular object is situated, but also, because of the sound, he can also know as to how many objects lie before/after the current object, this gives a sense of relative recognition and positional cognition of the objects.}, 
keywords={assisted living;cameras;hearing;image classification;object detection;object recognition;positional cognition;virtual zones;wearable CMOS camera;virtual harmonic zones;visually impaired;fixed DO Solfege based object detection and positional analysis;HAAR cascade based classifier trained system;Harmonic analysis;Object detection;Cameras;Object recognition;Cognition;Feature extraction;Reliability;object detection;HAAR transformation;positional analysis;CMOS camera;visually impaired}, 
doi={10.1109/ICRITO.2017.8342497}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7430522, 
author={W. C. S. S. Simões and V. F. de Lucena}, 
booktitle={2016 IEEE International Conference on Consumer Electronics (ICCE)}, 
title={Blind user wearable audio assistance for indoor navigation based on visual markers and ultrasonic obstacle detection}, 
year={2016}, 
volume={}, 
number={}, 
pages={60-63}, 
abstract={This paper presents an indoor navigation wearable system based on visual markers recognition and ultrasonic obstacles perception used as an audio assistance for blind people. In this prototype, visual markers identify the points of interest in the environment; additionally this location status is enriched with information obtained in real time by other sensors. A map lists these points and indicates the distance and direction between closer points, building a virtual path. The blind users wear also glasses built with sensors like RGB camera, ultrasonic, magnetometer, gyroscope, and accelerometer enhancing the amount and quality of the available information. The user navigates freely in the prepared environment identifying the location markers. Based on the origin point information or the location point information and on the gyro sensor value the path to next marker (target) is calculated. To raise the perception of the environment, avoiding possible obstacles, it is used a couple of ultrasonic sensors. The audio assistance provided to the user makes use of an audio bank, with simple known instructions to indicate precisely the desired route and obstacles. Ten blind users tested and evaluated the system. The results showed rates of about 94.92% successful recognition of the markers using only 26 frames per second and 98.33% of ultrasonic obstacles perception disposed between 0.50 meters and 4.0 meters.}, 
keywords={accelerometers;biomedical ultrasonics;cameras;glass;gyroscopes;indoor navigation;magnetometers;object detection;visual perception;wearable computers;blind user wearable audio assistance;ultrasonic obstacle detection;indoor navigation wearable system;visual markers recognition;ultrasonic obstacles perception;blind people;virtual path;RGB camera;magnetometer;gyroscope;accelerometer;origin point information;location point information;gyrosensor value;ultrasonic sensors;audio bank;Navigation;Acoustics;Visualization;Magnetic sensors;Glass;Cameras}, 
doi={10.1109/ICCE.2016.7430522}, 
ISSN={2158-4001}, 
month={Jan},}
@INPROCEEDINGS{6229180, 
author={A. Dionisi and E. Sardini and M. Serpelloni}, 
booktitle={2012 IEEE International Instrumentation and Measurement Technology Conference Proceedings}, 
title={Wearable object detection system for the blind}, 
year={2012}, 
volume={}, 
number={}, 
pages={1255-1258}, 
abstract={The blind's capacities to navigate in a particular place and to organize their daily activities are of vital importance for their health and well-being. Organizing any kind of simple daily activity can be especially difficult; it is not easy for the blind to distinguish the different items, such as packaged foods and drug containers just by touching with their hands. RFID, or radio frequency identification, is a technology that can provide a support for improving the organization and orientation during the daylight activities. RFID uses radio waves to deliver data from a tag, which stores information, to a reader, which can elaborate the information making decisions. This technology is very useful in many different contexts such as scanning passports, shipments and automatic highway toll collecting. As the RFID technology stands out for its inherent technical nature of any basic RFID system, it may involve additional improvements for numerous applications in the field of health care. In this paper, a RFID device designed as a support for the blind in searching some objects is presented; in particular, it has been develop for searching the medicines in a cabinet at home. The device is able to provide to the blind some pieces of information about the distance and simplify the search; besides identifying the medicines, the device is able to provide the user an acoustic signal in order to find easily the desired product as soon as possible. It is noteworthy the fact that it gives the blind some items of information about the distance of a defined object, that is how near or far it is. This application is obtained using the RSSI (Received Strength Signal Indicator) value, measuring the power of the received signal of the tag.}, 
keywords={acoustic devices;handicapped aids;object detection;radiofrequency identification;wearable object detection system;blind;RFID device;acoustic signal;defined object distance;received strength signal indicator;Radiofrequency identification;Antennas;Containers;Microcontrollers;Antenna measurements;Object recognition;Bluetooth;wearable system;health care;movement monitoring;biomedical devices;system health;RFID;RSSI}, 
doi={10.1109/I2MTC.2012.6229180}, 
ISSN={1091-5281}, 
month={May},}
@INPROCEEDINGS{6618345, 
author={K. A. Thakoor and S. Marat and P. J. Nasiatka and B. P. McIntosh and F. E. Sahin and A. R. Tanguay and J. D. Weiland and L. Itti}, 
booktitle={2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
title={Attention biased speeded up robust featureS (AB-SURF): A neurally-inspired object recognition algorithm for a wearable aid for the visually-impaired}, 
year={2013}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Humans recognize objects effortlessly, in spite of changes in scale, position, and illumination. Emulating human recognition in machines remains a challenge. This paper describes computer vision algorithms aimed at helping visually-impaired people locate and recognize objects. Our neurally-inspired computer vision algorithm, called Attention Biased Speeded Up Robust Features (AB-SURF), harnesses features that characterize human visual attention to make the recognition task more tractable. An attention biasing algorithm selects the most task-driven salient regions in an image. Next, the SURF object recognition algorithm is applied on this narrowed subsection of the original image. Testing on images containing 5 different objects exhibits accuracies ranging from 80% to 100%. Furthermore, testing on images containing 10 objects yields accuracies between 63% and 96% for the 5 objects that occupy the largest area within the image subwindows chosen by attention biasing. A five-fold speed-up is attained using AB-SURF as compared to the time estimated for sliding window recognition on the same images.}, 
keywords={computer vision;handicapped aids;object recognition;wearable computers;attention biased speeded up robust features;AB-SURF;neurally-inspired object recognition algorithm;wearable aid;human recognition emulation;computer vision algorithms;visually-impaired people;neurally-inspired computer vision algorithm;attention biasing algorithm;task-driven salient regions;image subwindows;five-fold speed-up;sliding window recognition;human-visual-attention-inspired features;Object recognition;Image recognition;Accuracy;Testing;Training;Visualization;Cameras;Object recognition;visual attention;neurally-inspired computer vision;visual aids for the blind}, 
doi={10.1109/ICMEW.2013.6618345}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{6908495, 
author={K. Yelamarthi and B. P. DeJong and K. Laubhan}, 
booktitle={2014 IEEE 57th International Midwest Symposium on Circuits and Systems (MWSCAS)}, 
title={A Kinect based vibrotactile feedback system to assist the visually impaired}, 
year={2014}, 
volume={}, 
number={}, 
pages={635-638}, 
abstract={This paper presents a Microsoft Kinect based vibrotactile feedback system to aid in navigation for the visually impaired. The lightweight wearable system interprets the visual scene and presents obstacle distance and characteristic information to the user. The scene is converted into a distance map using the Kinect, then processed and interpreted using an Intel Next Unit of Computing (NUC). That information is then converted via a microcontroller into vibrotactile feedback, presented to the user through two four-by-four vibration motor arrays woven into gloves. The system is shown to successfully identify, track, and present closest objects, closest humans, multiple humans, and perform distance measurements.}, 
keywords={feedback;handicapped aids;haptic interfaces;navigation;wearable computers;visually impaired assistance;Microsoft Kinect;vibrotactile feedback system;navigation;lightweight wearable system;obstacle distance;Intel Next Unit of Computing;NUC;Vibrations;Microcontrollers;Object recognition;Visualization;Sonar navigation;Assistive devices;blind;kinect sensor;navigation assistance;tactile feedback;visually impaired}, 
doi={10.1109/MWSCAS.2014.6908495}, 
ISSN={1548-3746}, 
month={Aug},}
@ARTICLE{5165018, 
author={D. Dakopoulos and N. G. Bourbakis}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
title={Wearable Obstacle Avoidance Electronic Travel Aids for Blind: A Survey}, 
year={2010}, 
volume={40}, 
number={1}, 
pages={25-35}, 
abstract={The last decades a variety of portable or wearable navigation systems have been developed to assist visually impaired people during navigation in known or unknown, indoor or outdoor environments. There are three main categories of these systems: electronic travel aids (ETAs), electronic orientation aids (EOAs), and position locator devices (PLDs). This paper presents a comparative survey among portable/wearable obstacle detection/avoidance systems (a subcategory of ETAs) in an effort to inform the research community and users about the capabilities of these systems and about the progress in assistive technology for visually impaired people. The survey is based on various features and performance parameters of the systems that classify them in categories, giving qualitative-quantitative measures. Finally, it offers a ranking, which will serve only as a reference point and not as a critique on these systems.}, 
keywords={collision avoidance;computerised instrumentation;handicapped aids;wearable computers;wearable obstacle avoidance;electronic travel aids;wearable navigation systems;visually impaired people;blind people;electronic orientation aids;position locator devices;wearable obstacle detection systems;Navigation;Time of arrival estimation;Cameras;Displays;Feedback amplifiers;State estimation;Eyes;Automation;Computer science;Object detection;Electronic travel aids;navigation systems;obstacle avoidance;survey;wearable systems}, 
doi={10.1109/TSMCC.2009.2021255}, 
ISSN={1094-6977}, 
month={Jan},}
@INPROCEEDINGS{5277757, 
author={H. Goto and M. Tanaka}, 
booktitle={2009 10th International Conference on Document Analysis and Recognition}, 
title={Text-Tracking Wearable Camera System for the Blind}, 
year={2009}, 
volume={}, 
number={}, 
pages={141-145}, 
abstract={Disability of visual text reading has a huge impact on the quality of life for visually disabled people.One of the most anticipated devices is a wearable camera capable of finding text regions in natural scenes and translating the text into another representation such as speech or braille.In order to develop such a device,text tracking in video sequences is required as well as text detection.The device needs to group homogeneous text regions to avoid multiple and redundant speech syntheses or braille conversions.An automatic text image selection is also required for better character recognition and timely text message presentation.We have developed a prototype system equipped with a head-mounted video camera.Particle filter is employed for fast and robust text tracking.We have tested the performance of our system using 1,730 video frames of hall ways with 27 signboards.The number of text candidate regions is reduced to 1.47%.}, 
keywords={handicapped aids;image sequences;object detection;speech synthesis;text analysis;text-tracking wearable camera system;blind;visual text reading disability;visually disabled people;text tracking;video sequences;text detection;speech syntheses;braille conversions;automatic text image selection;Cameras;Speech synthesis;Layout;Video sequences;Image converters;Character recognition;Prototypes;Filters;Robustness;Automatic testing;scene text;text tracking;wearable camera;text detection}, 
doi={10.1109/ICDAR.2009.102}, 
ISSN={1520-5363}, 
month={July},}
@INPROCEEDINGS{4145848, 
author={K. Karacs and T. Roska}, 
booktitle={2006 10th International Workshop on Cellular Neural Networks and Their Applications}, 
title={Route number recognition ot Public Transport Vehicles via the Bionic Eyeglass}, 
year={2006}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={In spite of the impressive advances related to retinal prostheses, there is no imminent promise to make them soon available with a realistic performance to help navigating blind persons. In our new project, we are designing a bionic eyeglass that is providing a wearable TeraOps visual computing power to guide visually impaired people in their daily life. Detection and recognition of signs and displays in real, noisy environments is a key element in many functions of the bionic eyeglass. This paper describes spatial-temporal analogic cellular algorithms used for localizing signs and displays, and recognition of numbers they contain}, 
keywords={biocybernetics;cellular neural nets;computer vision;handicapped aids;object recognition;vehicles;wearable computers;route number recognition;public transport vehicles;bionic eyeglass;retinal prostheses;wearable TeraOps visual computing;sign detection;sign recognition;spatial-temporal analogic cellular algorithms;sign localization;displays localization;machine vision;object recognition;cellular neural networks;Vehicles;Pattern recognition;Cameras;Displays;Lighting control;Cellular neural networks;Computer networks;Navigation;Wearable computers;Computational modeling;cellular neural networks;machine vision;object recognition}, 
doi={10.1109/CNNA.2006.341608}, 
ISSN={2165-0144}, 
month={Aug},}
@INPROCEEDINGS{8265387, 
author={L. Niu and C. Qian and J. Rizzo and T. Hudson and Z. Li and S. Enright and E. Sperling and K. Conti and E. Wong and Y. Fang}, 
booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)}, 
title={A Wearable Assistive Technology for the Visually Impaired with Door Knob Detection and Real-Time Feedback for Hand-to-Handle Manipulation}, 
year={2017}, 
volume={}, 
number={}, 
pages={1500-1508}, 
abstract={The visually impaired are consistently faced with mobility restrictions due to the lack of truly accessible environments. Even in structured settings, people with low vision may still have trouble navigating efficiently and safely due to hallway and threshold ambiguity. Assistive technologies that are currently available do not provide door and door-handle object detections nor do they concretely help the visually impaired reaching towards the object. In this paper, we propose an AI-driven wearable assistive technology that integrates door handle detection, user's real-time hand position in relation to this targeted object, and audio feedback for "joy stick-like command" for acquisition of the target and subsequent hand-to-handle manipulation. When fully envisioned, this platform will help end users locate doors and door handles and reach them with feedback, enabling them to travel safely and efficiently when navigating through environments with thresholds. Compared to the usual computer vision models, the one proposed in this paper requires significantly fewer computational resources, which allows it to pair with a stereoscopic camera running on a small graphics processing unit (GPU). This permits us to take advantage of its convenient portability. We also introduce a dataset containing different types of door handles and door knobs with bounding-box annotations, which can be used for training and testing in future research.}, 
keywords={cameras;computer vision;handicapped aids;learning (artificial intelligence);navigation;object detection;wearable computers;stereoscopic camera;graphics processing unit;GPU;audio feedback;real-time hand position;door handle detection;AI-driven wearable assistive technology;visually impaired reaching;door-handle object detections;assistive technologies;threshold ambiguity;hallway;low vision;structured settings;mobility restrictions;real-time feedback;door knob detection;door knobs;usual computer vision models;door handles;doors;hand-to-handle manipulation;joy stick-like command;Real-time systems;Object detection;Cameras;Neural networks;Visualization;Graphics processing units;Three-dimensional displays}, 
doi={10.1109/ICCVW.2017.177}, 
ISSN={2473-9944}, 
month={Oct},}
@ARTICLE{6819807, 
author={A. Aladrén and G. López-Nicolás and L. Puig and J. J. Guerrero}, 
journal={IEEE Systems Journal}, 
title={Navigation Assistance for the Visually Impaired Using RGB-D Sensor With Range Expansion}, 
year={2016}, 
volume={10}, 
number={3}, 
pages={922-932}, 
abstract={Navigation assistance for visually impaired (NAVI) refers to systems that are able to assist or guide people with vision loss, ranging from partially sighted to totally blind, by means of sound commands. In this paper, a new system for NAVI is presented based on visual and range information. Instead of using several sensors, we choose one device, a consumer RGB-D camera, and take advantage of both range and visual information. In particular, the main contribution is the combination of depth information with image intensities, resulting in the robust expansion of the range-based floor segmentation. On one hand, depth information, which is reliable but limited to a short range, is enhanced with the long-range visual information. On the other hand, the difficult and prone-to-error image processing is eased and improved with depth information. The proposed system detects and classifies the main structural elements of the scene providing the user with obstacle-free paths in order to navigate safely across unknown scenarios. The proposed system has been tested on a wide variety of scenarios and data sets, giving successful results and showing that the system is robust and works in challenging indoor environments.}, 
keywords={handicapped aids;image classification;image segmentation;object detection;blind people;wearable system;indoor environments;navigation safety;obstacle-free paths;element classification;element detection;image processing;long-range visual information;range-based floor segmentation;image intensities;depth information;RGB-D camera;range information;sound commands;vision loss;NAVI systems;range expansion;RGB-D sensor;visually impaired people;navigation assistance;Image segmentation;Image color analysis;Robot sensing systems;Navigation;Cameras;Robustness;Visualization;Navigation assistance for visually impaired (NAVI);range and vision;RGB-D camera;visually impaired assistance;wearable system}, 
doi={10.1109/JSYST.2014.2320639}, 
ISSN={1932-8184}, 
month={Sept},}
@INPROCEEDINGS{5959608, 
author={T. Kurata and M. Kourogi and T. Ishikawa and Y. Kameda and K. Aoki and J. Ishikawa}, 
booktitle={2011 15th Annual International Symposium on Wearable Computers}, 
title={Indoor-Outdoor Navigation System for Visually-Impaired Pedestrians: Preliminary Evaluation of Position Measurement and Obstacle Display}, 
year={2011}, 
volume={}, 
number={}, 
pages={123-124}, 
abstract={This paper reports on a preliminary evaluation of position measurement and obstacle display in a navigation system for visually-impaired pedestrians on a real-city course including indoor and outdoor areas.}, 
keywords={haptic interfaces;navigation;object detection;position measurement;indoor-outdoor navigation system;visually-impaired pedestrians;position measurement;obstacle display;real-city course;Legged locomotion;Trajectory;Position measurement;Global Positioning System;Accuracy;IEEE 802.11 Standards}, 
doi={10.1109/ISWC.2011.30}, 
ISSN={2376-8541}, 
month={June},}
@INPROCEEDINGS{8107095, 
author={P. Herghelegiu and A. Burlacu and S. Caraiman}, 
booktitle={2017 21st International Conference on System Theory, Control and Computing (ICSTCC)}, 
title={Negative obstacle detection for wearable assistive devices for visually impaired}, 
year={2017}, 
volume={}, 
number={}, 
pages={564-570}, 
abstract={The research on developing assistive devices for visually impaired people greatly relies on image processing techniques. Extensive effort is made on detecting the obstacles in front of the user. Beside the normal obstacles that the user can bump into, another category of obstacles, negative obstacles must also be detected. Negative obstacles are usually represented by holes in the ground or regions that lay bellow the ground surface. Such obstacles represent a potential danger to the visually impaired user and could lead to serious injury if not detected. In this paper, we introduce a stereo vision system to identify and track negative obstacles located in front of the user. The identification is performed in the disparity image based on an estimation of the ground surface in the stereo images. The tracking relies on a camera motion estimation approach. The algorithm we introduce in this paper was integrated and tested with a wearable assistive device to prove its efficiency. We evaluate the accuracy of our solution using various real-life scenarios.}, 
keywords={cameras;handicapped aids;motion estimation;object detection;object tracking;stereo image processing;negative obstacle detection;wearable assistive device;visually impaired people;image processing techniques;stereo vision system;camera motion estimation;Three-dimensional displays;Assistive devices;Image edge detection;Cameras;Stereo vision;Navigation;Haptic interfaces;visually impaired users;assistive devices;negative obstacles}, 
doi={10.1109/ICSTCC.2017.8107095}, 
ISSN={}, 
month={Oct},}
@INPROCEEDINGS{8288947, 
author={M. M. Kamal and A. I. Bayazid and M. S. Sadi and M. M. Islam and N. Hasan}, 
booktitle={2017 IEEE Region 10 Humanitarian Technology Conference (R10-HTC)}, 
title={Towards developing walking assistants for the visually impaired people}, 
year={2017}, 
volume={}, 
number={}, 
pages={238-241}, 
abstract={Blindness is the condition of lacking discernment because of physiological or neurological components. Different techniques have been established to define the degree of vision misfortune and visual deficiency. The problems that visually impaired people are facing are that they cannot avoid the objects around them while moving, so accident may occur. Various researches have been conducted to minimize the sufferings of the visually impaired people. But it does not reach to the optimized level. The contribution of this paper is to investigate a solution for the visually impaired people. This paper targets to provide a solution that helps blind to avoid obstacles around them without holding any sticks or other heavy things. The system calculated the smoothness of the surface both for the daylight and dark using RGB data through microcontroller and smartphone. The highest smoothness of the surface is achieved by the system is 96.341% and 98.683% for the day-night and dark respectively. It designs and develops a system which is consisting of small, wearable, lightweight, low-cost spectacle for the blind by which they can get assistance to walk easily and detect obstacles around them.}, 
keywords={assisted living;handicapped aids;image colour analysis;microcontrollers;object detection;sensory aids;smart phones;vision defects;visually impaired people;visual deficiency;walking assistants;blindness;vision misfortune;obstacle avoidance;RGB data;microcontroller;smartphone;surface smoothness;small wearable lightweight low-cost spectacle;obstacle detection;Sensors;Surface treatment;Microcontrollers;Sonar;Cameras;Lasers;Bluetooth;Blindness;Visually Impaired People;Walking Assistants;Object Detection;Surface Smoothness Detection}, 
doi={10.1109/R10-HTC.2017.8288947}, 
ISSN={2572-7621}, 
month={Dec},}
@INPROCEEDINGS{6732513, 
author={L. Tian and Y. Tian and C. Yi}, 
booktitle={2013 IEEE International Conference on Bioinformatics and Biomedicine}, 
title={Detecting good quality frames in videos captured by a wearable camera for blind navigation}, 
year={2013}, 
volume={}, 
number={}, 
pages={334-337}, 
abstract={Recent technology developments in computer vision, digital cameras, and portable computers make it possible to assist blind individuals by developing camera-based object recognition products. However, motion blur caused by a moving camera limits the real-world application of wayfinding for blind users. In this paper, we propose a new method to detect good quality frames from videos captured by cameras, which are taken by blind users. In our proposed method, both gradient and intensity statistics are extracted from video frames. Then a support vector machine (SVM) based classifier is applied to identify the frames with good quality (Unblurred) from those blurred frames. The Unblurred frames will be further processed to extract essential information for blind wayfinding and navigation such as signage recognition and text extraction. Experimental results demonstrate that our proposed method is able to robustly handle video motions in both indoor and outdoor environments.}, 
keywords={feature extraction;handicapped aids;image classification;image motion analysis;medical image processing;navigation;statistical analysis;support vector machines;text detection;video cameras;good quality frame detection;video capture;wearable camera;blind navigation;computer vision technology developments;digital camera technology developments;portable computer technology developments;blind individual assistance;camera-based object recognition product development;motion blur;camera movement effect;real-world application;gradient statistics extraction;intensity statistics extraction;support vector machine;SVM based classifier;unblurred video frame classification;unblurred frame processing;essential information extraction;blind wayfinding;signage recognition;text extraction;video motions;indoor environments;outdoor environments;Image edge detection;Videos;Feature extraction;Cameras;Navigation;Image quality;Data mining;Blind;Navigation;Wayfinding;Motion blur;Video quality}, 
doi={10.1109/BIBM.2013.6732513}, 
ISSN={}, 
month={Dec},}
@INPROCEEDINGS{6389214, 
author={L. Dunai and B. D. Garcia and I. Lengua and G. Peris-Fajarnés}, 
booktitle={IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society}, 
title={3D CMOS sensor based acoustic object detection and navigation system for blind people}, 
year={2012}, 
volume={}, 
number={}, 
pages={4208-4215}, 
abstract={The paper presents a new wearable Cognitive Aid System for Blind People (CASBliP). The prototype device was developed as an obstacle detector, orientation and navigation Electronic Travel Aid (ETA) for blind people. The device provides a binaural acoustic image representation. The environmental information acquisition system is based on an array of 1×64 CMOS Time-of-Flight sensors. Through stereoscopic (binaural) acoustic sounds the device relays the surrounding near and far environment. Experimental results demonstrate that blind users are able to detect obstacles and navigate through unknown and known environments safety and confidently. CASBliP works accurately in range of 15m in distance and 64° in azimuth, providing significant advantages in comparison with currently existing ETA systems.}, 
keywords={acoustic transducers;biomedical transducers;CMOS image sensors;handicapped aids;image representation;object detection;stereo image processing;visual perception;acoustic object detection;navigation system;blind people;wearable cognitive aid system for blind people;CASBIiP;obstacle detector;electronic travel aid;ETA;binaural acoustic image representation;environmental information acquisition system;3D CMOS time-of-flight sensor;stereoscopic binaural acoustic sound;relay device;Navigation;Arrays;Safety;Acoustics;Lighting;Laser applications;Power lasers}, 
doi={10.1109/IECON.2012.6389214}, 
ISSN={1553-572X}, 
month={Oct},}
@INPROCEEDINGS{7582990, 
author={P. Costa and H. Fernandes and J. Barroso and H. Paredes and L. J. Hadjileontiadis}, 
booktitle={2016 World Automation Congress (WAC)}, 
title={Obstacle detection and avoidance module for the blind}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={Assistive technology enables people to achieve independence when performing daily tasks and it enhances their overall quality of life. Visual information is the basis for most navigational tasks, so visually impaired individuals are at disadvantage due to the lack of sufficient information about their surrounding environment. With recent advances in inclusive technology it is possible to extend the support given to people with visual disabilities in terms of their mobility. In this context we present and describe a wearable system (Blavigator project), whose global objective is to assist visually impaired people in their navigation on indoor and outdoor environments. This paper is focused mainly on the Computer Vision module of the Blavigator prototype. We propose an object collision detection algorithm based on stereo vision. The proposed algorithm uses Peano-Hilbert Ensemble Empirical Mode Decomposition (PH-EEMD) for disparity image processing and a two layer disparity image segmentation to detect nearby objects. Using the adaptive ensemble empirical mode decomposition (EEMD) image analysis real time is not achieved, with PH-EEMD results on a fast implementation suitable for real time applications.}, 
keywords={assisted living;collision avoidance;computer vision;handicapped aids;image segmentation;indoor navigation;object detection;stereo image processing;wearable computers;obstacle detection;obstacle avoidance module;blind;assistive technology;visual information;navigational tasks;visually impaired individuals;visual disabilities;mobility;wearable system;Blavigator project;visually impaired people assistance;indoor navigation;outdoor navigation;computer vision module;object collision detection algorithm;stereo vision;Peano-Hilbert ensemble empirical mode decomposition;PH-EEMD;disparity image processing;two layer disparity image segmentation;Computer vision;Empirical mode decomposition;Two dimensional displays;Global Positioning System;Interpolation;Visualization;Assistive technologies;Obstacle detection;Stereo vision;Empirical Mode Decomposition;Peano Hilbert curves}, 
doi={10.1109/WAC.2016.7582990}, 
ISSN={}, 
month={July},}
@INPROCEEDINGS{5012244, 
author={Sehyung Park and Laehyun Kim and Sungdo Ha and Hyunchul Cho and Soo Yong Lee}, 
booktitle={2009 Digest of Technical Papers International Conference on Consumer Electronics}, 
title={An electronic aid for a visually impaired person using an ultrasonic sensor}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-2}, 
abstract={This paper presents an electronic device, called SmartWand, which can help a visually impaired person walk around more safely. Attached to a conventional white cane, the SmartWand detects obstacles that cannot be detected by a traditional cane and gives warnings in the forms of vibration or sound. In addition to obstacle detection, it also provides functions to detect colors of objects and the brightness of surroundings.}, 
keywords={handicapped aids;sensors;ultrasonic devices;electronic aid;visually impaired person;ultrasonic sensor;SmartWand;electronic device;obstacle detection;Object detection;Brightness;Switches;Wearable sensors;Sensor systems;Acoustic sensors;Motion detection;Batteries;Color;Power supplies}, 
doi={10.1109/ICCE.2009.5012244}, 
ISSN={2158-3994}, 
month={Jan},}
@INPROCEEDINGS{6850903, 
author={L. D. Dunai and I. L. Lengua and I. Tortajada and F. B. Simon}, 
booktitle={2014 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)}, 
title={Obstacle detectors for visually impaired people}, 
year={2014}, 
volume={}, 
number={}, 
pages={809-816}, 
abstract={This paper carries out a review on Electronic Travel Aid Systems (ETAS) for visually impaired people and describes a new wearable Cognitive Aid System for Blind People (CASBliP) developed within the frame of European CASBliP project, in which the authors are taking part. Information on the environment enables humans and vertebrates to know about sources that are in many different directions, particularly signals that are outside the detection range of other senses. Sound source localization is inherently important for safety-survival and navigation. In addition to the acoustical cues, the visual cues such as object detection, tracking and distance measurement play an important role in the navigation not only for robots, but also for blind people, since they are often dependent on artificial intelligence. Due to the fact that blind people make maximum use of sound not only to know the obstacle presence, but also how dangerous it is, in order to avoid it effectively, the CASBliP devices use acoustical sounds in order to represent the visual information detected by the sensors and artificial vision systems.}, 
keywords={acoustic signal detection;artificial intelligence;computer vision;handicapped aids;wearable computers;obstacle detectors;visually impaired people;electronic travel aid systems;ETAS;wearable cognitive aid system for blind people;European CASBliP project;sound source localization;acoustical cues;visual cues;object detection;object tracking;distance measurement;artificial intelligence;acoustical sounds;sensors;artificial vision systems;Global Positioning System;Acoustics;Cameras;Sensor systems;Prototypes}, 
doi={10.1109/OPTIM.2014.6850903}, 
ISSN={1842-0133}, 
month={May},}
@ARTICLE{7445819, 
author={D. J. Brown and M. J. Proulx}, 
journal={IEEE Journal of Selected Topics in Signal Processing}, 
title={Audio–Vision Substitution for Blind Individuals: Addressing Human Information Processing Capacity Limitations}, 
year={2016}, 
volume={10}, 
number={5}, 
pages={924-931}, 
abstract={In this contribution, we consider the factors that influence the information processing capacity of the person using sensory substitution devices, and the influence of how the translated information, here in audio, impacts performance. First, we review aspects of vision substitution by tactile and audio devices, and then we review key theory in human information processing limitations to devise and test use of an audio-vision substitution device, The vOICe, for recognizing visual objects with audio substitution for vision. Participants heard sonifications of two-dimensional (2-D) images and had to match them to alternatives presented either in visual or tactile modalities. To assess whether capacity limits constrain performance, objects were either presented with all information simultaneously (top and bottom as whole objects), or successively (top and bottom of the object one after the other). Performance was superior in the successive trials, indicative of a capacity limit in processing the auditory information. We discuss the implications for training protocols and design to provide a useful accessibility device for blind individuals.}, 
keywords={assisted living;audio signal processing;computer vision;handicapped aids;haptic interfaces;object recognition;wearable computers;audio-vision substitution device;blind individuals;human information processing capacity limitations;sensory substitution devices;tactile devices;vOICe;visual objects recognition;sonifications;2D images;visual modalities;tactile modalities;assistive technology;wearable computer;Visualization;Shape;Performance evaluation;Training;Object recognition;Information processing;Signal processing;Assistive Technology;Blindness;Wearable Computers;Sensory Substitution}, 
doi={10.1109/JSTSP.2016.2543678}, 
ISSN={1932-4553}, 
month={Aug},}
@ARTICLE{7786838, 
author={S. Advani and P. Zientara and N. Shukla and I. Okafor and K. Irick and J. Sampson and S. Datta and V. Narayanan}, 
journal={IEEE Consumer Electronics Magazine}, 
title={A Multitask Grocery Assist System for the Visually Impaired: Smart glasses, gloves, and shopping carts provide auditory and tactile feedback}, 
year={2017}, 
volume={6}, 
number={1}, 
pages={73-81}, 
abstract={According to the World Health Organization, "285 million people are estimated to be visually impaired worldwide" [1]. Several technologies such as automatic text readers, Braille note makers, and navigation assistance canes have been developed to assist the visually impaired. Concurrent advances in computer vision and hardware technologies provide opportunities for a visual-assistance system that can be used in multiple contexts. As part of the Visual Cortex on Silicon program, we have been developing interfaces, algorithms, and hardware platforms to assist the visually impaired with a focus on grocery shopping. This article describes the various features that we have incorporated into this visual-assistance system so that it can be used in multiple contexts.}, 
keywords={assisted living;computer vision;handicapped aids;grocery shopping;silicon program;visual cortex;visual-assistance system;computer vision;navigation assistance canes;Braille note makers;automatic text readers;world health organization;tactile feedback;auditory feedback;shopping carts;gloves;smart glasses;visually impaired;multitask grocery assist system;Visualization;Haptic interfaces;Assistive technologies;Image edge detection;Object recognition;Computer vision}, 
doi={10.1109/MCE.2016.2614422}, 
ISSN={2162-2248}, 
month={Jan},}
@INPROCEEDINGS{7523715, 
author={C. Ramer and T. Lichtenegger and J. Sessner and M. Landgraf and J. Franke}, 
booktitle={2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)}, 
title={An adaptive, color based lane detection of a wearable jogging navigation system for visually impaired on less structured paths}, 
year={2016}, 
volume={}, 
number={}, 
pages={741-746}, 
abstract={Based on a previously published version of a first prototypic jogging navigation system for visually impaired and blind people, this paper presents an adaptive lane detection method that extents the use on standard running tracks to general and less structured paths. Additional and supporting developments refer to an active camera stabilization, the improved intuitive vibration feedback and the integrated and embedded mechatronic system setup.}, 
keywords={cameras;embedded systems;handicapped aids;image colour analysis;mechatronics;object detection;vibrations;adaptive lane detection;color based lane detection;wearable jogging navigation system;visually impaired;blind people;active camera stabilization;intuitive vibration feedback;integrated mechatronic system setup;embedded mechatronic system setup;Navigation;Cameras;Vibrations;Brushless DC motors;Three-dimensional displays}, 
doi={10.1109/BIOROB.2016.7523715}, 
ISSN={2155-1782}, 
month={June},}
@INPROCEEDINGS{6460985, 
author={Y. H. Lee and T. Leung and G. Medioni}, 
booktitle={Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)}, 
title={Real-time staircase detection from a wearable stereo system}, 
year={2012}, 
volume={}, 
number={}, 
pages={3770-3773}, 
abstract={We address the problem of staircase detection, in the context of a navigation aid for the visually impaired. The requirements for such a system are robustness to viewpoint, distance, scale, real-time operation, high detection rate and low false alarm rate. Our approach uses classifiers trained using Haar features and Ad-aboost learning. This first stage does detect staircases, but produces many false alarms. The false alarm rate is drastically reduced by using spatial context in the form of the estimated ground plane, and by enforcing temporal consistency. We have validated our approach on many real sequences under various weather conditions, and are presenting some of the quantitative results here.}, 
keywords={Haar transforms;handicapped aids;learning (artificial intelligence);object detection;stereo image processing;real-time staircase detection;wearable stereo system;visually impaired;navigation aid;Haar features;Ad-aboost learning;false alarm rate;spatial context;temporal consistency;real sequences;weather conditions;Detectors;Cameras;Real-time systems;Estimation;Accuracy;Navigation;Robustness}, 
doi={}, 
ISSN={1051-4651}, 
month={Nov},}
@INPROCEEDINGS{4463247, 
author={L. A. Johnson and C. M. Higgins}, 
booktitle={2006 International Conference of the IEEE Engineering in Medicine and Biology Society}, 
title={A Navigation Aid for the Blind Using Tactile-Visual Sensory Substitution}, 
year={2006}, 
volume={}, 
number={}, 
pages={6289-6292}, 
abstract={The objective of this study is to improve the quality of life for the visually impaired by restoring their ability to self-navigate. In this paper we describe a compact, wearable device that converts visual information into a tactile signal. This device, constructed entirely from commercially available parts, enables the user to perceive distant objects via a different sensory modality. Preliminary data suggest that this device is useful for object avoidance in simple environments}, 
keywords={biomedical equipment;handicapped aids;sensory aids;navigation aid;blind;tactile-visual sensory substitution;quality of life;visually impaired person;wearable device;visual information;tactile signal;distant object perception;sensory modality;object avoidance;Navigation;Skin;Object recognition;Spatial resolution;Biomedical engineering;Cities and towns;USA Councils;Signal restoration;Organisms;Humans;Algorithms;Blindness;Brain Mapping;Computer Simulation;Electric Conductivity;Equipment Design;Humans;Pattern Recognition, Visual;Sensation;Sensory Aids;Somatosensory Cortex;Space Perception;Touch;Visual Perception;Visually Impaired Persons}, 
doi={10.1109/IEMBS.2006.259473}, 
ISSN={1557-170X}, 
month={Aug},}
@INPROCEEDINGS{7749459, 
author={L. Everding and L. Walger and V. S. Ghaderi and J. Conradt}, 
booktitle={2016 IEEE 18th International Conference on e-Health Networking, Applications and Services (Healthcom)}, 
title={A mobility device for the blind with improved vertical resolution using dynamic vision sensors}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={We propose an improved version of a wearable lightweight device to support visually impaired people during their everyday lives by facilitating autonomous navigation and obstacle avoidance. The system deploys two retina-inspired Dynamic Vision Sensors for visual information gathering. These sensors are characterized by very low power consumption, low latency and drastically reduced data rate in comparison with regular CMOS/ CCD cameras which makes them well suited for real-time mobile applications. Event-based algorithms operating on the visual data stream extract depth information in real-time which is translated into the acoustic domain. Spatial auditory signals are simulated at the computed origin of visual events in the real world. These sounds are modulated according to the position in the field of view which the user can change by moving their head. Here, different tests with eleven subjects are conducted to evaluate the performance of the system. These tests show that the modulation helps to improve object localization performance significantly in comparison to prior experiments. Further trials estimate the visual acuity a user of the device would have using the Landolt C test. The low power consumption of all integrated components in a final system will allow for a long lasting battery life of a small portable device, which might ultimately combine perceived visual information and environmental knowledge to provide a higher quality of life for the visually impaired.}, 
keywords={assisted living;auditory evoked potentials;body sensor networks;vision defects;visual evoked potentials;blind;improved vertical resolution;wearable lightweight device;visually impaired people;autonomous navigation;obstacle avoidance;retina-inspired dynamic vision sensors;visual information gathering;real-time mobile applications;event-based algorithms;acoustic domain;spatial auditory signal simulation;object localization performance;visual acuity estimation;all integrated components;Visualization;Sensors;Cameras;Voltage control;Performance evaluation;Object detection;Conferences;wearable assistive device;mobility aid;event-based vision;acoustic scene representation;sensory substitution}, 
doi={10.1109/HealthCom.2016.7749459}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7300753, 
author={M. Rey and I. Hertzog and N. Kagami and L. Nedel}, 
booktitle={2015 XVII Symposium on Virtual and Augmented Reality}, 
title={Blind Guardian: A Sonar-Based Solution for Avoiding Collisions with the Real World}, 
year={2015}, 
volume={}, 
number={}, 
pages={237-244}, 
abstract={Sightless navigation is an ever-present issue that affects a great part of the population. The affected include permanent or temporarily blind individuals, persons walking in the dark, and users of immersive virtual environments using real walking for navigation. This paper presents an alternative solution to this problem, which relies on a simple wearable device based on ultrasonic waves to detect obstacles and on vibrotactile feedback to warn the user of nearby obstacles. In the following pages, we describe the design and implementation of this apparatus, called the Blind Guardian. We conducted user tests with 29 subjects in a controlled environment. Results demonstrated the potential of Blind Guardian for future use in real life situations, as well as for immersive virtual reality applications based on the use of head-mounted displays.}, 
keywords={collision avoidance;handicapped aids;haptic interfaces;object detection;sonar imaging;ultrasonic waves;virtual reality;Blind Guardian;sonar-based solution;collision avoidance;sightless navigation;blind individuals;wearable device;ultrasonic waves;obstacle detection;vibrotactile feedback;immersive virtual reality applications;head-mounted displays;Legged locomotion;Vibrations;Acoustics;Global Positioning System;Light emitting diodes;Sociology;blind navigation;real walking;collision detection;ultrasonic signs;tactile feedback;arduino}, 
doi={10.1109/SVR.2015.41}, 
ISSN={}, 
month={May},}
@ARTICLE{7842859, 
author={P. A. Zientara and S. Lee and G. H. Smith and R. Brenner and L. Itti and M. B. Rosson and J. M. Carroll and K. M. Irick and V. Narayanan}, 
journal={Computer}, 
title={Third Eye: A Shopping Assistant for the Visually Impaired}, 
year={2017}, 
volume={50}, 
number={2}, 
pages={16-24}, 
abstract={Through a combination of wearable cameras, hardware accelerators, and algorithms, a vision-based automatic shopping assistant allows users with limited or no sight to select products from grocery shelves.}, 
keywords={computer vision;handicapped aids;wearable cameras;hardware accelerators;vision-based automatic shopping assistant;grocery shelves;product selection;Cameras;Navigation;Visualization;Feature extraction;Asssistive technology;Object recognition;Haptic interfaces;Pervasive computing;assistive technology;visual sensing;object recognition;object detection;visual algorithms;human augmentation;visual augmentation;haptic gloves;pervasive computing}, 
doi={10.1109/MC.2017.36}, 
ISSN={0018-9162}, 
month={Feb},}
@ARTICLE{8319419, 
author={W. M. Elmannai and K. M. Elleithy}, 
journal={IEEE Access}, 
title={A Highly Accurate and Reliable Data Fusion Framework for Guiding the Visually Impaired}, 
year={2018}, 
volume={6}, 
number={}, 
pages={33029-33054}, 
abstract={The world has approximately 253 million visually impaired (VI) people according to a report by the world health organization (WHO) in 2014. Thirty-six million people are estimated to be blind. According to WHO, 217 million people are estimated to have moderate to severe visual impairment. An important factor that motivated this research is the fact that 90% of VI people live in developing countries. Several systems were designed to improve the quality of the life of VI people and support their mobility. Unfortunately, none of these systems are considered to be a complete solution for VI people and these systems are very expensive. We present in this paper an intelligent framework for supporting VI people. The proposed work integrates sensor-based and computer vision-based techniques to provide an accurate and economical solution. These techniques allow us to detect multiple objects and enhance the accuracy of the collision avoidance system. In addition, we introduce a novel obstacle avoidance algorithm based on the image depth information and fuzzy logic. By using the fuzzy logic, we were able to provide precise information to help the VI user in avoiding front obstacles. The system has been deployed and tested in real-time scenarios. An accuracy of 98% was obtained for detecting objects and 100% accuracy in avoiding the detected objects.}, 
keywords={collision avoidance;computer vision;fuzzy logic;handicapped aids;object detection;sensor fusion;fuzzy logic;collision avoidance system;computer vision-based techniques;sensor-based techniques;blind;WHO;VI user;VI people;severe visual impairment;thirty-six million people;world health organization;approximately 253 million visually impaired people;reliable data fusion framework;Navigation;Sensors;Assistive technology;Visualization;Acoustics;Collision avoidance;Real-time systems;Assistive wearable devices;computer vision systems;data fusion algorithm;obstacle detection and obstacle collision avoidance;sensor-based networks;visual impairment;blindness;mobility limitation}, 
doi={10.1109/ACCESS.2018.2817164}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{8319310, 
author={W. M. Elmannai and K. M. Elleithy}, 
booktitle={2018 15th IEEE Annual Consumer Communications Networking Conference (CCNC)}, 
title={A novel obstacle avoidance system for guiding the visually impaired through the use of fuzzy control logic}, 
year={2018}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={This paper presents an intelligent framework that includes several types of sensors embedded in a wearable device to support the visually impaired (VI) community. The proposed work is based on an integration of sensor-based techniques and a computer vision-based technology in order to introduce an efficient and economical visual device. The 98% accuracy rate of the proposed sequence is based on a wide detection view that used two camera modules and a detection range of approximate 9meteres. In addition, we introduce a novel obstacle avoidance approach based on the image depth and fuzzy control rules. In this approach, each frame divided into three areas. By using the fuzzy logic, we were able to provide precise information to help the VI user in avoiding front obstacles. The strength of this proposed approach aids the VI users in avoiding 100% of all identified objects. Once the device is initialized, the VI user can confidently enter unfamiliar surroundings. Therefore, this implemented device can be described as following: accurate, reliable, friendly, light, and economically accessible that facilitates the indoor and outdoor mobility of VI people and does not require any previous knowledge of the surrounding environment.}, 
keywords={cameras;collision avoidance;computer vision;fuzzy logic;handicapped aids;mobile robots;fuzzy control logic;intelligent framework;wearable device;visually impaired community;computer vision;economical visual device;camera modules;detection range;obstacle avoidance approach;image depth;fuzzy control rules;fuzzy logic;VI people;obstacle avoidance system;Collision avoidance;Fuzzy logic;Computer vision;Navigation;Object detection;Sensors;Conferences;assistive wearable devices;computer vision techniques;obstacle detection;obstacle avoidance;visual impairment;fuzzy logic}, 
doi={10.1109/CCNC.2018.8319310}, 
ISSN={2331-9860}, 
month={Jan},}
@INPROCEEDINGS{7830606, 
author={L. González-Delgado and L. Serpa-Andrade and K. Calle-Urgiléz and A. Guzhñay-Lucero and V. Robles-Bykbaev and M. Mena-Salcedo}, 
booktitle={2016 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)}, 
title={A low-cost wearable support system for visually disabled people}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-5}, 
abstract={According to World Health Organization (WHO), currently in the World there are around 39 million persons visually impaired. These persons need developing several daily activities in the same way that they are done by people without visual impairments. These activities can be very difficult or dangerous for visually impaired persons (e.g. using public transport, crossing streets, walking alone, etc.). On those grounds, in this paper we present a low-cost system able to meet some needs of blind persons, such as face recognition, reminders to take medications, automatic mail reading, automatic detection of objects and their distance, among others. The proposed system can be easily added to any type of glove. With the aim of determining the real feasibility of the proposed approach, we have carried a pilot experiment with 5 persons with visual impairments. The first part of pilot experiment consisted of determining the response of the device under 4 environments with different light conditions as well as the object detection. In the second part we have measured the perceptions of participants after using device in real conditions. The achieved results shown good agreement of users with respect to weight and handling of the device (80% of participants are strongly agree); whereas the comfort must be improved (60% of participants are agree).}, 
keywords={data gloves;handicapped aids;object detection;low-cost wearable support system;visually disabled people;World Health Organization;visually impaired persons;glove;visual impairments;object detection;Visualization;Blindness;Face recognition;Robots;Vibrations;Diseases;Electronic mail;Speech-Language Therapy;robotic assistant;communication disorders;children with disabilities}, 
doi={10.1109/ROPEC.2016.7830606}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{8354230, 
author={H. Yu and E. Ohn-Bar and D. Yoo and K. M. Kitani}, 
booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
title={SmartPartNet: Part-Informed Person Detection for Body-Worn Smartphones}, 
year={2018}, 
volume={}, 
number={}, 
pages={1103-1112}, 
abstract={We are interested in the development of image-based person detection algorithms for wearable computing using commodity smartphones. We focus on the use of smartphones as a wearable device because it is a practical means of augmenting human sensing for applications such as navigation for the blind or assisting social interaction. We identify two unique features of developing a vision-based person detector for body-worn smartphones: (1) the detector must take into account the strong bias in the size of people in the images taken with a wearable device and (2) the detector must consider the low image quality due to dim lighting and rapid ego-motion which leads to motion blur. In order to account for the unique distribution over the visibility of body parts when using a wearable camera, we propose a part-based person detector specialized for chestmounted smartphones. We perform extensive ablative analysis on the usefulness of part information, providing several insights regarding the design of the optimal person detector across different application domains. To account for the frequent occurrence of motion blur in our target domain, we introduce a data augmentation technique to generate synthetic motion-blurred images during training. In addition to addressing the aforementioned features, the final detector must also run in real-time using only smartphone resources. We leverage recent progress in deep neural networks for mobile devices and show that our proposed person detector, SmartPartNet, obtains performance similar to state-of-the-art pedestrian detection networks, while being 3X smaller and 5X faster.}, 
keywords={computer vision;image motion analysis;mobile computing;neural nets;object detection;smart phones;wearable computers;SmartPartNet;person detection algorithms;wearable computing;commodity smartphones;wearable device;rapid ego-motion;motion blur;body parts;wearable camera;chestmounted smartphones;optimal person detector;smartphone resources;application domains;pedestrian detection networks;part-informed person detection;body-worn smartphones;image-based person detection;vision-based person detector;data augmentation;synthetic motion-blurred image generation;deep neural networks;Detectors;Smart phones;Training;Object detection;Cameras;Task analysis;Computer vision}, 
doi={10.1109/WACV.2018.00126}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{7727129, 
author={K. P. Hariharasudhan and B. V. S. Vignesh and R. Muniraj}, 
booktitle={2016 10th International Conference on Intelligent Systems and Control (ISCO)}, 
title={Visual aid a next generation concept}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-6}, 
abstract={The conventional visual aids use a simpler procedure in guiding the individuals. These devices warn the individual through physical contact which might be life threatening in a few cases. Today wearable devices are revolutionary by their extensive uses to humans. The gadgets do a lot of things useful to the user. Our idea throws light into a newer dimension to the wearable devices to visually impaired individuals. The idea arose due to the practical difficulties faced by them. The main difficulty is finding the path in front of them, taking action/making responses to tackle the obstacles. This device works based on the visual guiding in bats, which is they use high frequency ultrasonic waves to find obstacles and make responses. The generation of pulses and receiving of pulses helps the device to sense the obstacles and alarm the user. The obstacles are identified based on the time interval between the sending and receiving of the signal.}, 
keywords={acoustic pulses;acoustic signal processing;handicapped aids;object detection;ultrasonic applications;vision defects;visual aid;physical contact;wearable devices;gadgets;visually impaired individuals;visual guidance;high frequency ultrasonic waves;obstacle finding;pulse generation;pulse reception;obstacle sensing;obstacle identification;Acoustics;Rectifiers;Transmitters;Microcontrollers;Programming;Visualization;Receivers}, 
doi={10.1109/ISCO.2016.7727129}, 
ISSN={}, 
month={Jan},}
@INPROCEEDINGS{7020476, 
author={B. Söveny and G. Kovács and Z. T. Kardkovács}, 
booktitle={2014 5th IEEE Conference on Cognitive Infocommunications (CogInfoCom)}, 
title={Blind guide - A virtual eye for guiding indoor and outdoor movement}, 
year={2014}, 
volume={}, 
number={}, 
pages={343-347}, 
abstract={In this paper, we present a design of a wearable equipment that helps with the perception of the environment for blind and visually impaired people in indoor and outdoor mobility and navigation. Our prototype can detect and identify traffic situations such as street crossings, traffic lamps, cars, cyclists, other people and low and high obstacles. The detection takes place in real time based on input data of sensors and optical cameras, the mobility of the user is aided with audio signals.}, 
keywords={assisted living;eye;handicapped aids;indoor navigation;blind guide;virtual eye;indoor movement guidance;outdoor movement guidance;visually impaired people;navigation;traffic situation identification;street crossings;traffic lamps;cars;cyclists;audio signals;Cameras;Navigation;Prototypes;Vectors;Real-time systems;Image color analysis;Object detection}, 
doi={10.1109/CogInfoCom.2014.7020476}, 
ISSN={}, 
month={Nov},}
@INPROCEEDINGS{1565326, 
author={G. Iannizzotto and C. Costanzo and P. Lanzafame and F. La Rosa}, 
booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops}, 
title={Badge3D for Visually Impaired}, 
year={2005}, 
volume={}, 
number={}, 
pages={29-29}, 
abstract={Conducting an independent life is probably the most important issue for visually impaired people. In this paper, we suggest a contribution to the solution of this problem using wearable computer technology. We present a visual support system that provides acoustic information about the objects in the surrounding environment, obtained by remotely reading barcode tags sticked on significant objects and surrounding elements, such as doors, windows and so on. The user, walking in an indoor environment, is informed in realtime about the location (direction, distance and pose) of the available objects. Barcode tags deployed in the environment can act as reliable stimuli that trigger local navigation behaviours to achieve global navigation objectives. The proposed system is expected to be useful in the real-time interaction with dynamic environments. To illustrate our work, we introduce a proof-of-concept multimodal, sensorbased application and discuss its implementation and the obtained experimental results.}, 
keywords={Cameras;Navigation;Wheels;Stereo vision;Object detection;Mathematics;Wearable computers;Legged locomotion;Indoor environments;Real time systems}, 
doi={10.1109/CVPR.2005.420}, 
ISSN={2160-7508}, 
month={Sept},}
@INPROCEEDINGS{7351291, 
author={T. Schwarze and Z. Zhong}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={Stair detection and tracking from egocentric stereo vision}, 
year={2015}, 
volume={}, 
number={}, 
pages={2690-2694}, 
abstract={In this work we present a real-time approach to capture the properties of staircases from a free moving, head mounted stereo-camera. A variety of systems can profit from such ability, examples range from robots in multi-floor exploration scenarios to wearable assistance systems for the visually impaired. We introduce a light-weight method to measure the individual steps and use this information to update a minimal stair model while approaching and traversing the stair. Results are evaluated on an in- and outdoor scenario and show competitive accuracy to state of the art approaches working on precise lidar-sensors.}, 
keywords={handicapped aids;helmet mounted displays;object detection;object tracking;real-time systems;robot vision;stereo image processing;visually impaired;wearable assistance systems;multifloor exploration scenarios;robots;head mounted stereo-camera;real-time approach;egocentric stereo vision;stair tracking;stair detection;Image edge detection;Cameras;Estimation;Three-dimensional displays;Visualization;Feature extraction;Tracking;Environment perception;Scene Reconstruction;Object tracking}, 
doi={10.1109/ICIP.2015.7351291}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7477714, 
author={L. Chen and K. Duan}, 
booktitle={2016 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
title={MIDI-assisted egocentric optical music recognition}, 
year={2016}, 
volume={}, 
number={}, 
pages={1-9}, 
abstract={Egocentric vision has received increasing attention in recent years due to the vast development of wearable devices and their applications. Although there are numerous existing work on egocentric vision, none of them solve Optical Music Recognition (OMR) problem. In this paper, we propose a novel optical music recognition approach for egocentric device (e.g. Google Glass) with the assistance of MIDI data. We formulate the problem as a structured sequence alignment problem as opposed to the blind recognition in traditional OMR systems. We propose a linear-chain Conditional Random Field (CRF) to model the note event sequence, which translates the relative temporal relations contained by MIDI to spatial constraints over the egocentric observation. We performed evaluations to compare the proposed approach with several different baselines and proved that our approach achieved the highest recognition accuracy. We view our work as the first step towards egocentric optical music recognition, and believe it will bring insights for next-generation music pedagogy and music entertainment.}, 
keywords={computer vision;music;object recognition;random processes;MIDI;egocentric optical music recognition;egocentric vision;wearable device;egocentric device;Google Glass;structured sequence alignment problem;blind recognition;linear-chain conditional random field;CRF;note event sequence;spatial constraint;next-generation music pedagogy;music entertainment;Computational modeling;Optical sensors;Optical imaging;Cameras;Data models;Head;Google}, 
doi={10.1109/WACV.2016.7477714}, 
ISSN={}, 
month={March},}
@INPROCEEDINGS{6345928, 
author={J. D. Weiland and N. Parikh and V. Pradeep and G. Medioni}, 
booktitle={2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society}, 
title={Smart image processing system for retinal prosthesis}, 
year={2012}, 
volume={}, 
number={}, 
pages={300-303}, 
abstract={Retinal prostheses for the blind have demonstrated the ability to provide the sensation of light in otherwise blind individuals. However, visual task performance in these patients remains poor relative to someone with normal vision. Computer vision algorithms for navigation and object detection were evaluated for their ability to improve task performance. Blind subjects navigating a mobility course had fewer collisions when using a wearable camera system that guided them on a safe path. Subjects using a retinal prosthesis simulator could locate objects more quickly when an object detection algorithm assisted them. Computer vision algorithms can assist retinal prosthesis patients and low-vision patients in general.}, 
keywords={artificial organs;computer vision;eye;handicapped aids;smart image processing system;retinal prosthesis;visual task performance;computer vision algorithms;mobility course navigation;object detection algorithm;low vision patients;Retina;Prosthetics;Navigation;Cameras;Image processing;Visualization;Computer vision;Algorithms;Blindness;Eye, Artificial;Female;Humans;Image Processing, Computer-Assisted;Image Processing, Computer-Assisted;Male;Retina;Task Performance and Analysis;Visual Prosthesis}, 
doi={10.1109/EMBC.2012.6345928}, 
ISSN={1558-4615}, 
month={Aug},}
@ARTICLE{7497561, 
author={R. Ma and F. Hu and Q. Hao}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
title={Active Compressive Sensing via Pyroelectric Infrared Sensor for Human Situation Recognition}, 
year={2017}, 
volume={47}, 
number={12}, 
pages={3340-3350}, 
abstract={Conventional pyroelectric infrared (PIR) motion sensors use paired elements for the detection of moving targets. This method makes them incapable of measuring thermal signals from static targets. We need an active sensor that can detect static thermal subjects. This paper presents our design of active PIR sensors. The proposed PIR sensing systems can actively detect static thermal targets by using three methods that are suitable to different applications: 1) a sensor that can be rotated by a self-controlled servo motor for the detection of moving or static thermal subjects nearby; 2) a sensor that is equipped with a mask for low-complexity posture recognition; and 3) a sensor that can be worn on the wrist for the recognition of surrounding subjects (this sensor is especially useful for blind users). Compressive sensing (CS) theory indicates that random down-sampling method can capture more accurate information of the original signal than the evenly spaced sampling. Based on CS theory, we have developed the random sampling structures for the active PIR systems, and have built a statistical feature space for human scenario recognition. The experimental results demonstrate that the active sensing system can efficiently measure the static thermal targets, and the random sampling scheme has a better recognition performance than the even sampling scheme.}, 
keywords={compressed sensing;feature extraction;infrared detectors;infrared imaging;object detection;pyroelectric detectors;sampling methods;sensors;human scenario recognition;active sensing system;recognition performance;active compressive sensing;human situation recognition;conventional pyroelectric infrared motion sensors;thermal signals;moving targets detection;active PIR systems;random sampling structures;down-sampling method;low-complexity posture recognition;static thermal targets;PIR sensing systems;Thermal sensors;Sensor systems;Cameras;Wearable sensors;Choppers (circuits);Magnetic sensors;Active sensing;compressive sensing (CS);pyroelectric infrared (PIR) sensor;random sampling;situation recognition}, 
doi={10.1109/TSMC.2016.2578465}, 
ISSN={2168-2216}, 
month={Dec},}
@INPROCEEDINGS{6328048, 
author={M. Dao and R. Mattivi and F. G. B. De Natale and K. Masui and N. Babaguchi}, 
booktitle={2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance}, 
title={Abandoned Object's Owner Detection: A Case Study of Hybrid Mobile-Fixed Video Surveillance System}, 
year={2012}, 
volume={}, 
number={}, 
pages={404-409}, 
abstract={In this paper, a new framework of hybrid mobile-fixed video surveillance system (HMFVSS) is introduced. The purpose of this framework is to overcome common problems of existing mobile or fixed video surveillance systems: (1) moral harassment: due to unfriendly or unnaturally installed mobile sensors, and (2) blind areas: due to narrow-scope moving of fixed cameras. A case study of abandoned object's owner alert system (AOOAS) is also presented to emphasize the framework's advantages. IP cameras and "Spyglass" (i.e. a mobile camera embedded on glasses) are used as fixed and mobile sensors, respectively. There are three main tasks are inherited, developed, and integrated: (1) image registration for automatically locating abandoned object, (2) common histogram based abandoned object's owner detection, and (3) faces recognition. The experimental results with careful evaluation and comparison with others shows that the proposed framework moves a step ahead in video surveillance system.}, 
keywords={face recognition;image motion analysis;image registration;object detection;video surveillance;hybrid mobile-fixed video surveillance system;HMFVSS;moral harassment;blind areas;abandoned object owner alert system;AOOAS;IP cameras;spyglass;image registration;common histogram-based abandoned object owner detection;face recognition;narrow-scope movement;Cameras;Mobile communication;Security;IP networks;Sensors;Video surveillance;Video Surveillance;Abandoned Objects;Image Registration;Spyglass}, 
doi={10.1109/AVSS.2012.4}, 
ISSN={}, 
month={Sept},}
@INPROCEEDINGS{7001338, 
author={M. Cotter and S. Advani and J. Sampson and K. Irick and V. Narayanan}, 
booktitle={2014 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
title={A hardware accelerated multilevel visual classifier for embedded visual-assist systems}, 
year={2014}, 
volume={}, 
number={}, 
pages={96-100}, 
abstract={Embedded visual assist systems are emerging as increasingly viable tools for aiding visually impaired persons in their day-to-day life activities. Novel wearable devices with imaging capabilities will be uniquely positioned to assist visually impaired in activities such as grocery shopping. However, supporting such time-sensitive applications on embedded platforms requires an intelligent trade-off between accuracy and computational efficiency. In order to maximize their utility in real-world scenarios, visual classifiers often need to recognize objects within large sets of object classes that are both diverse and deep. In a grocery market, simultaneously recognizing the appearance of people, shopping carts, and pasta is an example of a common diverse object classification task. Moreover, a useful visual-aid system would need deep classification capability to distinguish among the many styles and brands of pasta to direct attention to a particular box. Exemplar Support Vector Machines (ESVMs) provide a means of achieving this specificity, but are resource intensive as computation increases rapidly with the number of classes to be recognized. To maintain scalability without sacrificing accuracy, we examine the use of a biologically-inspired classifier (HMAX) as a front-end filter that can narrow the set of ESVMs to be evaluated. We show that a hierarchical classifier combining HMAX and ESVM performs better than either of the two individually. We achieve 12% improvement in accuracy over HMAX and 4% improvement over ESVM while reducing computational overhead of evaluating all possible exemplars.}, 
keywords={image classification;image filtering;marketing;object recognition;support vector machines;multilevel visual classifier;embedded visual-assist systems;grocery market;object classification;exemplar support vector machines;ESVM;biologically-inspired classifier;HMAX;multilevel filtering;Accuracy;Visualization;Computer vision;Computational modeling;Computer architecture;Training;Feature extraction}, 
doi={10.1109/ICCAD.2014.7001338}, 
ISSN={1092-3152}, 
month={Nov},}