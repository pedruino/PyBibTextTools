@inproceedings{Navik:2017:IEB:3102304.3102333,
    author= {Navik, Ankit P. and Dirisina, Vaishnavi and Thakkar, Hitendra and Sharma, Madhuri R. and Bhat, Aditya},
    title= {Intel Edison Based Scalable Platform Solution for Visually Impaired People on W-IoT},
    booktitle= {Proceedings of the International Conference on Future Networks and Distributed Systems},
    series= {ICFNDS '17},
    year= {2017},
    isbn= {978-1-4503-4844-7},
    location= {Cambridge, United Kingdom},
    pages= {29:1--29:3},
    articleno= {29},
    numpages= {3},
    url= {https://dl.acm.org/tab\_abstract.cfm?id=3102333},
    doi= {10.1145/3102304.3102333},
    acmid= {3102333},
    publisher= {ACM},
    address= {New York, NY, USA},
    keywords= {Computer vision, navigation, object detection, object recognition, speech processing, wearable IoT},
    abstract= {

Wearable Internet of Things (IoT) is seen as one of the advance paradigm in widespread applications such as health care, fitness, etc. There is a huge gap in the technology that helps today the visually disabled people to this advance level. There is a very much need to have a single robust solution utilizing the information from Computer vision and combining with a 360 degree protection for the user, without constraining the user from anything. A novel scalable solution for visually impaired is developed and a prototype is made using Intel Edison, camera and sensors. The proposed methodology provides the visually impaired with detail information about their surroundings. In this paper, we presents scalable solution using computer vision techniques and sensors to achieve independent navigation.

}
}

@inproceedings{Medeiros:2017:RCC:3132525.3134805,
    author= {Medeiros, Alexander J. and Stearns, Lee and Findlater, Leah and Chen, Chuan and Froehlich, Jon E.},
    title= {Recognizing Clothing Colors and Visual Textures Using a Finger-Mounted Camera: An Initial Investigation},
    booktitle= {Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility},
    series= {ASSETS '17},
    year= {2017},
    isbn= {978-1-4503-4926-0},
    location= {Baltimore, Maryland, USA},
    pages= {393--394},
    numpages= {2},
    url= {https://dl.acm.org/tab\_abstract.cfm?id=3134805},
    doi= {10.1145/3132525.3134805},
    acmid= {3134805},
    publisher= {ACM},
    address= {New York, NY, USA},
    keywords= {blind, texture recognition, visually impaired, wearables},
    abstract= {

We investigate clothing color and visual texture recognition using images from a finger-mounted camera to support people with visual impairments. Our approach mitigates issues with distance and lighting that can impact the accuracy of existing color and texture recognizers and allows for easy touch-based interrogation to better understand clothing appearance. We classify image textures by combining two off-the-shelf techniques commonly used for object recognition achieving 99.4\% accuracy on a dataset of 520 clothing images across 9 texture categories. We close with a discussion of potential applications, user evaluation plans, and open questions.

}
}

@inproceedings{Korn:2018:EPD:3197768.3201541,
    author= {Korn, Oliver and Holt, Raymond and Kontopoulos, Efstratios and Kappers, Astrid M.L. and Persson, Nils-Krister and Olson, Nasrine},
    title= {Empowering Persons with Deafblindness: Designing an Intelligent Assistive Wearable in the SUITCEYES Project},
    booktitle= {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
    series= {PETRA '18},
    year= {2018},
    isbn= {978-1-4503-6390-7},
    location= {Corfu, Greece},
    pages= {545--551},
    numpages= {7},
    url= {https://dl.acm.org/tab\_abstract.cfm?id=3201541},
    doi= {10.1145/3197768.3201541},
    acmid= {3201541},
    publisher= {ACM},
    address= {New York, NY, USA},
    keywords= {Assistive Technologies, Deafblindness, Gamification, Haptics, Hearing Impairment, Smart Textiles, Visual Impairments, Wearables},
    abstract= {

Deafblindness is a condition that limits communication capabilities primarily to the haptic channel. In the EU-funded project SUITCEYES we design a system which allows haptic and thermal communication via soft interfaces and textiles. Based on user needs and informed by disability studies, we combine elements from smart textiles, sensors, semantic technologies, image processing, face and object recognition, machine learning, affective computing, and gamification. In this work, we present the underlying concepts and the overall design vision of the resulting assistive smart wearable.

}
}

@inproceedings{Buimer:2017:MFE:3132525.3134823,
    author= {Buimer, Hendrik and Van der Geest, Thea and Nemri, Abdellatif and Schellens, Renske and Van Wezel, Richard and Zhao, Yan},
    title= {Making Facial Expressions of Emotions Accessible for Visually Impaired Persons},
    booktitle= {Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility},
    series= {ASSETS '17},
    year= {2017},
    isbn= {978-1-4503-4926-0},
    location= {Baltimore, Maryland, USA},
    pages= {331--332},
    numpages= {2},
    url= {https://dl.acm.org/tab\_abstract.cfm?id=3134823},
    doi= {10.1145/3132525.3134823},
    acmid= {3134823},
    publisher= {ACM},
    address= {New York, NY, USA},
    keywords= {emotion recognition, facial expressions, vibrotactile output, vision-impaired persons, wearables},
    abstract= {

One of the big problems visually impaired persons experience in their daily lives, is the inability to see non-verbal cues of conversation partners. In this study, a wearable assistive technology is presented and evaluated which supports visually impaired persons with the recognition of facial expressions of emotions. The wearable assistive technology consists of a camera clipped on spectacles, emotion recognition software, and a vibrotactile belt with six tactors. An earlier controlled experimental study showed that users of the system improved significantly in their ability to recognize emotions from validated stimuli. In this paper, the next iteration in testing the system is presented, in which a more realistic usage situation was simulated. Eight visually impaired persons were invited to participate in conversations with an actor, who was instructed not to exaggerate his facial expressions. Participants engaged in two 15-minute mock job interview conversations, during one of which they were wearing the system. In the other conversation, no assistive technologies were used. The preliminary results showed that the concept of such wearable assistive technologies remains feasible. Participants within the study found it easy to learn and interpret the vibrotactile cues, which was also shown in their training performance. Furthermore, most participants could use the vibrotactile cues, while being able to stay engaged in the conversation. Nevertheless, some improvements are needed before the system can be used as assistive technology. The accuracy of the system was negatively affected by the lighting and movement conditions present in realistic conversations, compared to the controlled experiment condition. Furthermore, participants requested developments to improve the wearability of the system.

}
}
