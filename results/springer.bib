@Article{10.1007/s00371-013-0886-1,
    author = {Jafri, Rabia and Ali, Syed Abid and Arabnia, Hamid R. and Fatima, Shameem},
    title = {Computer vision-based object recognition for the visually impaired in an indoors environment: a survey},
    journal = {The Visual Computer},
    volume = {30.0},
    issue = {11},
    doi = {10.1007/s00371-013-0886-1},
    year = {2014},
    url = {http://link.springer.com/article/10.1007/s00371-013-0886-1},
    abstract = {Though several electronic assistive devices have been developed for the visually impaired in the past few decades, however, relatively few solutions have been devised to aid them in recognizing generic objects in their environment, particularly indoors. Nevertheless, research in this area is gaining momentum. Among the various technologies being utilized for this purpose, computer vision based solutions are emerging as one of the most promising options mainly due to their affordability and accessibility. This paper provides an overview of the various technologies that have been developed in recent years to assist the visually impaired in recognizing generic objects in an indoors environment with a focus on approaches based on computer vision. It aims to introduce researchers to the latest trends in this area as well as to serve as a resource for developers who wish to incorporate such solutions into their own work.}
}

@Article{10.1007/s12193-016-0235-6,
    author = {Bhowmick, Alexy and Hazarika, Shyamanta M.},
    title = {An insight into assistive technology for the visually impaired and blind people: state-of-the-art and future trends},
    journal = {Journal on Multimodal User Interfaces},
    volume = {11.0},
    issue = {2},
    doi = {10.1007/s12193-016-0235-6},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s12193-016-0235-6},
    abstract = {Assistive technology for the visually impaired and blind people is a research field that is gaining increasing prominence owing to an explosion of new interest in it from disparate disciplines. The field has a very relevant social impact on our ever-increasing aging and blind populations. While many excellent state-of-the-art accounts have been written till date, all of them are subjective in nature. We performed an objective statistical survey across the various sub-disciplines in the field and applied information analysis and network-theory techniques to answer several key questions relevant to the field. To analyze the field we compiled an extensive database of scientific research publications over the last two decades. We inferred interesting patterns and statistics concerning the main research areas and underlying themes, identified leading journals and conferences, captured growth patterns of the research field; identified active research communities and present our interpretation of trends in the field for the near future. Our results reveal that there has been a sustained growth in this field; from less than 50 publications per year in the mid 1990s to close to 400 scientific publications per year in 2014. Assistive Technology for persons with visually impairments is expected to grow at a swift pace and impact the lives of individuals and the elderly in ways not previously possible.}
}

@Article{10.1007/s13721-013-0026-x,
    author = {Yi, Chucai and Flores, Roberto W. and Chincha, Ricardo and Ying and Tian, Li},
    title = {Finding objects for assisting blind people},
    journal = {Network Modeling Analysis in Health Informatics and Bioinformatics},
    volume = {2.0},
    issue = {2},
    doi = {10.1007/s13721-013-0026-x},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s13721-013-0026-x},
    abstract = {Computer vision technology has been widely used for blind assistance, such as navigation and way finding. However, few camera-based systems are developed for helping blind or visually impaired people to find daily necessities. In this paper, we propose a prototype system of blind-assistant object finding by camera-based network and matching-based recognition. We collect a dataset of daily necessities and apply Speeded-Up Robust Features and Scale Invariant Feature Transform feature descriptors to perform object recognition. Experimental results demonstrate the effectiveness of our prototype system.}
}

@Article{10.1007/s00138-012-0431-7,
    author = {Ying and Tian, Li and Yang, Xiaodong and Yi, Chucai and Arditi, Aries},
    title = {Toward a computer vision-based wayfinding aid for blind persons to access unfamiliar indoor environments},
    journal = {Machine Vision and Applications},
    volume = {24.0},
    issue = {3},
    doi = {10.1007/s00138-012-0431-7},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s00138-012-0431-7},
    abstract = {Independent travel is a well-known challenge for blind and visually impaired persons. In this paper, we propose a proof-of-concept computer vision-based wayfinding aid for blind people to independently access unfamiliar indoor environments. In order to find different rooms (e.g. an office, a laboratory, or a bathroom) and other building amenities (e.g. an exit or an elevator), we incorporate object detection with text recognition. First, we develop a robust and efficient algorithm to detect doors, elevators, and cabinets based on their general geometric shape, by combining edges and corners. The algorithm is general enough to handle large intra-class variations of objects with different appearances among different indoor environments, as well as small inter-class differences between different objects such as doors and door-like cabinets. Next, to distinguish intra-class objects (e.g. an office door from a bathroom door), we extract and recognize text information associated with the detected objects. For text recognition, we first extract text regions from signs with multiple colors and possibly complex backgrounds, and then apply character localization and topological analysis to filter out background interference. The extracted text is recognized using off-the-shelf optical character recognition software products. The object type, orientation, location, and text information are presented to the blind traveler as speech.}
}

@Article{10.1007/s11042-016-3617-6,
    author = {Tapu, Ruxandra and Mocanu, Bogdan and Zaharia, Titus},
    title = {A computer vision-based perception system for visually impaired},
    journal = {Multimedia Tools and Applications},
    volume = {76.0},
    issue = {9},
    doi = {10.1007/s11042-016-3617-6},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11042-016-3617-6},
    abstract = {In this paper, we introduce a novel computer vision-based perception system, dedicated to the autonomous navigation of visually impaired people. A first feature concerns the real-time detection and recognition of obstacles and moving objects present in potentially cluttered urban scenes. To this purpose, a motion-based, real-time object detection and classification method is proposed. The method requires no a priori information about the obstacle type, size, position or location. In order to enhance the navigation/positioning capabilities offered by traditional GPS-based approaches, which are often unreliably in urban environments, a building/landmark recognition approach is also proposed. Finally, for the specific case of indoor applications, the system has the possibility to learn a set of user-defined objects of interest. Here, multi-object identification and tracking is applied in order to guide the user to localize such objects of interest. The feedback is presented to user by audio warnings/alerts/indications. Bone conduction headphones are employed in order to allow visually impaired to hear the systems warnings without obstructing the sounds from the environment. At the hardware level, the system is totally integrated on an android smartphone which makes it easy to wear, non-invasive and low-cost.}
}

@Article{10.1007/s00779-015-0841-4,
    author = {Takizawa, Hotaka and Yamaguchi, Shotaro and Aoyagi, Mayumi and Ezaki, Nobuo and Mizuno, Shinji},
    title = {Kinect cane: an assistive system for the visually impaired based on the concept of object recognition aid},
    journal = {Personal and Ubiquitous Computing},
    volume = {19.0},
    issue = {5 - 6},
    doi = {10.1007/s00779-015-0841-4},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s00779-015-0841-4},
    abstract = {This paper proposes a novel concept to assist visually impaired individuals in recognizing three-dimensional objects in everyday environments. This concept is realized as a portable system that consists of a white cane, a Microsoft Kinect sensor, a numeric keypad, a tactile feedback device, and other components. By the use of the Kinect sensor, the system searches for an object that a visually impaired user instructs the system to find and then returns a searching result to the user via the tactile feedback device. The major advantage of the system is the ability to recognize the objects of various classes, such as chairs and staircases, out of detectable range of white canes. Furthermore, the system is designed to return minimum required information related to the instruction of a user so that the user can obtain necessary information more efficiently. The system is evaluated through two types of experiment: object recognition test and user study. The experimental results indicate that the system is promising as a means of helping visually impaired users recognize objects.}
}

@Article{10.1007/s12193-015-0182-7,
    author = {Csapó, Ádám and Wersényi, György and Nagy, Hunor and Stockman, Tony},
    title = {A survey of assistive technologies and applications for blind users on mobile platforms: a review and foundation for research},
    journal = {Journal on Multimodal User Interfaces},
    volume = {9.0},
    issue = {4},
    doi = {10.1007/s12193-015-0182-7},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s12193-015-0182-7},
    abstract = {This paper summarizes recent developments in audio and tactile feedback based assistive technologies targeting the blind community. Current technology allows applications to be efficiently distributed and run on mobile and handheld devices, even in cases where computational requirements are significant. As a result, electronic travel aids, navigational assistance modules, text-to-speech applications, as well as virtual audio displays which combine audio with haptic channels are becoming integrated into standard mobile devices. This trend, combined with the appearance of increasingly user-friendly interfaces and modes of interaction has opened a variety of new perspectives for the rehabilitation and training of users with visual impairments. The goal of this paper is to provide an overview of these developments based on recent advances in basic research and application development. Using this overview as a foundation, an agenda is outlined for future research in mobile interaction design with respect to users with special needs, as well as ultimately in relation to sensor-bridging applications in general.}
}

@Article{10.1007/s10209-017-0550-z,
    author = {Huang, Hsinfu},
    title = {Blind users’ expectations of touch interfaces: factors affecting interface accessibility of touchscreen-based smartphones for people with moderate visual impairment},
    journal = {Universal Access in the Information Society},
    volume = {17.0},
    issue = {2},
    doi = {10.1007/s10209-017-0550-z},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s10209-017-0550-z},
    abstract = {Although current touchscreen-based smartphones are equipped with some accessibility functions for visually impaired people, these users still face substantial challenges, particularly when using smooth touchscreens. In this study, the accessibility factors of touchscreen interfaces for people with visual impairment were explored through principal components analysis. A total of 32 persons with moderate visual impairment and an average age of 35.6 years (SD = 1.62) participated. The accessibility requirements and user experiences of smartphone touchscreen interfaces were collected. The results indicate that touchscreen interfaces have six major accessibility factors. In addition, the operational style of touchscreen interfaces should be redesigned according to a two-stage process. Furthermore, the design guidelines for accessible touchscreen interfaces that meet the requirements of people with visual impairments are summarised. The findings provide an essential reference for product designers constructing accessible touchscreen interfaces, and reinforce the concept of equality in product use.}
}

@Article{10.1007/s11227-016-1891-8,
    author = {Jafri, Rabia},
    title = {A GPU-accelerated real-time contextual awareness application for the visually impaired on Google’s project Tango device},
    journal = {The Journal of Supercomputing},
    volume = {73.0},
    issue = {2},
    doi = {10.1007/s11227-016-1891-8},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11227-016-1891-8},
    abstract = {An application for the recently introduced Google Project Tango Tablet Development Kit to assist visually impaired (VI) users in understanding their environmental context by identifying and locating multiple faces and objects in their vicinity in real-time is presented. CUDA-based GPU-accelerated algorithms would be utilized to detect and recognize faces and objects from the visual data, while the locations of these entities relative to the user would be estimated from the depth data acquired via the tablet. The interaction would be speech based with the user being offered several options for requesting information about the identities and/or relative locations of face and objects. The aim is to create a portable, affordable, power-efficient, standalone assistive application to increase the autonomy of VI users which can run in real time on the device itself.}
}

@Article{10.1007/s13721-013-0027-9,
    author = {Wang, Shuihua and Yang, Xiaodong and Tian, Yingli},
    title = {Detecting signage and doors for blind navigation and wayfinding},
    journal = {Network Modeling Analysis in Health Informatics and Bioinformatics},
    volume = {2.0},
    issue = {2},
    doi = {10.1007/s13721-013-0027-9},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s13721-013-0027-9},
    abstract = {Signage plays a very important role to find destinations in applications of navigation and wayfinding. In this paper, we propose a novel framework to detect doors and signage to help blind people accessing unfamiliar indoor environments. In order to eliminate the interference information and improve the accuracy of signage detection, we first extract the attended areas using a saliency map. Then the signage is detected in the attended areas using a bipartite graph matching. The proposed method can handle multiple signage detection. Furthermore, in order to provide more information for blind users to access the area associated with the detected signage, we develop a robust method to detect doors based on a geometric door frame model which is independent to door appearances. Experimental results on our collected datasets of indoor signage and doors demonstrate the effectiveness and efficiency of our proposed method.}
}

@Article{10.1007/s40595-016-0075-z,
    author = {Hoang, Van-Nam and Nguyen, Thanh-Huong and Le, Thi-Lan and Tran, Thanh-Hai and Vuong, Tan-Phu and Vuillerme, Nicolas},
    title = {Obstacle detection and warning system for visually impaired people based on electrode matrix and mobile Kinect},
    journal = {Vietnam Journal of Computer Science},
    volume = {4.0},
    issue = {2},
    doi = {10.1007/s40595-016-0075-z},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s40595-016-0075-z},
    abstract = {Obstacle detection and warning can improve the mobility as well as the safety of visually impaired people specially in unfamiliar environments. For this, firstly, obstacles are detected and localized and then the information of the obstacles will be sent to the visually impaired people by using different modalities such as voice, tactile, vibration. In this paper, we present an assistive system for visually impaired people based on the matrix of electrode and a mobile Kinect. This system consists of two main components: environment information acquisition and analysis and information representation. The first component aims at capturing the environment by using a mobile Kinect and analyzing it in order to detect the predefined obstacles for visually impaired people, while the second component tries to represent obstacle’s information under the form of electrode matrix.}
}

@Article{10.1007/s12193-015-0191-6,
    author = {Sövény, Bálint and Kovács, Gábor and Kardkovács, Zsolt T.},
    title = {Blind guide},
    journal = {Journal on Multimodal User Interfaces},
    volume = {9.0},
    issue = {4},
    doi = {10.1007/s12193-015-0191-6},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s12193-015-0191-6},
    abstract = {In this paper, we present a design of a wearable equipment that helps with the perception of the environment for visually impaired people in both indoor and outdoor mobility and navigation. Our prototype can detect and identify traffic situations such as street crossings, traffic lamps, cars, cyclists, other people and obstacles hanging down from above or placed on the ground. The detection takes place in real time based on input data of sensors and cameras, the mobility of the user is aided with audio signals.}
}

@Article{10.1186/s13673-018-0134-9,
    author = {Jafri, Rabia and Khan, Marwa Mahmoud},
    title = {User-centered design of a depth data based obstacle detection and avoidance system for the visually impaired},
    journal = {Human-centric Computing and Information Sciences},
    volume = {8.0},
    issue = {1},
    doi = {10.1186/s13673-018-0134-9},
    year = {2018},
    url = {http://link.springer.com/article/10.1186/s13673-018-0134-9},
    abstract = {The development of a novel depth-data based real-time obstacle detection and avoidance application for visually impaired (VI) individuals to assist them in navigating independently in indoors environments is presented in this paper. The application utilizes a mainstream, computationally efficient mobile device as the development platform in order to create a solution which not only is aesthetically appealing, cost-effective, lightweight and portable but also provides real-time performance and freedom from network connectivity constraints. To alleviate usability problems, a user-centered design approach has been adopted wherein semi-structured interviews with VI individuals in the local context were conducted to understand their micro-navigation practices, challenges and needs. The invaluable insights gained from these interviews have not only informed the design of our system but would also benefit other researchers developing similar applications. The resulting system design along with a detailed description of its obstacle detection and unique multimodal feedback generation modules has been provided. We plan to iteratively develop and test the initial prototype of the system with the end users to resolve any usability issues and better adapt it to their needs.}
}

@Article{10.1007/s11042-015-3204-2,
    author = {Nguyen, Quoc-Hung and Vu, Hai and Tran, Thanh-Hai and Nguyen, Quang-Hoan},
    title = {Developing a way-finding system on mobile robot assisting visually impaired people in an indoor environment},
    journal = {Multimedia Tools and Applications},
    volume = {76.0},
    issue = {2},
    doi = {10.1007/s11042-015-3204-2},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11042-015-3204-2},
    abstract = {A way-finding system in an indoor environment consists of several components: localization, representation, path planning, and interaction. For each component, numerous relevant techniques have been proposed. However, deploying feasible techniques, particularly in real scenarios, remains challenging. In this paper, we describe a functional way-finding system deployed on a mobile robot to assist visual impairments (VI). The proposed system deploys state-of-the-art techniques that are adapted to the practical issues at hand. First, we adapt an outdoor visual odometry technique to indoor use by covering manual markers or stickers on ground-planes. The main purpose is to build reliable travel routes in the environment. Second, we propose a procedure to define and optimize the landmark/representative scenes of the environment. This technique handles the repetitive and ambiguous structures of the environment. In order to interact with VI people, we deploy a convenient interface on a smart phone. Three different indoor scenarios and thirteen subjects are conducted in our evaluations. Our experimental results show that VI people, particularly VI pupils, can find the right way to requested targets.}
}

@Article{10.1007/s13218-015-0407-7,
    author = {Schwarze, Tobias and Lauer, Martin and Schwaab, Manuel and Romanovas, Michailas and Böhm, Sandra and Jürgensohn, Thomas},
    title = {A Camera-Based Mobility Aid for Visually Impaired People},
    journal = {KI - Künstliche Intelligenz},
    volume = {30.0},
    issue = {1},
    doi = {10.1007/s13218-015-0407-7},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s13218-015-0407-7},
    abstract = {We present a wearable assistance system for visually impaired persons that perceives the environment with a stereo camera system and communicates obstacles and other objects to the user in form of intuitive acoustic feedback. The system is designed to complement traditional assistance aids. We describe the core techniques of scene understanding, head tracking, and sonification and show in an experimental study that it enables users to walk in unknown urban terrain and to avoid obstacles safely.}
}

@Article{10.1007/s10055-012-0213-6,
    author = {Katz, Brian F. G. and Kammoun, Slim and Parseihian, Gaëtan and Gutierrez, Olivier and Brilhault, Adrien and Auvray, Malika and Truillet, Philippe and Denis, Michel and Thorpe, Simon and Jouffrais, Christophe},
    title = {NAVIG: augmented reality guidance system for the visually impaired},
    journal = {Virtual Reality},
    volume = {16.0},
    issue = {4},
    doi = {10.1007/s10055-012-0213-6},
    year = {2012},
    url = {http://link.springer.com/article/10.1007/s10055-012-0213-6},
    abstract = {Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired.}
}

@Article{10.1007/s10209-007-0109-5,
    author = {Evreinova, Tatiana V. and Evreinov, Grigori and Raisamo, Roope},
    title = {A camera-joystick for sound-augmented non-visual navigation and target acquisition: a case study},
    journal = {Universal Access in the Information Society},
    volume = {7.0},
    issue = {3},
    doi = {10.1007/s10209-007-0109-5},
    year = {2008},
    url = {http://link.springer.com/article/10.1007/s10209-007-0109-5},
    abstract = {This paper presents the results of a comparative study of user input with a camera-joystick and a manual joystick used in a target acquisition task when neither targets nor pointer could be perceived visually. The camera-joystick is an input technique in which each on-screen item is accessible from the center with a predefined vector of head motion. Absolute pointing was implemented with an acceleration factor of 1.7 and a moving average on 5 detected head positions. The underlying assumption was that, in order to provide a robust input for blind users, the interaction technique has to be based on perceptually well-discriminated human movements, which compose a basic framework of an accessible virtual workspace demanding minimum external auxiliary cues. The target spots, having a diameter of 35 mm and a distance between the centers of adjacent spots of 60 mm, were arranged in a rectangular grid of 5 rows by 5 columns. The targets were captured from a distance of 600 mm. The results have shown that the camera input is a promising technique for non-visual human–computer interaction. The subjects demonstrated, more than twice, better performance in the target acquisition task with the camera-joystick versus the manual joystick. All the participants reported that the camera-joystick was a robust and preferable input technique when visual information was not available. Blind interaction techniques could be significantly further improved allowing a user-dependent activation of the navigational cues to better coordinate feedbacks with exploratory behavior.}
}

@Article{10.1007/s13721-013-0025-y,
    author = {Hasanuzzaman, Faiz M. and Yang, Xiaodong and Ying and Tian, Li and Liu, Qingshan and Capezuti, Elizabeth},
    title = {Monitoring activity of taking medicine by incorporating RFID and video analysis},
    journal = {Network Modeling Analysis in Health Informatics and Bioinformatics},
    volume = {2.0},
    issue = {2},
    doi = {10.1007/s13721-013-0025-y},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s13721-013-0025-y},
    abstract = {In this paper, we present a new framework to monitor medication intake for elderly individuals by incorporating a video camera and radio frequency identification (RFID) sensors. The proposed framework can provide a key function for monitoring activities of daily living (ADLs) of elderly people at their own home. In an assistive environment, RFID tags are applied on medicine bottles located in a medicine cabinet so that each medicine bottle will have a unique ID. The description of the medicine data for each tag is manually input to a database. RFID readers will detect if any of these bottles are taken away from the medicine cabinet and identify the tag attached on the medicine bottle. A video camera is installed to continue monitoring the activity of taking medicine by integrating face detection and tracking, mouth detection, background subtraction, and activity detection. The preliminary results demonstrate that 100 \% detection accuracy for identifying medicine bottles and promising results for monitoring activity of taking medicine.}
}

@Article{10.1007/s00138-015-0706-x,
    author = {Baştan, Muhammet},
    title = {Multi-view object detection in dual-energy X-ray images},
    journal = {Machine Vision and Applications},
    volume = {26.0},
    issue = {7 - 8},
    doi = {10.1007/s00138-015-0706-x},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s00138-015-0706-x},
    abstract = {Automatic inspection of X-ray scans at security checkpoints can improve the public security. X-ray images are different from photographic images. They are transparent. They contain much less texture. They may be highly cluttered. Objects may undergo in- and out-of-plane rotations. On the other hand, scale and illumination change is less of an issue. More importantly, X-ray imaging provides extra information which are usually not available in regular images: dual-energy imaging, which provides material information about the objects; and multi-view imaging, which provides multiple images of objects from different viewing angles. Such peculiarities of X-ray images should be leveraged for high-performance object recognition systems to be deployed on X-ray scanners. To this end, we first present an extensive evaluation of standard local features for object detection on a large X-ray image dataset in a structured learning framework. Then, we propose two dense sampling methods as keypoint detector for textureless objects and extend the SPIN color descriptor to utilize the material information. Finally, we propose a multi-view branch-and-bound search algorithm for multi-view object detection. Through extensive experiments on three object categories, we show that object detection performance on X-ray images improves substantially with the help of extended features and multiple views.}
}

@Article{10.1007/s11042-007-0167-y,
    author = {Kahol, Kanav and Panchanathan, Sethuraman},
    title = {Neuro-cognitively inspired haptic user interfaces},
    journal = {Multimedia Tools and Applications},
    volume = {37.0},
    issue = {1},
    doi = {10.1007/s11042-007-0167-y},
    year = {2008},
    url = {http://link.springer.com/article/10.1007/s11042-007-0167-y},
    abstract = {Haptic systems and devices are a recent addition to multimodal systems. These devices have widespread applications such as surgical simulations, medical and procedural training, scientific visualizations, assistive and rehabilitative devices for individuals who have physical or neurological impediments and assistive devices for individuals who are blind. While the potential of haptics in natural human machine interaction is undisputable, the realization of such means is still a long way ahead. There are considerable research challenges to development of natural haptic interfaces. The study of human tactile abilities is a recent endeavor and many of the available systems still do not incorporate the domain knowledge of psychophysics, biomechanics and neurological elements of haptic perception. Development of smart and effective haptic interfaces and devices requires extensive studies that link perceptual phenomena with measurable parameters and incorporation of such domain knowledge in the engineering of haptic interfaces. This paper presents design, development and usability testing of a neuro-cognitively inspired haptic user interface for individuals who are blind. The proposed system design is inspired by neuro-cognitive basis of haptic perception and incorporates the computational aspects and requirements of multimodal information processing system. Usability testing of the system suggests that a biologically inspired haptic user interfaces may form a powerful paradigm for haptic user interface design.}
}

@Article{10.1007/s41315-017-0023-9,
    author = {Kim, Pileun and Chen, Jingdao and Cho, Yong K.},
    title = {Robotic sensing and object recognition from thermal-mapped point clouds},
    journal = {International Journal of Intelligent Robotics and Applications},
    volume = {1.0},
    issue = {3},
    doi = {10.1007/s41315-017-0023-9},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s41315-017-0023-9},
    abstract = {Many of the civil structures are more than half way through or nearing their intended service life; frequently assessing and maintaining structural integrity is a top maintenance priority. Robotic inspection technologies using ground and aerial robots with 3D scanning and imaging capabilities have the potential to improve safety and efficiency of infrastructure management. To provide more valuable information to inspectors and agency decision makers, automatic environment sensing and semantic information extraction are fundamental issues in this field. This paper introduces an innovative method for generating thermal-mapped point clouds of a robot’s work environment and performing automatic object recognition with the aid of thermal data fused to 3D point clouds. The laser scanned point cloud and thermal data were collected using a custom-designed mobile robot. The multimodal data was combined with a data fusion process based on texture mapping. The automatic object recognition was performed by two processes: segmentation with thermal data and classification with scanned geometric features. The proposed method was validated with the scan data collected in an entire building floor. Experimental results show that the thermal integrated object recognition approach achieved better performance than a geometry only-based approach, with an average recognition accuracy of 93\%, precision of 83\%, and recall rate of 86\% for objects in the tested environment including humans, display monitors and light fixtures.
}
}

@Article{10.1007/s00146-017-0766-8,
    author = {Chacin, Aisen C. and Iwata, Hiroo and Vesna, Victoria},
    title = {Assistive Device Art: aiding audio spatial location through the Echolocation Headphones},
    journal = {AI \\& SOCIETY},
    doi = {10.1007/s00146-017-0766-8},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s00146-017-0766-8},
    abstract = {Assistive Device Art derives from the integration of Assistive Technology and Art, involving the mediation of sensorimotor functions and perception from both, psychophysical methods and conceptual mechanics of sensory embodiment. This paper describes the concept of ADA and its origins by observing the phenomena that surround the aesthetics of prosthesis-related art. It also analyzes one case study, the Echolocation Headphones, relating its provenience and performance to this new conceptual and psychophysical approach of tool design. This ADA tool is designed to aid human echolocation. They facilitate the experience of sonic vision, as a way of reflecting and learning about the construct of our spatial perception. Echolocation Headphones are a pair of opaque goggles which disable the participant’s vision. This device emits a focused sound beam which activates the space with directional acoustic reflection, giving the user the ability to navigate and perceive space through audition. The directional properties of parametric sound provide the participant a focal echo, similar to the focal point of vision. This study analyzes the effectiveness of this wearable sensory extension for aiding auditory spatial location in three experiments; optimal sound type and distance for object location, perceptual resolution by just noticeable difference, and goal-directed spatial navigation for open pathway detection, all conducted at the Virtual Reality Lab of the University of Tsukuba, Japan. The Echolocation Headphones have been designed for a diverse participant base. They have both the potential to aid auditory spatial perception for the visually impaired and to train sighted individuals in gaining human echolocation abilities. Furthermore, this Assistive Device artwork instigates participants to contemplate on the plasticity of their sensorimotor architecture.}
}

@Article{10.1007/s10209-013-0338-8,
    author = {Serrão, M. and Shahrabadi, S. and Moreno, M. and José, J. T. and Rodrigues, J. I. and Rodrigues, J. M. F. and du Buf, J. M. H.},
    title = {Computer vision and GIS for the navigation of blind persons in buildings},
    journal = {Universal Access in the Information Society},
    volume = {14.0},
    issue = {1},
    doi = {10.1007/s10209-013-0338-8},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s10209-013-0338-8},
    abstract = {This paper presents a system which integrates a geographic information system of a building with computer vision. It uses only one camera, for example, the one of a mobile phone. Visual landmarks, such as frontal and lateral doors, stairs, signs, and fire extinguishers, are employed for localizing the user in the building and for tracing and validating a route for the user’s navigation. The developed system clearly improves the autonomy of persons with a very low vision during indoor navigation.}
}

@Article{10.1007/s11263-005-6642-x,
    author = {Tu, Zhuowen and Chen, Xiangrong and Yuille, Alan L. and Zhu, Song-Chun},
    title = {Image Parsing: Unifying Segmentation, Detection, and Recognition},
    journal = {International Journal of Computer Vision},
    volume = {63.0},
    issue = {2},
    doi = {10.1007/s11263-005-6642-x},
    year = {2005},
    url = {http://link.springer.com/article/10.1007/s11263-005-6642-x},
    abstract = {In this paper we present a Bayesian framework for parsing images into their constituent visual patterns. The parsing algorithm optimizes the posterior probability and outputs a scene representation as a “parsing graph”, in a spirit similar to parsing sentences in speech and natural language. The algorithm constructs the parsing graph and re-configures it dynamically using a set of moves, which are mostly reversible Markov chain jumps. This computational framework integrates two popular inference approaches—generative (top-down) methods and discriminative (bottom-up) methods. The former formulates the posterior probability in terms of generative models for images defined by likelihood functions and priors. The latter computes discriminative probabilities based on a sequence (cascade) of bottom-up tests/filters. In our Markov chain algorithm design, the posterior probability, defined by the generative models, is the invariant (target) probability for the Markov chain, and the discriminative probabilities are used to construct proposal probabilities to drive the Markov chain. Intuitively, the bottom-up discriminative probabilities activate top-down generative models. In this paper, we focus on two types of visual patterns—generic visual patterns, such as texture and shading, and object patterns including human faces and text. These types of patterns compete and cooperate to explain the image and so image parsing unifies image segmentation, object detection, and recognition (if we use generic visual patterns only then image parsing will correspond to image segmentation (Tu and Zhu, 2002. IEEE Trans. PAMI, 24(5):657–673). We illustrate our algorithm on natural images of complex city scenes and show examples where image segmentation can be improved by allowing object specific knowledge to disambiguate low-level segmentation cues, and conversely where object detection can be improved by using generic visual patterns to explain away shadows and occlusions.}
}

@Article{10.1007/s10209-009-0171-2,
    author = {Sauer, Graig and Holman, Jonathan and Lazar, Jonathan and Hochheiser, Harry and Feng, Jinjuan},
    title = {Accessible privacy and security: a universally usable human-interaction proof tool},
    journal = {Universal Access in the Information Society},
    volume = {9.0},
    issue = {3},
    doi = {10.1007/s10209-009-0171-2},
    year = {2010},
    url = {http://link.springer.com/article/10.1007/s10209-009-0171-2},
    abstract = {Despite growing interest in designing usable systems for managing privacy and security, recent efforts have generally failed to address the needs of users with disabilities. As security and privacy tools often rely upon subtle visual cues or other potentially inaccessible indicators, users with perceptual limitations might find such tools particularly challenging. To understand the needs of an important group of users with disabilities, a focus group was conducted with blind users to determine their perceptions of security-related challenges. Human-interaction proof (HIP) tools, commonly known as CAPTCHAs, are used by web pages to defeat robots and were identified in the focus group as a major concern. Therefore, a usability test was conducted to see how well blind users were able to use audio equivalents of these graphical tools. Finally, an accessible HIP tool was developed which combines audio and matching images, supporting both visual and audio output. Encouraging results from a small usability evaluation of the prototype with five sighted users and five blind users show that this new form of HIP is preferred by both blind and visual users to previous forms of text-based HIPs. Future directions for research are also discussed.}
}

@Article{10.1007/s11042-015-2745-8,
    author = {Parra, Lorena and Sendra, Sandra and Jiménez, José Miguel and Lloret, Jaime},
    title = {Multimedia sensors embedded in smartphones for ambient assisted living and e-health},
    journal = {Multimedia Tools and Applications},
    volume = {75.0},
    issue = {21},
    doi = {10.1007/s11042-015-2745-8},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s11042-015-2745-8},
    abstract = {Nowadays, it is widely extended the use of smartphones to make human life more comfortable. Moreover, there is a special interest on Ambient Assisted Living (AAL) and e-Health applications. The sensor technology is growing and amount of embedded sensors in the smartphones can be very useful for AAL and e-Health. While some sensors like the accelerometer, gyroscope or light sensor are very used in applications such as motion detection or light meter, there are other ones, like the microphone and camera which can be used as multimedia sensors. This paper reviews the published papers focused on showing proposals, designs and deployments of that make use of multimedia sensors for AAL and e-health. We have classified them as a function of their main use. They are the sound gathered by the microphone and image recorded by the camera. We also include a comparative table and analyze the gathered information.}
}

@Article{10.1007/s00138-011-0381-5,
    author = {Metari, S. and Prel, F. and Moszkowicz, T. and Laurendeau, D. and Teasdale, N. and Beauchemin, S. and Simoneau, M.},
    title = {A computer vision framework for the analysis and interpretation of the cephalo-ocular behavior of drivers},
    journal = {Machine Vision and Applications},
    volume = {24.0},
    issue = {1},
    doi = {10.1007/s00138-011-0381-5},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s00138-011-0381-5},
    abstract = {In this paper, we introduce a computer vision system specially designed for the analysis and interpretation of the cephalo-ocular behavior of drivers. The system is composed of both hardware and software components and is described in three steps. The first step is devoted to the description of the driving simulator and the developed software. The second step deals with the identification of the driver’s visual search actions using computer vision. The latter are related to specific driving events such as blind spot checking and rear-view/lateral mirror verification. Based on the simulator’s open module, the third step is concerned with the identification of car/road events (overtaking, crossing an intersection) and the mapping of these events with the driver’s behavior. The proposed system will be used by a kinesiology research group for the evaluation and improvement of driver performances in a safe environment (driving simulator). In addition to the controlled environment, a modified version of the system also deals with real driving contexts (i.e. driving in a real car). Experimental results confirm both the robustness and the effectiveness of the proposed cephalo-ocular analysis framework.}
}

@Article{10.1007/s10032-004-0138-z,
    author = {Liang, Jian and Doermann, David and Li, Huiping},
    title = {Camera-based analysis of text and documents: a survey},
    journal = {International Journal of Document Analysis and Recognition (IJDAR)},
    volume = {7.0},
    issue = {2 - 3},
    doi = {10.1007/s10032-004-0138-z},
    year = {2005},
    url = {http://link.springer.com/article/10.1007/s10032-004-0138-z},
    abstract = {The increasing availability of high-performance, low-priced, portable digital imaging devices has created a tremendous opportunity for supplementing traditional scanning for document image acquisition. Digital cameras attached to cellular phones, PDAs, or wearable computers, and standalone image or video devices are highly mobile and easy to use; they can capture images of thick books, historical manuscripts too fragile to touch, and text in scenes, making them much more versatile than desktop scanners. Should robust solutions to the analysis of documents captured with such devices become available, there will clearly be a demand in many domains. Traditional scanner-based document analysis techniques provide us with a good reference and starting point, but they cannot be used directly on camera-captured images. Camera-captured images can suffer from low resolution, blur, and perspective distortion, as well as complex layout and interaction of the content and background. In this paper we present a survey of application domains, technical challenges, and solutions for the analysis of documents captured by digital cameras. We begin by describing typical imaging devices and the imaging process. We discuss document analysis from a single camera-captured image as well as multiple frames and highlight some sample applications under development and feasible ideas for future development.}
}

@Article{10.1007/s11042-017-5054-6,
    author = {Abdi, Lotfi and Meddeb, Aref},
    title = {Driver information system: a combination of augmented reality, deep learning and vehicular Ad-hoc networks},
    journal = {Multimedia Tools and Applications},
    volume = {77.0},
    issue = {12},
    doi = {10.1007/s11042-017-5054-6},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-017-5054-6},
    abstract = {Improving traffic safety is one of the important goals of Intelligent Transportation Systems (ITS). In vehicle-based safety systems, it is more desirable to prevent an accident than to reduce severity of injuries. Critical traffic problems such as accidents and traffic congestion require the development of new transportation systems. Research in perceptual and human factors assessment is needed for relevant and correct display of this information for maximal road traffic safety as well as optimal driver comfort. One of the solutions to prevent accidents is to provide information on the surrounding environment of the driver. Augmented Reality Head-Up Display (AR-HUD) can facilitate a new form of dialogue between the vehicle and the driver; and enhance ITS by superimposing surrounding traffic information on the users view and keep drivers view on roads. In this paper, we propose a fast deep-learning-based object detection approaches for identifying and recognizing road obstacles types, as well as interpreting and predicting complex traffic situations. A single convolutional neural network predicts region of interest and class probabilities directly from full images in one evaluation. We also investigated potential costs and benefits of using dynamic conformal AR cues in improving driving safety. A new AR-HUD approach to create real-time interactive traffic animations was introduced in terms of types of obstacle, rules for placement and visibility, and projection of these on an in-vehicle display. The novelty of our approach is that both global and local context information are integrated into a unified framework to distinguish the ambiguous detection outcomes, enhance ITS by superimposing surrounding traffic information on the users view and keep drivers view on roads.}
}

@Article{10.1007/s11042-018-5846-3,
    author = {Talebi, Mehdi and Vafaei, Abbas and Monadjemi, Amirhassan},
    title = {Vision-based entrance detection in outdoor scenes},
    journal = {Multimedia Tools and Applications},
    doi = {10.1007/s11042-018-5846-3},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-5846-3},
    abstract = {Doors are a significant object for the visually impaired and robots to enter and exit buildings. Although the accuracy of door detection is reported high in indoor scenes, it has become a difficult problem in outdoor scenes in computer vision. The reason may lie in the fact that such properties of a simple ordinary door such as handles, corners, and the gap between the door and the ground may not be visible due to the great variety of doors in outdoor environments. In this paper, we present a vision-based method for detecting building entrances in outdoor images. After extracting the lines and deleting the extra ones, regions between the vertical lines are specified and the features including height, width, location, color, texture and the number of lines inside the regions are obtained. Finally, some additional knowledge such as door existence at the bottom of the image, a reasonable height and width of a door, the difference between color and texture of the doors and those of the neighboring regions, and numerous lines on doors is used to decide on door detection. The method was tested on the eTRIMS dataset, door images from the ImageNet dataset, and our own dataset including doors of houses, apartments, and stores leading to acceptable results. The obtained results show that our approach outperforms comparable state-of-the-art approaches.}
}

@Article{10.1007/s00371-016-1250-z,
    author = {Catala, Alejandro and Oliver, Miguel and Molina, Jose Pascual and Gonzalez, Pascual},
    title = {Involving multiple fingers in exploring a haptic surface: an evaluation study},
    journal = {The Visual Computer},
    volume = {32.0},
    issue = {6 - 8},
    doi = {10.1007/s00371-016-1250-z},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s00371-016-1250-z},
    abstract = {In most haptic search tasks, tactile stimuli are usually presented to the fingers to discriminate simulated features and identify patterns. In this paper, we focus on a more complex exploration task in which users have to discriminate different stimuli, move their fingers in a free way to find and locate an object in a wider area of exploration and integrate all the perceived information to determine the position of the object. The study explores how users perform this haptic search task involving one or two fingers on a surface area. In order to carry out this research and overcome limitations of current hardware approaches to multi-point haptic surfaces, we used a setting consisting of a capacitive multi-touch screen and a general-purpose wearable vibrotactile device designed in our laboratory. The results indicate that using one finger in one hand shows to be more effective than using two fingers in either one or two hands in the task under study. Users showed higher confidence, lower exploration times, higher amount of right answers and higher exploration speed. This suggests that great efforts in providing independent multi-point haptic surface hardware could not be a priority for this kind of exploration task.}
}

@Article{10.1007/s00138-002-0096-8,
    author = {Starner, Thad and Leibe, Bastian and Minnen, David and Westyn, Tracy and Hurst, Amy and Weeks, Justin},
    title = {The perceptive workbench: Computer-vision-based gesture tracking, object tracking, and 3D reconstruction for augmented desks},
    journal = {Machine Vision and Applications},
    volume = {14.0},
    issue = {1},
    doi = {10.1007/s00138-002-0096-8},
    year = {2003},
    url = {http://link.springer.com/article/10.1007/s00138-002-0096-8},
    abstract = {Abstract. The Perceptive Workbench endeavors to create a spontaneous and unimpeded interface between the physical and virtual worlds. Its vision-based methods for interaction constitute an alternative to wired input devices and tethered tracking. Objects are recognized and tracked when placed on the display surface. By using multiple infrared light sources, the object's 3-D shape can be captured and inserted into the virtual interface. This ability permits spontaneity, since either preloaded objects or those objects selected at run-time by the user can become physical icons. Integrated into the same vision-based interface is the ability to identify 3-D hand position, pointing direction, and sweeping arm gestures. Such gestures can enhance selection, manipulation, and navigation tasks. The Perceptive Workbench has been used for a variety of applications, including augmented reality gaming and terrain navigation. This paper focuses on the techniques used in implementing the Perceptive Workbench and the system's performance.}
}

@Article{10.1007/s11263-018-1065-7,
    author = {Gurari, Danna and He, Kun and Xiong, Bo and Zhang, Jianming and Sameki, Mehrnoosh and Jain, Suyog Dutt and Sclaroff, Stan and Betke, Margrit and Grauman, Kristen},
    title = {Predicting Foreground Object Ambiguity and Efficiently Crowdsourcing the Segmentation(s)},
    journal = {International Journal of Computer Vision},
    volume = {126.0},
    issue = {7},
    doi = {10.1007/s11263-018-1065-7},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11263-018-1065-7},
    abstract = {We propose the ambiguity problem for the foreground object segmentation task and motivate the importance of estimating and accounting for this ambiguity when designing vision systems. Specifically, we distinguish between images which lead multiple annotators to segment different foreground objects (ambiguous) versus minor inter-annotator differences of the same object. Taking images from eight widely used datasets, we crowdsource labeling the images as “ambiguous” or “not ambiguous” to segment in order to construct a new dataset we call STATIC. Using STATIC, we develop a system that automatically predicts which images are ambiguous. Experiments demonstrate the advantage of our prediction system over existing saliency-based methods on images from vision benchmarks and images taken by blind people who are trying to recognize objects in their environment. Finally, we introduce a crowdsourcing system to achieve cost savings for collecting the diversity of all valid “ground truth” foreground object segmentations by collecting extra segmentations only when ambiguity is expected. Experiments show our system eliminates up to 47\% of human effort compared to existing crowdsourcing methods with no loss in capturing the diversity of ground truths.}
}

@Article{10.1007/s11042-017-5438-7,
    author = {Abbas, Qaisar and Ibrahim, Mostafa E. A. and Jaffar, M. Arfan},
    title = {Video scene analysis: an overview and challenges on deep learning algorithms},
    journal = {Multimedia Tools and Applications},
    volume = {77.0},
    issue = {16},
    doi = {10.1007/s11042-017-5438-7},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-017-5438-7},
    abstract = {Video scene analysis is a recent research topic due to its vital importance in many applications such as real-time vehicle activity tracking, pedestrian detection, surveillance, and robotics. Despite its popularity, the video scene analysis is still an open challenging task and require more accurate algorithms. However, the advances in deep learning algorithms for video scene analysis have been emerged in last few years for solving the problem of real-time processing. In this paper, a review of the recent developments in deep learning and video scene analysis problems is presented. In addition, this paper also briefly describes the most recent used datasets along with their limitations. Moreover, this review provides a detailed overview of the particular challenges existed in real-time video scene analysis that has been contributed towards activity recognition, scene interpretation, and video description/captioning. Finally, the paper summarizes the future trends and challenges in video scene analysis tasks and our insights are provided to inspire further research efforts.}
}

@Article{10.1007/s10111-017-0442-2,
    author = {Friedland, Heath and Snycerski, Susan and Palmer, Evan M. and Laraway, Sean},
    title = {The effectiveness of glare-reducing glasses on simulated nighttime driving performance in younger and older adults},
    journal = {Cognition, Technology \\& Work},
    volume = {19.0},
    issue = {4},
    doi = {10.1007/s10111-017-0442-2},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s10111-017-0442-2},
    abstract = {Glare from oncoming headlights is a problem for nighttime drivers because it can decrease visual acuity and cause discomfort. This diminished visual ability and discomfort due can increase drivers’ risk for traffic accidents. Older drivers experience more severe detrimental effects from nighttime glare, and these effects may pose a growing roadway hazard as the number of older drivers increases. The increased brightness of popular high-intensity-discharge (HID) headlights may further exacerbate these visibility problems. In a sample of younger (under 40 years of age) and older (40 years of age and older) drivers, we examined the impact of headlight glare from HID and traditional halogen lights on driving performance in a simulator, as well as the effectiveness of novel polarized glare-reducing eyeglasses for mitigating glare-induced performance deficits. The glare-reducing glasses increased visual awareness in the face of oncoming HID headlights compared to halogen headlights in both age groups. Older drivers performed significantly worse than did younger drivers on several measures of driving and visual detection performance. The glare-reducing glasses mitigated performance deficits, with older drivers performing similarly to younger drivers when exposed to HID headlights while wearing the polarized glasses. Due to the introduction of brighter LED-based headlights to the consumer automotive market and an expanding population of older drivers, automotive manufactures should consider glare-mitigation strategies when designing future headlight systems.}
}

@Article{10.1007/s11263-015-0851-8,
    author = {Rohrbach, Marcus and Rohrbach, Anna and Regneri, Michaela and Amin, Sikandar and Andriluka, Mykhaylo and Pinkal, Manfred and Schiele, Bernt},
    title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},
    journal = {International Journal of Computer Vision},
    volume = {119.0},
    issue = {3},
    doi = {10.1007/s11263-015-0851-8},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s11263-015-0851-8},
    abstract = {Activity recognition has shown impressive progress in recent years. However, the challenges of detecting fine-grained activities and understanding how they are combined into composite activities have been largely overlooked. In this work we approach both tasks and present a dataset which provides detailed annotations to address them. The first challenge is to detect fine-grained activities, which are defined by low inter-class variability and are typically characterized by fine-grained body motions. We explore how human pose and hands can help to approach this challenge by comparing two pose-based and two hand-centric features with state-of-the-art holistic features. To attack the second challenge, recognizing composite activities, we leverage the fact that these activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts. We show the benefits of our hand-centric approach for fine-grained activity classification and detection. For composite activity recognition we find that decomposition into attributes allows sharing information across composites and is essential to attack this hard task. Using script data we can recognize novel composites without having training data for them.}
}

@Article{10.1007/s00371-016-1302-4,
    author = {Wang, Jianhua and Zheng, Chuanxia and Chen, Weihai and Wu, Xingming},
    title = {Learning aggregated features and optimizing model for semantic labeling},
    journal = {The Visual Computer},
    volume = {33.0},
    issue = {12},
    doi = {10.1007/s00371-016-1302-4},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s00371-016-1302-4},
    abstract = {Semantic labeling for indoor scenes has been extensively developed with the wide availability of affordable RGB-D sensors. However, it is still a challenging task for multi-class recognition, especially for “small” objects. In this paper, a novel semantic labeling model based on aggregated features and contextual information is proposed. Given an RGB-D image, the proposed model first creates a hierarchical segmentation using an adapted gPb/UCM algorithm. Then, a support vector machine is trained to predict initial labels using aggregated features, which fuse small-scale appearance features, mid-scale geometric features, and large-scale scene features. Finally, a joint multi-label Conditional random field model that exploits both spatial and attributive contextual relations is constructed to optimize the initial semantic and attributive predicted results. The experimental results on the public NYU v2 dataset demonstrate the proposed model outperforms the existing state-of-the-art methods on the challenging 40 dominant classes task, and the model also achieves a good performance on a recent SUN RGB-D dataset. Especially, the prediction accuracy of “small” classes has been improved significantly.}
}

@Article{10.1007/s11432-012-4649-9,
    author = {De and Li, Ren and Liu, Yong and Xiu and Yuan, Xiao},
    title = {Image-based self-position and orientation method for moving platform},
    journal = {Science China Information Sciences},
    volume = {56.0},
    issue = {4},
    doi = {10.1007/s11432-012-4649-9},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s11432-012-4649-9},
    abstract = {The position and orientation of moving platform mainly depends on global positioning system and inertial navigation system in the field of low-altitude surveying, mapping and remote sensing and land-based mobile mapping system. However, GPS signal is unavailable in the application of deep space exploration and indoor robot control. In such circumstances, image-based methods are very important for self-position and orientation of moving platform. Therefore, this paper firstly introduces state of the art development of the image-based self-position and orientation method (ISPOM) for moving platform from the following aspects: 1) A comparison among major image-based methods (i.e., visual odometry, structure from motion, simultaneous localization and mapping) for position and orientation; 2) types of moving platform; 3) integration schemes of image sensor with other sensors; 4) calculation methodology and quantity of image sensors. Then, the paper proposes a new scheme of ISPOM for mobile robot — depending merely on image sensors. It takes the advantages of both monocular vision and stereo vision, and estimates the relative position and orientation of moving platform with high precision and high frequency. In a word, ISPOM will gradually speed from research to application, as well as play a vital role in deep space exploration and indoor robot control.}
}

@Article{10.1007/s11042-018-6515-2,
    author = {Obeso, A. Montoya and Benois-Pineau, J. and Vázquez, M. S. García and Acosta, A. A. Ramírez},
    title = {Saliency-based selection of visual content for deep convolutional neural networks},
    journal = {Multimedia Tools and Applications},
    doi = {10.1007/s11042-018-6515-2},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-6515-2},
    abstract = {The automatic description of digital multimedia content was mainly developed for classification tasks, retrieval systems and massive ordering of data. Preservation of cultural heritage is a field of high importance of application of these methods. We address classification problem in cultural heritage such as classification of architectural styles in digital photographs of Mexican cultural heritage. In general, the selection of relevant content in the scene for training classification models makes the models more efficient in terms of accuracy and training time. Here we use a saliency-driven approach to predict visual attention in images and use it to train a Deep Convolutional Neural Network. Also, we present an analysis of the behavior of the models trained under the state-of-the-art image cropping and the saliency maps. To train invariant models to rotations, data augmentation of training set is required, which posses problems of filling normalization of crops, we study were different padding techniques and we find an optimal solution. The results are compared with the state-of-the-art in terms of accuracy and training time. Furthermore, we are studying saliency cropping in training and generalization for another classical task such as weak labeling of massive collections of images containing objects of interest. Here the experiments are conducted on a large subset of ImageNet database. This work is an extension of preliminary research in terms of image padding methods and generalization on large scale generic database.}
}

@Article{10.1007/s11263-014-0794-5,
    author = {Lee, Yong Jae and Grauman, Kristen},
    title = {Predicting Important Objects for Egocentric Video Summarization},
    journal = {International Journal of Computer Vision},
    volume = {114.0},
    issue = {1},
    doi = {10.1007/s11263-014-0794-5},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s11263-014-0794-5},
    abstract = {We present a video summarization approach for egocentric or “wearable” camera data. Given hours of video, the proposed method produces a compact storyboard summary of the camera wearer’s day. In contrast to traditional keyframe selection techniques, the resulting summary focuses on the most important objects and people with which the camera wearer interacts. To accomplish this, we develop region cues indicative of high-level saliency in egocentric video—such as the nearness to hands, gaze, and frequency of occurrence—and learn a regressor to predict the relative importance of any new region based on these cues. Using these predictions and a simple form of temporal event detection, our method selects frames for the storyboard that reflect the key object-driven happenings. We adjust the compactness of the final summary given either an importance selection criterion or a length budget; for the latter, we design an efficient dynamic programming solution that accounts for importance, visual uniqueness, and temporal displacement. Critically, the approach is neither camera-wearer-specific nor object-specific; that means the learned importance metric need not be trained for a given user or context, and it can predict the importance of objects and people that have never been seen previously. Our results on two egocentric video datasets show the method’s promise relative to existing techniques for saliency and summarization.}
}

@Article{10.1007/s11263-007-0075-7,
    author = {Ross, David A. and Lim, Jongwoo and Lin, Ruei-Sung and Yang, Ming-Hsuan},
    title = {Incremental Learning for Robust Visual Tracking},
    journal = {International Journal of Computer Vision},
    volume = {77.0},
    issue = {1 - 3},
    doi = {10.1007/s11263-007-0075-7},
    year = {2008},
    url = {http://link.springer.com/article/10.1007/s11263-007-0075-7},
    abstract = {
Visual tracking, in essence, deals with non-stationary image streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail in the presence of significant variation of the object’s appearance or surrounding illumination. One reason for such failures is that many algorithms employ fixed appearance models of the target. Such models are trained using only appearance data available before tracking begins, which in practice limits the range of appearances that are modeled, and ignores the large volume of information (such as shape changes or specific lighting conditions) that becomes available during tracking. In this paper, we present a tracking method that incrementally learns a low-dimensional subspace representation, efficiently adapting online to changes in the appearance of the target. The model update, based on incremental algorithms for principal component analysis, includes two important features: a method for correctly updating the sample mean, and a forgetting factor to ensure less modeling power is expended fitting older observations. Both of these features contribute measurably to improving overall tracking performance. Numerous experiments demonstrate the effectiveness of the proposed tracking algorithm in indoor and outdoor environments where the target objects undergo large changes in pose, scale, and illumination.
}
}

@Article{10.1186/s40537-015-0031-2,
    author = {Olshannikova, Ekaterina and Ometov, Aleksandr and Koucheryavy, Yevgeni and Olsson, Thomas},
    title = {Visualizing Big Data with augmented and virtual reality: challenges and research agenda},
    journal = {Journal of Big Data},
    volume = {2.0},
    issue = {1},
    doi = {10.1186/s40537-015-0031-2},
    year = {2015},
    url = {http://link.springer.com/article/10.1186/s40537-015-0031-2},
    abstract = {This paper provides a multi-disciplinary overview of the research issues and achievements in the field of Big Data and its visualization techniques and tools. The main aim is to summarize challenges in visualization methods for existing Big Data, as well as to offer novel solutions for issues related to the current state of Big Data Visualization. This paper provides a classification
 of existing data types, analytical methods, visualization techniques and tools, with a particular emphasis placed on surveying the evolution of visualization methodology over the past years. Based on the results, we reveal disadvantages of existing visualization methods. Despite the technological development of the modern world, human involvement (interaction), judgment and logical thinking are necessary while working with Big Data. Therefore, the role of human perceptional limitations involving large amounts of information is evaluated. Based on the results, a non-traditional approach is proposed: we discuss how the capabilities of Augmented Reality and Virtual Reality could be applied to the field of Big Data Visualization. We discuss the promising utility of Mixed Reality technology integration with applications in Big Data Visualization. Placing the most essential data in the central area of the human visual field in Mixed Reality would allow one to obtain the presented information in a short period of time without significant data losses due to human perceptual issues. Furthermore, we discuss the impacts of new technologies, such as Virtual Reality displays and Augmented Reality helmets on the Big Data visualization as well as to the classification of the main challenges of integrating the technology.}
}

@Article{10.1007/s10015-009-0669-y,
    author = {Aly, Saleh and Tsuruta, Naoyuki and Taniguchi, Rin-ichiro},
    title = {Feature map sharing hypercolumn model for shift invariant face recognition},
    journal = {Artificial Life and Robotics},
    volume = {14.0},
    issue = {2},
    doi = {10.1007/s10015-009-0669-y},
    year = {2009},
    url = {http://link.springer.com/article/10.1007/s10015-009-0669-y},
    abstract = {In this article, we propose a shift-invariant pattern recognition mechanism using a feature-sharing hypercolumn model (FSHCM). To improve the recognition rate and to reduce the memory requirements of the hypercolumn model (HCM), a shared map is constructed to replace a set of local neighborhood maps in the feature extraction and feature integration layers. The shared maps increase the ability of the network to deal with translation and distortion variations in the input image. The proposed face recognition system employed a FSHCM neural network to perform feature extraction and use a linear support vector machine for a recognition task. The effectiveness of the proposed approach is verified by measuring the recognition accuracy using the misaligned ORL face database.}
}

@Article{10.1007/s00138-013-0525-x,
    author = {Oh, Sangmin and Mc, Scott and Closkey and Kim, Ilseo and Vahdat, Arash and Cannons, Kevin J. and Hajimirsadeghi, Hossein and Mori, Greg and Perera, A. G. Amitha and Pandey, Megha and Corso, Jason J.},
    title = {Multimedia event detection with multimodal feature fusion and temporal concept localization},
    journal = {Machine Vision and Applications},
    volume = {25.0},
    issue = {1},
    doi = {10.1007/s00138-013-0525-x},
    year = {2014},
    url = {http://link.springer.com/article/10.1007/s00138-013-0525-x},
    abstract = {We present a system for multimedia event detection. The developed system characterizes complex multimedia events based on a large array of multimodal features, and classifies unseen videos by effectively fusing diverse responses. We present three major technical innovations. First, we explore novel visual and audio features across multiple semantic granularities, including building, often in an unsupervised manner, mid-level and high-level features upon low-level features to enable semantic understanding. Second, we show a novel Latent SVM model which learns and localizes discriminative high-level concepts in cluttered video sequences. In addition to improving detection accuracy beyond existing approaches, it enables a unique summary for every retrieval by its use of high-level concepts and temporal evidence localization. The resulting summary provides some transparency into why the system classified the video as it did. Finally, we present novel fusion learning algorithms and our methodology to improve fusion learning under limited training data condition. Thorough evaluation on a large TRECVID MED 2011 dataset showcases the benefits of the presented system.}
}

@Article{10.1007/s11263-017-1001-2,
    author = {Jayaraman, Dinesh and Grauman, Kristen},
    title = {Learning Image Representations Tied to Egomotion from Unlabeled Video},
    journal = {International Journal of Computer Vision},
    volume = {125.0},
    issue = {1 - 3},
    doi = {10.1007/s11263-017-1001-2},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11263-017-1001-2},
    abstract = {Understanding how images of objects and scenes behave in response to specific egomotions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose a new “embodied” visual learning paradigm, exploiting proprioceptive motor signals to train visual representations from egocentric video with no manual supervision. Specifically, we enforce that our learned features exhibit equivariance i.e., they respond predictably to transformations associated with distinct egomotions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.}
}

@Article{10.1007/s00779-014-0769-0,
    author = {Kim, Seokhwan and Takahashi, Shin and Tanaka, Jiro},
    title = {A location-sensitive visual interface on the palm: interacting with common objects in an augmented space},
    journal = {Personal and Ubiquitous Computing},
    volume = {19.0},
    issue = {1},
    doi = {10.1007/s00779-014-0769-0},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s00779-014-0769-0},
    abstract = {We have created a visual interface
 using the human palm that is location sensitive and always available. To accomplish this, we constructed an augmented space in an actual workspace by installing several depth cameras. To manage and connect the multiple depth cameras, we constructed a distributed system based on scalable client––server architecture. By merging depth images from different cameras, the distributed system can track the locations of users within their area of coverage. The system also has a convenient feature that allows users to collect the locations of objects while visualizing the objects via images from the depth cameras. Consequently, the locations of both users and objects are available to the system, thus providing a location-based context for determining which user is close to which object. As a result, the visual interface on the palm becomes location sensitive, which could lead to various applications in daily life. In this paper, we describe the implementation of the aforementioned system and demonstrate its potential applicability.}
}

@Article{10.1007/s11554-007-0044-y,
    author = {Felsberg, Michael and Hedborg, Johan},
    title = {Real-time view-based pose recognition and interpolation for tracking initialization},
    journal = {Journal of Real-Time Image Processing},
    volume = {2.0},
    issue = {2 - 3},
    doi = {10.1007/s11554-007-0044-y},
    year = {2007},
    url = {http://link.springer.com/article/10.1007/s11554-007-0044-y},
    abstract = {In this paper we propose a new approach to real-time view-based pose recognition and interpolation. Pose recognition is particularly useful for identifying camera views in databases, video sequences, video streams, and live recordings. All of these applications require a fast pose recognition process, in many cases video real-time. It should further be possible to extend the database with new material, i.e., to update the recognition system online. The method that we propose is based on P-channels, a special kind of information representation which combines advantages of histograms and local linear models. Our approach is motivated by its similarity to information representation in biological systems but its main advantage is its robustness against common distortions such as clutter and occlusion. The recognition algorithm consists of three steps: (1) low-level image features for color and local orientation are extracted in each point of the image; (2) these features are encoded into P-channels by combining similar features within local image regions; (3) the query P-channels are compared to a set of prototype P-channels in a database using a least-squares approach. The algorithm is applied in two scene registration experiments with fisheye camera data, one for pose interpolation from synthetic images and one for finding the nearest view in a set of real images. The method compares favorable to SIFT-based methods, in particular concerning interpolation. The method can be used for initializing pose-tracking systems, either when starting the tracking or when the tracking has failed and the system needs to re-initialize. Due to its real-time performance, the method can also be embedded directly into the tracking system, allowing a sensor fusion unit choosing dynamically between the frame-by-frame tracking and the pose recognition.}
}

@Article{10.1007/s11042-018-5730-1,
    author = {Avola, Danilo and Cinque, Luigi and Foresti, Gian Luca and Marini, Marco Raoul and Pannone, Daniele},
    title = {VRheab: a fully immersive motor rehabilitation system based on recurrent neural network},
    journal = {Multimedia Tools and Applications},
    volume = {77.0},
    issue = {19},
    doi = {10.1007/s11042-018-5730-1},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-5730-1},
    abstract = {In this paper, a fully immersive serious game system that combines two Natural User Interfaces (NUIs) and a Head Mounted Display (HMD) to provide an interactive Virtual Environment (VE) for patient rehabilitation is proposed. Patients’ data are acquired in real-time by the NUIs, while by the HMD the VE is shown to them, thus allowing the interaction. A Long Short-Term Memory Recurrent Neural Network (LSTM-RNN), previously trained by healthy subjects (i.e., baseline), processes patients’ movements in real-time during the rehabilitation exercises to provide the degree of their performance. By comparing the functionalities of the proposed system with the ongoing state-of-the-art, it is worth noting that the reported fully immersive serious game system provides a concrete contribute to the current literature in terms of completeness and versatility. The results obtained by three rehabilitation exercises, chosen as reference case studies, performed on real patients affected by Parkinson’s disease have shown the effectiveness of the presented approach. Finally, the analysis of the feedbacks received by the therapists and patients who have used the system have highlighted remarkable results in terms of motivation, acceptance, and usability.}
}

@Article{10.1007/s11390-017-1751-x,
    author = {Ou, Xin-Yu and Li, Ping and Ling, He-Fei and Liu, Si and Wang, Tian-Jiang and Li, Dan},
    title = {Objectness Region Enhancement Networks for Scene Parsing},
    journal = {Journal of Computer Science and Technology},
    volume = {32.0},
    issue = {4},
    doi = {10.1007/s11390-017-1751-x},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11390-017-1751-x},
    abstract = {Semantic segmentation has recently witnessed rapid progress, but existing methods only focus on identifying objects or instances. In this work, we aim to address the task of semantic understanding of scenes with deep learning. Different from many existing methods, our method focuses on putting forward some techniques to improve the existing algorithms, rather than to propose a whole new framework. Objectness enhancement is the first effective technique. It exploits the detection module to produce object region proposals with category probability, and these regions are used to weight the parsing feature map directly. “Extra background” category, as a specific category, is often attached to the category space for improving parsing result in semantic and instance segmentation tasks. In scene parsing tasks, extra background category is still beneficial to improve the model in training. However, some pixels may be assigned into this nonexistent category in inference. Black-hole filling technique is proposed to avoid the incorrect classification. For verifying these two techniques, we integrate them into a parsing framework for generating parsing result. We call this unified framework as Objectness Enhancement Network (OENet). Compared with previous work, our proposed OENet system effectively improves the performance over the original model on SceneParse150 scene parsing dataset, reaching 38.4 mIoU (mean intersectionover-union) and 77.9\% accuracy in the validation set without assembling multiple models. Its effectiveness is also verified on the Cityscapes dataset.}
}

@Article{10.1007/s10044-013-0354-6,
    author = {Filipe, Sílvio and Alexandre, Luís A.},
    title = {Algorithms for invariant long-wave infrared face segmentation: evaluation and comparison},
    journal = {Pattern Analysis and Applications},
    volume = {17.0},
    issue = {4},
    doi = {10.1007/s10044-013-0354-6},
    year = {2014},
    url = {http://link.springer.com/article/10.1007/s10044-013-0354-6},
    abstract = {This paper 
presents two methods for automatic segmentation of images of faces captured in long wavelength infrared, allowing a wide range of face
 rotations, expressions and artifacts (such as glasses and hats). We also present the validation of segmentation results using a recognition method to show the impact of the segmentation accuracy on the recognition. The paper presents two different approaches (one aimed at real-time performance and the other at high accuracy) and compares their performance against three other previously published methods. The proposed approaches are based on statistical modeling of pixel intensities and active contour application, although several other image processing operations are also performed. Experiments were performed on a total of 893 test images from four public available databases. The obtained results improve on previous existing methods up to 29.5 \% for the first measure error (E 1) and up to 34.7 \% for the second measure (E 2), depending on the method and database. Regarding the computational time, our proposals improve up to 63.32 \% when compared with the other proposals. We also present the validation of the various segmentation methods that are presented by applying a face recognition method.}
}

@Article{10.1631/FITEE.1700808,
    author = {Zhang, Quan-shi and Zhu, Song-chun},
    title = {Visual interpretability for deep learning: a survey},
    journal = {Frontiers of Information Technology \\& Electronic Engineering},
    volume = {19.0},
    issue = {1},
    doi = {10.1631/FITEE.1700808},
    year = {2018},
    url = {http://link.springer.com/article/10.1631/FITEE.1700808},
    abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.}
}

@Article{10.1007/s11042-016-3394-2,
    author = {Tawadrous, Mina and Rojas, David and Kapralos, Bill and Hogue, Andrew and Dubrowski, Adam},
    title = {The effects of stereoscopic 3D on knowledge retention within a serious gaming environment},
    journal = {Multimedia Tools and Applications},
    volume = {76.0},
    issue = {5},
    doi = {10.1007/s11042-016-3394-2},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11042-016-3394-2},
    abstract = {We present the results of an experiment that investigated the effects of stereoscopic 3D viewing on knowledge retention with respect to a spatial interactive task within a serious game that was designed for fire safety training. Participants were trained to identify the safe distance to remain from a (virtual) fire in both stereoscopic 3D and non-stereoscopic 3D contexts. After a 24 h period, they were then tested to determine whether they retained the information that they were taught. Contrary to prior work that suggests stereoscopic 3D has an impact on knowledge retention, our results indicate no significant difference between knowledge retention in a stereoscopic 3D versus a non-stereoscopic 3D interactive environment. Although greater work remains to be done and no firm conclusions can be made regarding the use of stereoscopic 3D, our results have shown that stereoscopic 3D does not always lead to greater performance. Our results have implications for designers of serious games; the discussion and decision to use stereoscopic 3D should be incorporated early in the design phase and there should be some consideration placed on individualized calibration of stereoscopic 3D settings.}
}

@Article{10.1007/s11042-018-6458-7,
    author = {Khoh, Wee How and Pang, Ying Han and Teoh, Andrew Beng Jin},
    title = {In-air hand gesture signature recognition system based on 3-dimensional imagery},
    journal = {Multimedia Tools and Applications},
    doi = {10.1007/s11042-018-6458-7},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-6458-7},
    abstract = {A traditional online handwritten signature recognition system requires direct contact to acquisition device and usually will leave a traceable print on the surface. This made a signature possible and vulnerable to certain attempts of tracking and imitated. Looking into this shortfall, this paper proposes a novel approach to recognise an individual based on his/ her in-air hand motion while signing his/her signature. In this study, a low-cost acquisition device – Microsoft Kinect sensor is adopted to capture an image sequence of hand gesture signature. Palm region is first located and segmented through a predictive palm segmentation algorithm, which are then combined to generate a volume data. The volume data is condensed and reduced into a motion representation image by means of Motion History Image (MHI), which produces rich motion and temporal information. Several features are extracted from the MHI for empirical evaluation. Two classical recognition modes – identification and verification, are testified with an in-house database (HGS database). The proposed system achieves 90.4\% identification accuracy and 3.22\% equal error rate in verification mode. The experimental results substantiated the potential of the proposed system.}
}

@Article{10.1007/s10044-018-0741-0,
    author = {Nowosielski, Adam and Forczmański, Paweł},
    title = {Touchless typing with head movements captured in thermal spectrum},
    journal = {Pattern Analysis and Applications},
    doi = {10.1007/s10044-018-0741-0},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s10044-018-0741-0},
    abstract = {Many physically challenged people are unable to operate standard electronic equipment or computer input devices. They need special assistive technologies and one of the options is the head operated interface. Face-oriented algorithms often assume a particular level of lighting with adequate intensity and spatial configuration. In the paper, we propose a thermal-imaging-based algorithm of head operating typing. It does not assume the visible light illumination. We investigated, in context of thermal imagery, several contemporary general purpose object detectors known to be accurate in case of images captured by the visible light camera. Then, a selected face detector is employed in the head operated interface analysing head movements in the thermal spectrum. The attention has been focused on the problem of touchless typing which is performed in the existing solutions either through the camera mouse or through traverse procedure with an addition mechanism (like eye blink or mouth open) needed for clicking events in both cases. Our novel solution for touchless typing with head movements combines the thermal imaging for capturing user action with the hierarchical letter selection procedure. The solution employed allows to reach any alphabet character in just three steps, i.e. with directional head movements, without the need of any additional mechanisms for clicking events.
}
}

@Article{10.1007/BF02944908,
    author = {Liao, Pin and Shen, Li},
    title = {Unified probabilistic models for face recognition from a single example image per person},
    journal = {Journal of Computer Science and Technology},
    volume = {19.0},
    issue = {3},
    doi = {10.1007/BF02944908},
    year = {2004},
    url = {http://link.springer.com/article/10.1007/BF02944908},
    abstract = {This paper presents a new technique of unified probabilistic models for face recognition from only one single example image per person. The unified models, trained on an obtained training set with multiple samples per person, are used to recognize facial images from another disjoint database with a single sample per person. Variations between facial images are modeled as two unified probabilistic models: within-class variations and between-class variations. Gaussian Mixture Models are used to approximate the distributions of the two variations and exploit a classifier combination method to improve the performance. Extensive experimental results on the ORL face database and the authors' database (the ICT-JDL database) including totally 1,750 facial images of 350 individuals demonstrate that the proposed technique, compared with traditional eigenface method and some well-known traditional algorithms, is a significantly more effective and robust approach, for face recognition.}
}

@Article{10.1007/s11042-015-2684-4,
    author = {Dziech, Andrzej and Baran, Remigiusz and Leszczuk, Mikołaj},
    title = {Guest Editorial: Intelligent Processing for Citizen Security},
    journal = {Multimedia Tools and Applications},
    volume = {74.0},
    issue = {12},
    doi = {10.1007/s11042-015-2684-4},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s11042-015-2684-4}
}

@Article{10.1023/A:1023709501986,
    author = {la Torre, Fernando De and Black, Michael J.},
    title = {A Framework for Robust Subspace Learning},
    journal = {International Journal of Computer Vision},
    volume = {54.0},
    issue = {1 - 3},
    doi = {10.1023/A:1023709501986},
    year = {2003},
    url = {http://link.springer.com/article/10.1023/A:1023709501986},
    abstract = {Many computer vision, signal processing and statistical problems can be posed as problems of learning low dimensional linear or multi-linear models. These models have been widely used for the representation of shape, appearance, motion, etc., in computer vision applications. Methods for learning linear models can be seen as a special case of subspace fitting. One draw-back of previous learning methods is that they are based on least squares estimation techniques and hence fail to account for “outliers” which are common in realistic training sets. We review previous approaches for making linear learning methods robust to outliers and present a new method that uses an intra-sample outlier process to account for pixel outliers. We develop the theory of Robust Subspace Learning (RSL) for linear models within a continuous optimization framework based on robust M-estimation. The framework applies to a variety of linear learning problems in computer vision including eigen-analysis and structure from motion. Several synthetic and natural examples are used to develop and illustrate the theory and applications of robust subspace learning in computer vision.}
}

@Article{10.1007/s11042-012-0997-0,
    author = {Nam, Yunyoung and Rho, Seungmin and Park, Jong Hyuk},
    title = {Inference topology of distributed camera networks with multiple cameras},
    journal = {Multimedia Tools and Applications},
    volume = {67.0},
    issue = {1},
    doi = {10.1007/s11042-012-0997-0},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s11042-012-0997-0},
    abstract = {This paper proposes an inference method to construct the topology of a camera network with overlapping and non-overlapping fields of view for a commercial surveillance system equipped with multiple cameras. It provides autonomous object detection, tracking and recognition in indoor or outdoor urban environments. The camera network topology is estimated from object tracking results among and within FOVs. The merge-split method is used for object occlusion in a single camera and an EM-based approach for extracting the accurate object feature to track moving people and establishing object correspondence across multiple cameras. The appearance of moving people and the transition time between entry and exit zones is measured to track moving people across blind regions of multiple cameras with non-overlapping FOVs. Our proposed method graphically represents the camera network topology, as an undirected weighted graph using the transition probabilities and 8-directional chain code. The training phase and the test were run with eight cameras to evaluate the performance of our method. The temporal probability distribution and the undirected weighted graph are shown in the experiments.}
}

@Article{10.1007/s00371-011-0642-3,
    author = {Shen, Jianbing and Zhao, Ying and He, Ying},
    title = {Detail-preserving exposure fusion using subband architecture},
    journal = {The Visual Computer},
    volume = {28.0},
    issue = {5},
    doi = {10.1007/s00371-011-0642-3},
    year = {2012},
    url = {http://link.springer.com/article/10.1007/s00371-011-0642-3},
    abstract = {In this paper, we present a novel detail-preserving fusion approach from multiple exposure images using subband architecture. Given a sequence of different exposures, the Quadrature Mirror Filter (QMF) based subband architecture is first employed to decompose the original sequence into different frequency subbands. After that, we compute the importance weight maps according to the image appearance measurements, such as exposure, contrast, and saturation. In order to preserve the details of the subband signals, we compute the gain control maps and improve these subbands. Finally, the coefficients of subbands are blended into a high-quality detail-preserving fusion image. Experimental results demonstrate that the proposed approach successfully creates a visually pleasing exposure fusion image.}
}

@Article{10.1007/s11042-018-5837-4,
    author = {Banitalebi-Dehkordi, Amin and Nasiopoulos, Panos},
    title = {Saliency inspired quality assessment of stereoscopic 3D video},
    journal = {Multimedia Tools and Applications},
    volume = {77.0},
    issue = {19},
    doi = {10.1007/s11042-018-5837-4},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-5837-4},
    abstract = {To study the visual attentional behavior of Human Visual System (HVS) on 3D content, eye tracking experiments are performed and Visual Attention Models (VAMs) are designed. One of the main applications of these VAMs is in quality assessment of 3D video. The usage of 2D VAMs in designing 2D quality metrics is already well explored. This paper investigates the added value of incorporating 3D VAMs into Full-Reference (FR) and No-Reference (NR) quality assessment metrics for stereoscopic 3D video. To this end, state-of-the-art 3D VAMs are integrated to quality assessment pipeline of various existing FR and NR stereoscopic video quality metrics. Performance evaluations using a large scale database of stereoscopic videos with various types of distortions demonstrated that using saliency maps generally improves the performance of the quality assessment task for stereoscopic video. However, depending on the type of distortion, utilized metric, and VAM, the amount of improvement will change.}
}

@Article{10.1007/s00138-007-0077-z,
    author = {Dee, Hannah M. and Velastin, Sergio A.},
    title = {How close are we to solving the problem of automated visual surveillance?},
    journal = {Machine Vision and Applications},
    volume = {19.0},
    issue = {5 - 6},
    doi = {10.1007/s00138-007-0077-z},
    year = {2008},
    url = {http://link.springer.com/article/10.1007/s00138-007-0077-z},
    abstract = {The problem of automated visual surveillance has spawned a lively research area, with 2005 seeing three conferences or workshops and special issues of two major journals devoted to the topic. These alone are responsible for somewhere in the region of 240 papers and posters on automated visual surveillance before we begin to count those presented in more general fora. Many of these systems and algorithms perform one small sub-part of the surveillance task, such as motion detection. But even with low level image processing tasks it is often difficult to compare systems on the basis of published results alone. This review paper aims to answer the difficult question “How close are we to developing surveillance related systems which are really useful?” The first section of this paper considers the question of surveillance in the real world: installations, systems and practises. The main body of the paper then considers existing computer vision techniques with an emphasis on higher level processes such as behaviour modelling and event detection. We conclude with a review of the evaluative mechanisms that have grown from within the computer vision community in an attempt to provide some form of robust evaluation and cross-system comparability.}
}

@Article{10.1007/s11432-015-5516-2,
    author = {Tan, Zhongwei and Yang, Chuanchuan and Li, Yuliang and Yan, Yan and He, Changhong and Wang, Xinyue and Wang, Ziyu},
    title = {A low-complexity sensor fusion algorithm based on a fiber-optic gyroscope aided camera pose estimation system},
    journal = {Science China Information Sciences},
    volume = {59.0},
    issue = {4},
    doi = {10.1007/s11432-015-5516-2},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s11432-015-5516-2},
    abstract = {Visual tracking, as a popular computer vision technique, has a wide range of applications, such as camera pose estimation. Conventional methods for it are mostly based on vision only, which are complex for image processing due to the use of only one sensor. This paper proposes a novel sensor fusion algorithm fusing the data from the camera and the fiber-optic gyroscope. In this system, the camera acquires images and detects the object directly at the beginning of each tracking stage; while the relative motion between the camera and the object measured by the fiber-optic gyroscope can track the object coordinate so that it can improve the effectiveness of visual tracking. Therefore, the sensor fusion algorithm presented based on the tracking system can overcome the drawbacks of the two sensors and take advantage of the sensor fusion to track the object accurately. In addition, the computational complexity of our proposed algorithm is obviously lower compared with the existing approaches (86\% reducing for a 0.5 min visual tracking). Experiment results show that this visual tracking system reduces the tracking error by 6.15\% comparing with the conventional vision-only tracking scheme (edge detection), and our proposed sensor fusion algorithm can achieve a long-term tracking with the help of bias drift suppression calibration.}
}

@Article{10.1007/s12193-015-0204-5,
    author = {Liu, Mengyi and Wang, Ruiping and Li, Shaoxin and Huang, Zhiwu and Shan, Shiguang and Chen, Xilin},
    title = {Video modeling and learning on Riemannian manifold for emotion recognition in the wild},
    journal = {Journal on Multimodal User Interfaces},
    volume = {10.0},
    issue = {2},
    doi = {10.1007/s12193-015-0204-5},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s12193-015-0204-5},
    abstract = {In this paper, we present the method for our submission to the emotion recognition in the wild challenge (EmotiW). The challenge is to automatically classify the emotions acted by human subjects in video clips under real-world environment. In our method, each video clip can be represented by three types of image set models (i.e. linear subspace, covariance matrix, and Gaussian distribution) respectively, which can all be viewed as points residing on some Riemannian manifolds. Then different Riemannian kernels are employed on these set models correspondingly for similarity/distance measurement. For classification, three types of classifiers, i.e. kernel SVM, logistic regression, and partial least squares, are investigated for comparisons. Finally, an optimal fusion of classifiers learned from different kernels and different modalities (video and audio) is conducted at the decision level for further boosting the performance. We perform extensive evaluations on the EmotiW 2014 challenge data (including validation set and blind test set), and evaluate the effects of different components in our pipeline. It is observed that our method has achieved the best performance reported so far. To further evaluate the generalization ability, we also perform experiments on the EmotiW 2013 data and two well-known lab-controlled databases: CK+ and MMI. The results show that the proposed framework significantly outperforms the state-of-the-art methods.}
}

@Article{10.1007/BF02910056,
    author = {Karpov, Alexey and Ronzhin, Andrey},
    title = {ICANDO: Low cost multimodal interface for hand disabled people},
    journal = {Journal on Multimodal User Interfaces},
    volume = {1.0},
    issue = {2},
    doi = {10.1007/BF02910056},
    year = {2007},
    url = {http://link.springer.com/article/10.1007/BF02910056},
    abstract = {The article presents the multimodal user interface ICANDO (Intellectual Computer AssistaNt for Disabled Operators) that was awarded with the first prize of the Loco Mummy Software Contest in 2006. The interface is intended mainly for assistance to the persons without hands or with disabilities of their hands or arms but could be useful for ordinary users at hands-free contactless human-computer interaction too. It combines the module for automatic recognition of voice commands in English, French and Russian as well as the head tracking module in one multimodal interface. ICANDO interface was applied for hands-free work with Graphical User Interface of a personal computer in such tasks as Internet communication and work with graphical and text documents. The article describes the aim and the architecture of the interface, the methods for speech recognition and head tracking, information fusion and synchronization of the multimodal streams. The presented results of testing and exploitation of ICANDO user interface have confirmed high accuracy and robustness of the interface for contactless operation with a computer. The comparison of multimodal and standard ways of interaction has discovered that the first one is slower by a factor of 1.9 that is quite well for hands-free interaction between a computer and an impaired person.}
}

@Article{10.1007/BF00158168,
    author = {Beveridge, J. Ross and Griffith, Joey and Kohler, Ralf R. and Hanson, Allen R. and Riseman, Edward M.},
    title = {Segmenting images using localized histograms and region merging},
    journal = {International Journal of Computer Vision},
    volume = {2.0},
    issue = {3},
    doi = {10.1007/BF00158168},
    year = {1989},
    url = {http://link.springer.com/article/10.1007/BF00158168},
    abstract = {A working system for segmenting images of complex scenes is presented. The system integrates techniques that have evolved out of many years of research in low-level image segmentation at the University of Massachusetts and elsewhere. This paper documents the result of this historical evolution. Segmentations produced by the system are used extensively in related image interpretation research.The system first produces segmentations based upon an analysis of spatially localized feature histograms. These initial segmentations are then simplified using a region merging algorithm. Parameter selection for the local histogram segmentation algorithm is facilitated by mapping the multidimensional parameter space to a one-dimensional parameter which regulates region fragmentation. An extension of this algorithm to multiple features is also presented. Experience with roughly 100 images from different domains has shown the system to be robust and effective. Samples of these results are included.}
}

@Article{10.1007/s00138-008-0153-z,
    author = {Tian, Ying-li and Brown, Lisa and Hampapur, Arun and Lu, Max and Senior, Andrew and Shu, Chiao-fe},
    title = {IBM smart surveillance system (S3): event based video surveillance system with an open and extensible framework},
    journal = {Machine Vision and Applications},
    volume = {19.0},
    issue = {5 - 6},
    doi = {10.1007/s00138-008-0153-z},
    year = {2008},
    url = {http://link.springer.com/article/10.1007/s00138-008-0153-z},
    abstract = {The increasing need for sophisticated surveillance systems and the move to a digital infrastructure has transformed surveillance into a large scale data analysis and management challenge. Smart surveillance systems use automatic image understanding techniques to extract information from the surveillance data. While the majority of the research and commercial systems have focused on the information extraction aspect of the challenge, very few systems have explored the use of extracted information in the search, retrieval, data management and investigation context. The IBM smart surveillance system (S3) is one of the few advanced surveillance systems which provides not only the capability to automatically monitor a scene but also the capability to manage the surveillance data, perform event based retrieval, receive real time event alerts thru standard web infrastructure and extract long term statistical patterns of activity. The IBM S3 is easily customized to fit the requirements of different applications by using an open-standards based architecture for surveillance.}
}

@Article{10.1007/s11042-015-3125-0,
    author = {Ioannidou, Anastasia and Apostolidis, Evlampios and Collyda, Chrysa and Mezaris, Vasileios},
    title = {A web-based tool for fast instance-level labeling of videos and the creation of spatiotemporal media fragments},
    journal = {Multimedia Tools and Applications},
    volume = {76.0},
    issue = {2},
    doi = {10.1007/s11042-015-3125-0},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11042-015-3125-0},
    abstract = {This paper presents a web-based interactive tool for time-efficient instance-level spatiotemporal labeling of videos, based on the re-detection of manually selected objects of interest that appear in them. The developed tool allows the user to select a number of instances of the object that will be used for annotating the video via detecting and spatially demarcating it in the video frames, and provide a short description about the selected object. These instances are given as input to the object re-detection module of the tool, which detects and spatially demarcates re-occurrences of the object in the video frames. The video segments that contain detected instances of the given object can be then considered as object-related media fragments, being annotated with the user-provided information about the object. A key component for building such a tool is the development of an algorithm that performs the re-detection of the object throughout the video frames. For this, the first part of this work presents our study on different approaches for object re-detection and the finally developed one, which combines the recently proposed BRISK descriptors with a descriptor matching strategy that relies on the LSH algorithm. Following, the second part of this work is dedicated to the description of the implemented tool, introducing the supported functionalities and demonstrating its use for object-specific labeling of videos. A set of experiments and a user study regarding the efficiency of the introduced object re-detection method and the performance of the developed tool indicate that the proposed framework can be used for accurate and time-efficient instance-based annotation of videos, and the creation of object-related spatiotemporal media fragments.}
}
