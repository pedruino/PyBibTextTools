@Chapter{10.1007/978-3-319-08651-4_9,
    author = {Tian, Yingli},
    title = {RGB-D Sensor-Based Computer Vision Assistive Technology for Visually Impaired Persons},
    journal = {Computer Vision and Machine Learning with RGB-D Sensors},
    doi = {10.1007/978-3-319-08651-4\_9},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-08651-4\_9},
    abstract = {A computer vision-based wayfinding and navigation aid can improve the mobility of blind and visually impaired people to travel independently. In this chapter, we focus on RGB-D sensor-based computer vision technologies in application to assist blind and visually impaired persons. We first briefly review the existing computer vision based assistive technology for the visually impaired. Then we provide a detailed description of the recent RGB-D sensor based assistive technology to help blind or visually impaired people. Next, we present the prototype system to detect and recognize stairs and pedestrian crosswalks based on RGB-D images. Since both stairs and pedestrian crosswalks are featured by a group of parallel lines, Hough transform is applied to extract the concurrent parallel lines based on the RGB (Red, Green, and Blue) channels. Then, the Depth channel is employed to recognize pedestrian crosswalks and stairs. The detected stairs are further identified as stairs going up (upstairs) and stairs going down (downstairs). The distance between the camera and stairs is also estimated for blind users. The detection and recognition results on our collected datasets demonstrate the effectiveness and efficiency of our developed prototype. We conclude the chapter by the discussion of the future directions.}
}

@Article{10.1007/s00371-013-0886-1,
    author = {Jafri, Rabia and Ali, Syed Abid and Arabnia, Hamid R. and Fatima, Shameem},
    title = {Computer vision-based object recognition for the visually impaired in an indoors environment: a survey},
    journal = {The Visual Computer},
    volume = {30.0},
    issue = {11},
    doi = {10.1007/s00371-013-0886-1},
    year = {2014},
    url = {http://link.springer.com/article/10.1007/s00371-013-0886-1},
    abstract = {Though several electronic assistive devices have been developed for the visually impaired in the past few decades, however, relatively few solutions have been devised to aid them in recognizing generic objects in their environment, particularly indoors. Nevertheless, research in this area is gaining momentum. Among the various technologies being utilized for this purpose, computer vision based solutions are emerging as one of the most promising options mainly due to their affordability and accessibility. This paper provides an overview of the various technologies that have been developed in recent years to assist the visually impaired in recognizing generic objects in an indoors environment with a focus on approaches based on computer vision. It aims to introduce researchers to the latest trends in this area as well as to serve as a resource for developers who wish to incorporate such solutions into their own work.}
}

@Article{10.1007/s12193-016-0235-6,
    author = {Bhowmick, Alexy and Hazarika, Shyamanta M.},
    title = {An insight into assistive technology for the visually impaired and blind people: state-of-the-art and future trends},
    journal = {Journal on Multimodal User Interfaces},
    volume = {11.0},
    issue = {2},
    doi = {10.1007/s12193-016-0235-6},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s12193-016-0235-6},
    abstract = {Assistive technology for the visually impaired and blind people is a research field that is gaining increasing prominence owing to an explosion of new interest in it from disparate disciplines. The field has a very relevant social impact on our ever-increasing aging and blind populations. While many excellent state-of-the-art accounts have been written till date, all of them are subjective in nature. We performed an objective statistical survey across the various sub-disciplines in the field and applied information analysis and network-theory techniques to answer several key questions relevant to the field. To analyze the field we compiled an extensive database of scientific research publications over the last two decades. We inferred interesting patterns and statistics concerning the main research areas and underlying themes, identified leading journals and conferences, captured growth patterns of the research field; identified active research communities and present our interpretation of trends in the field for the near future. Our results reveal that there has been a sustained growth in this field; from less than 50 publications per year in the mid 1990s to close to 400 scientific publications per year in 2014. Assistive Technology for persons with visually impairments is expected to grow at a swift pace and impact the lives of individuals and the elderly in ways not previously possible.}
}

@Article{10.1007/s13721-013-0026-x,
    author = {Yi, Chucai and Flores, Roberto W. and Chincha, Ricardo and Ying and Tian, Li},
    title = {Finding objects for assisting blind people},
    journal = {Network Modeling Analysis in Health Informatics and Bioinformatics},
    volume = {2.0},
    issue = {2},
    doi = {10.1007/s13721-013-0026-x},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s13721-013-0026-x},
    abstract = {Computer vision technology has been widely used for blind assistance, such as navigation and way finding. However, few camera-based systems are developed for helping blind or visually impaired people to find daily necessities. In this paper, we propose a prototype system of blind-assistant object finding by camera-based network and matching-based recognition. We collect a dataset of daily necessities and apply Speeded-Up Robust Features and Scale Invariant Feature Transform feature descriptors to perform object recognition. Experimental results demonstrate the effectiveness of our prototype system.}
}

@Chapter{10.1007/978-3-642-22098-2_77,
    author = {Lawson, Marc A. and Do, Ellen Yi-Luen and Marston, James R. and Ross, David A.},
    title = {Helping Hands versus ERSP Vision: Comparing Object Recognition Technologies for the Visually Impaired},
    journal = {HCI International 2011 – Posters’ Extended Abstracts},
    doi = {10.1007/978-3-642-22098-2\_77},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-22098-2\_77},
    abstract = {A major challenge for people with vision impairments ranging from severely low visual acuity to no light perception (NLP) is identifying or distinguishing the difference between objects of similar size and shape. For many of these individuals, locating and identifying specific objects can be an arduous task. This paper explores the design and evaluation of the “Helping Hand”: A radio frequency identification (RFID) glove that audibly identifies tagged objects. In this paper we describe the design of a wearable RFID apparatus used for object identification. We evaluated the effectiveness of the glove by conducting a three-arm randomized controlled study. In our experiment, we compare a baseline (no assistive device), RFID (Helping Hand) and computer vision (ERSP Vision Software) in identifying common household objects. We also administered a questionnaire to obtain subjective data about the usability experience of the participants. Our experimental results show a reduction in the amount of time required to identify objects when using the Helping Hand glove versus the other two methods.}
}

@Article{10.1007/s00138-012-0431-7,
    author = {Ying and Tian, Li and Yang, Xiaodong and Yi, Chucai and Arditi, Aries},
    title = {Toward a computer vision-based wayfinding aid for blind persons to access unfamiliar indoor environments},
    journal = {Machine Vision and Applications},
    volume = {24.0},
    issue = {3},
    doi = {10.1007/s00138-012-0431-7},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s00138-012-0431-7},
    abstract = {Independent travel is a well-known challenge for blind and visually impaired persons. In this paper, we propose a proof-of-concept computer vision-based wayfinding aid for blind people to independently access unfamiliar indoor environments. In order to find different rooms (e.g. an office, a laboratory, or a bathroom) and other building amenities (e.g. an exit or an elevator), we incorporate object detection with text recognition. First, we develop a robust and efficient algorithm to detect doors, elevators, and cabinets based on their general geometric shape, by combining edges and corners. The algorithm is general enough to handle large intra-class variations of objects with different appearances among different indoor environments, as well as small inter-class differences between different objects such as doors and door-like cabinets. Next, to distinguish intra-class objects (e.g. an office door from a bathroom door), we extract and recognize text information associated with the detected objects. For text recognition, we first extract text regions from signs with multiple colors and possibly complex backgrounds, and then apply character localization and topological analysis to filter out background interference. The extracted text is recognized using off-the-shelf optical character recognition software products. The object type, orientation, location, and text information are presented to the blind traveler as speech.}
}

@Chapter{10.1007/978-3-319-24702-1_9,
    author = {Yi, Chucai and Tian, Yingli},
    title = {Assistive Text Reading from Natural Scene for Blind Persons},
    journal = {Mobile Cloud Visual Media Computing},
    doi = {10.1007/978-3-319-24702-1\_9},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-24702-1\_9},
    abstract = {Text information serves as an understandable and comprehensive indicator, which plays a significant role in navigation and recognition in our daily lives. It is very difficult to access this valuable information for blind or visually impaired persons, in particular, in unfamiliar environments. With the development of computer vision technology and smart mobile applications, many assistive systems are developed to help blind or visually impaired persons in their daily lives. This chapter focuses on the methods of text reading from natural scene as well as their applications to assist people who are visually impaired. With the research work on accessibility for the disabled, the assistive text reading technique for the blind is implemented in mobile platform, such as smart phone, tablet, and other wearable device. The popularity and interconnection of mobile devices would provide more low-cost and convenient assistance for blind or visually impaired persons.}
}

@Chapter{10.1007/978-3-319-13365-2_16,
    author = {Bhowmick, Alexy and Prakash, Saurabh and Bhagat, Rukmani and Prasad, Vijay and Hazarika, Shyamanta M.},
    title = {IntelliNavi: Navigation for Blind Based on Kinect and Machine Learning},
    journal = {Multi-disciplinary Trends in Artificial Intelligence},
    doi = {10.1007/978-3-319-13365-2\_16},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-13365-2\_16},
    abstract = {This paper presents a wearable navigation assistive system for the blind and the visually impaired built with off-the-shelf technology. Microsoft Kinect’s on board depth sensor is used to extract Red, Green, Blue and Depth (RGB-D) data of the indoor environment. Speeded-Up Robust Features (SURF) and Bag-of-Visual-Words (BOVW) model is used to extract features and reduce generic indoor object detection into a machine learning problem. A Support Vector Machine classifier is used to classify scene objects and obstacles to issue critical real-time information to the user through an external aid (earphone) for safe navigation. We performed a user-study with blind-fold users to measure the efficiency of the overall framework.}
}

@Chapter{10.1007/978-3-319-16199-0_45,
    author = {Thakoor, Kaveri and Mante, Nii and Zhang, Carey and Siagian, Christian and Weiland, James and Itti, Laurent and Medioni, Gérard},
    title = {A System for Assisting the Visually Impaired in Localization and Grasp of Desired Objects},
    journal = {Computer Vision - ECCV 2014 Workshops},
    doi = {10.1007/978-3-319-16199-0\_45},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-16199-0\_45},
    abstract = {A prototype wearable visual aid for helping visually impaired people find desired objects in their environment is described. The system is comprised of a head-worn camera to capture the scene, an Android phone interface to specify a desired object, and an attention-biasing-enhanced object recognition algorithm to identify three most likely object candidate regions, select the best-matching one, and pass its location to an object tracking algorithm. The object is tracked as the user’s head moves, and auditory feedback is provided to help the user maintain the object in the field of view, enabling easy reach and grasp. The implementation and integration of the system leading to testing of the working prototype with visually-impaired subjects at the Braille Institute in Los Angeles (demonstration in the accompanying video) is described. Results indicate that this system has clear potential to help visually-impaired users in achieving near-real-time object localization and grasp.}
}

@Article{10.1007/s11042-016-3617-6,
    author = {Tapu, Ruxandra and Mocanu, Bogdan and Zaharia, Titus},
    title = {A computer vision-based perception system for visually impaired},
    journal = {Multimedia Tools and Applications},
    volume = {76.0},
    issue = {9},
    doi = {10.1007/s11042-016-3617-6},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11042-016-3617-6},
    abstract = {In this paper, we introduce a novel computer vision-based perception system, dedicated to the autonomous navigation of visually impaired people. A first feature concerns the real-time detection and recognition of obstacles and moving objects present in potentially cluttered urban scenes. To this purpose, a motion-based, real-time object detection and classification method is proposed. The method requires no a priori information about the obstacle type, size, position or location. In order to enhance the navigation/positioning capabilities offered by traditional GPS-based approaches, which are often unreliably in urban environments, a building/landmark recognition approach is also proposed. Finally, for the specific case of indoor applications, the system has the possibility to learn a set of user-defined objects of interest. Here, multi-object identification and tracking is applied in order to guide the user to localize such objects of interest. The feedback is presented to user by audio warnings/alerts/indications. Bone conduction headphones are employed in order to allow visually impaired to hear the systems warnings without obstructing the sounds from the environment. At the hardware level, the system is totally integrated on an android smartphone which makes it easy to wear, non-invasive and low-cost.}
}

@Article{10.1007/s00779-015-0841-4,
    author = {Takizawa, Hotaka and Yamaguchi, Shotaro and Aoyagi, Mayumi and Ezaki, Nobuo and Mizuno, Shinji},
    title = {Kinect cane: an assistive system for the visually impaired based on the concept of object recognition aid},
    journal = {Personal and Ubiquitous Computing},
    volume = {19.0},
    issue = {5 - 6},
    doi = {10.1007/s00779-015-0841-4},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s00779-015-0841-4},
    abstract = {This paper proposes a novel concept to assist visually impaired individuals in recognizing three-dimensional objects in everyday environments. This concept is realized as a portable system that consists of a white cane, a Microsoft Kinect sensor, a numeric keypad, a tactile feedback device, and other components. By the use of the Kinect sensor, the system searches for an object that a visually impaired user instructs the system to find and then returns a searching result to the user via the tactile feedback device. The major advantage of the system is the ability to recognize the objects of various classes, such as chairs and staircases, out of detectable range of white canes. Furthermore, the system is designed to return minimum required information related to the instruction of a user so that the user can obtain necessary information more efficiently. The system is evaluated through two types of experiment: object recognition test and user study. The experimental results indicate that the system is promising as a means of helping visually impaired users recognize objects.}
}

@Chapter{10.1007/978-3-319-29133-8_25,
    author = {de Sousa Britto Neto, Laurindo and Maike, Vanessa Regina Margareth Lima and Koch, Fernando Luiz and Baranauskas, Maria Cecília Calani and de Rezende Rocha, Anderson and Goldenstein, Siome Klein},
    title = {A Wearable Face Recognition System Built into a Smartwatch and the Blind and Low Vision Users},
    journal = {Enterprise Information Systems},
    doi = {10.1007/978-3-319-29133-8\_25},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-29133-8\_25},
    abstract = {Assistive technologies need to be affordable, ergonomic and easy to use. In this work we argue that smartwatches could be assistive devices for the visually impaired, if they have the potential to run complex applications. Hence, in this paper we propose a face recognition system to show that it’s technically possible to develop a real-time computer vision system in a wearable device with limited hardware, since such systems generally require powerful hardware. A case study is presented using the first generation Samsung Galaxy Gear smartwatch. The system runs only on the watch’s hardware and consists in a face detection and recognition software that emits an audio feedback so that visually impaired users know who is around them. The case study includes an evaluation of the proposal with users. Results are shown and discussed validating the technological aspects of the proposal and pointing out room for improving the aspects of interaction.}
}

@Chapter{10.1007/978-3-319-23222-5_35,
    author = {Poggi, Matteo and Nanni, Luca and Mattoccia, Stefano},
    title = {Crosswalk Recognition Through Point-Cloud Processing and Deep-Learning Suited to a Wearable Mobility Aid for the Visually Impaired},
    journal = {New Trends in Image Analysis and Processing -- ICIAP 2015 Workshops},
    doi = {10.1007/978-3-319-23222-5\_35},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-23222-5\_35},
    abstract = {In smart-cities, computer vision has the potential to dramatically improve the quality of life of people suffering of visual impairments. In this field, we have been working on a wearable mobility aid aimed at detecting in real-time obstacles in front of a visually impaired. Our approach relies on a custom RGBD camera, with FPGA on-board processing, worn as traditional eyeglasses and effective point-cloud processing implemented on a compact and lightweight embedded computer. This latter device also provides feedback to the user by means of an haptic interface as well as audio messages. In this paper we address crosswalk recognition that, as pointed out by several visually impaired users involved in the evaluation of our system, is a crucial requirement in the design of an effective mobility aid. Specifically, we propose a reliable methodology to detect and categorize crosswalks by leveraging on point-cloud processing and deep-learning techniques. The experimental results reported, on 10000+ frames, confirm that the proposed approach is invariant to head/camera pose and extremely effective even when dealing with large occlusions typically found in urban environments.}
}

@Article{10.1007/s12193-015-0182-7,
    author = {Csapó, Ádám and Wersényi, György and Nagy, Hunor and Stockman, Tony},
    title = {A survey of assistive technologies and applications for blind users on mobile platforms: a review and foundation for research},
    journal = {Journal on Multimodal User Interfaces},
    volume = {9.0},
    issue = {4},
    doi = {10.1007/s12193-015-0182-7},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s12193-015-0182-7},
    abstract = {This paper summarizes recent developments in audio and tactile feedback based assistive technologies targeting the blind community. Current technology allows applications to be efficiently distributed and run on mobile and handheld devices, even in cases where computational requirements are significant. As a result, electronic travel aids, navigational assistance modules, text-to-speech applications, as well as virtual audio displays which combine audio with haptic channels are becoming integrated into standard mobile devices. This trend, combined with the appearance of increasingly user-friendly interfaces and modes of interaction has opened a variety of new perspectives for the rehabilitation and training of users with visual impairments. The goal of this paper is to provide an overview of these developments based on recent advances in basic research and application development. Using this overview as a foundation, an agenda is outlined for future research in mobile interaction design with respect to users with special needs, as well as ultimately in relation to sensor-bridging applications in general.}
}

@Article{10.1007/s10209-017-0550-z,
    author = {Huang, Hsinfu},
    title = {Blind users’ expectations of touch interfaces: factors affecting interface accessibility of touchscreen-based smartphones for people with moderate visual impairment},
    journal = {Universal Access in the Information Society},
    volume = {17.0},
    issue = {2},
    doi = {10.1007/s10209-017-0550-z},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s10209-017-0550-z},
    abstract = {Although current touchscreen-based smartphones are equipped with some accessibility functions for visually impaired people, these users still face substantial challenges, particularly when using smooth touchscreens. In this study, the accessibility factors of touchscreen interfaces for people with visual impairment were explored through principal components analysis. A total of 32 persons with moderate visual impairment and an average age of 35.6 years (SD = 1.62) participated. The accessibility requirements and user experiences of smartphone touchscreen interfaces were collected. The results indicate that touchscreen interfaces have six major accessibility factors. In addition, the operational style of touchscreen interfaces should be redesigned according to a two-stage process. Furthermore, the design guidelines for accessible touchscreen interfaces that meet the requirements of people with visual impairments are summarised. The findings provide an essential reference for product designers constructing accessible touchscreen interfaces, and reinforce the concept of equality in product use.}
}

@Chapter{10.1007/978-3-319-13461-1_4,
    author = {Ghantous, Milad and Nahas, Michel and Ghamloush, Maya and Rida, Maya},
    title = {iSee: An Android Application for the Assistance of the Visually Impaired},
    journal = {Advanced Machine Learning Technologies and Applications},
    doi = {10.1007/978-3-319-13461-1\_4},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-13461-1\_4},
    abstract = {Smart phone technology and mobile applications have become an indispensable part of our daily life. The primary use however, is targeted towards social media and photography. While some camera-based approaches provided partial solutions for the visually impaired, they still constitute a cumbersome process for the user. iSee is an Android based application that benefits from the commercially available technology to help the visually impaired people improve their day-to-day activities. A single screen tap in iSee is able to serve as a virtual eye by providing a sense of seeing to the blind person by audibly communicating the object(s) names and description. iSee employs efficient object recognition algorithms based on FAST and BRIEF. Implementation results are promising and allow iSee to constitute a basis for more advanced applications.}
}

@Article{10.1007/s11227-016-1891-8,
    author = {Jafri, Rabia},
    title = {A GPU-accelerated real-time contextual awareness application for the visually impaired on Google’s project Tango device},
    journal = {The Journal of Supercomputing},
    volume = {73.0},
    issue = {2},
    doi = {10.1007/s11227-016-1891-8},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11227-016-1891-8},
    abstract = {An application for the recently introduced Google Project Tango Tablet Development Kit to assist visually impaired (VI) users in understanding their environmental context by identifying and locating multiple faces and objects in their vicinity in real-time is presented. CUDA-based GPU-accelerated algorithms would be utilized to detect and recognize faces and objects from the visual data, while the locations of these entities relative to the user would be estimated from the depth data acquired via the tablet. The interaction would be speech based with the user being offered several options for requesting information about the identities and/or relative locations of face and objects. The aim is to create a portable, affordable, power-efficient, standalone assistive application to increase the autonomy of VI users which can run in real time on the device itself.}
}

@Chapter{10.1007/978-3-319-25903-1_63,
    author = {Carrato, Sergio and Fenu, Gianfranco and Medvet, Eric and Mumolo, Enzo and Pellegrino, Felice Andrea and Ramponi, Giovanni},
    title = {Towards More Natural Social Interactions of Visually Impaired Persons},
    journal = {Advanced Concepts for Intelligent Vision Systems},
    doi = {10.1007/978-3-319-25903-1\_63},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-25903-1\_63},
    abstract = {We review recent computer vision techniques with reference to the specific goal of assisting the social interactions of a person affected by very severe visual impairment or by total blindness. We consider a scenario in which a sequence of images is acquired and processed by a wearable device, and we focus on the basic tasks of detecting and recognizing people and their facial expression. We review some methodologies of Visual Domain Adaptation that could be employed to adapt existing classification strategies to the specific scenario. We also consider other sources of information that could be exploited to improve the performance of the system.}
}

@Chapter{10.1007/978-3-319-54407-6_20,
    author = {Rollend, Derek and Rosendall, Paul and Billings, Seth and Burlina, Philippe and Wolfe, Kevin and Katyal, Kapil},
    title = {Face Detection and Object Recognition for a Retinal Prosthesis},
    journal = {Computer Vision – ACCV 2016 Workshops},
    doi = {10.1007/978-3-319-54407-6\_20},
    year = {2017},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-54407-6\_20},
    abstract = {We describe the recent development of assistive computer vision algorithms for use with the Argus II retinal prosthesis system. While users of the prosthetic system can learn and adapt to the limited stimulation resolution, there exists great potential for computer vision algorithms to augment the experience and significantly increase the utility of the system for the user. To this end, our recent work has focused on helping with two different challenges encountered by the visually impaired: face detection and object recognition. In this paper, we describe algorithm implementations in both of these areas that make use of the retinal prosthesis for visual feedback to the user, and discuss the unique challenges faced in this domain.}
}

@Chapter{10.1007/978-3-319-07446-7_48,
    author = {Fernandes, Hugo and Costa, Paulo and Paredes, Hugo and Filipe, Vítor and Barroso, João},
    title = {Integrating Computer Vision Object Recognition with Location Based Services for the Blind},
    journal = {Universal Access in Human-Computer Interaction. Aging and Assistive Environments},
    doi = {10.1007/978-3-319-07446-7\_48},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-07446-7\_48},
    abstract = {The task of moving from one place to another is a difficult challenge that involves obstacle avoidance, staying on street walks, finding doors, knowing the current location and keeping on track through the desired path. Nowadays, navigation systems are widely used to find the correct path, or the quickest, between two places. While assistive technology has contributed to the improvement of the quality of life of people with disabilities, people with visual impairment still face enormous limitations in terms of their mobility. In recent years, several approaches have been made to create systems that allow seamless tracking and navigation both in indoor and outdoor environments. However there is still an enormous lack of availability of information that can be used to assist the navigation of users with visual impairments as well as a lack of sufficient precision in terms of the estimation of the user’s location. Blavigator is a navigation system designed to help users with visual impairments. In a known location, the use of object recognition algorithms can provide contextual feedback to the user and even serve as a validator to the positioning module and geographic information system of a navigation system for the visually impaired. This paper proposes a method where the use of computer vision algorithms validate the outputs of the positioning system of the Blavigator prototype.}
}

@Article{10.1007/s13721-013-0027-9,
    author = {Wang, Shuihua and Yang, Xiaodong and Tian, Yingli},
    title = {Detecting signage and doors for blind navigation and wayfinding},
    journal = {Network Modeling Analysis in Health Informatics and Bioinformatics},
    volume = {2.0},
    issue = {2},
    doi = {10.1007/s13721-013-0027-9},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s13721-013-0027-9},
    abstract = {Signage plays a very important role to find destinations in applications of navigation and wayfinding. In this paper, we propose a novel framework to detect doors and signage to help blind people accessing unfamiliar indoor environments. In order to eliminate the interference information and improve the accuracy of signage detection, we first extract the attended areas using a saliency map. Then the signage is detected in the attended areas using a bipartite graph matching. The proposed method can handle multiple signage detection. Furthermore, in order to provide more information for blind users to access the area associated with the detected signage, we develop a robust method to detect doors based on a geometric door frame model which is independent to door appearances. Experimental results on our collected datasets of indoor signage and doors demonstrate the effectiveness and efficiency of our proposed method.}
}

@Article{10.1007/s40595-016-0075-z,
    author = {Hoang, Van-Nam and Nguyen, Thanh-Huong and Le, Thi-Lan and Tran, Thanh-Hai and Vuong, Tan-Phu and Vuillerme, Nicolas},
    title = {Obstacle detection and warning system for visually impaired people based on electrode matrix and mobile Kinect},
    journal = {Vietnam Journal of Computer Science},
    volume = {4.0},
    issue = {2},
    doi = {10.1007/s40595-016-0075-z},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s40595-016-0075-z},
    abstract = {Obstacle detection and warning can improve the mobility as well as the safety of visually impaired people specially in unfamiliar environments. For this, firstly, obstacles are detected and localized and then the information of the obstacles will be sent to the visually impaired people by using different modalities such as voice, tactile, vibration. In this paper, we present an assistive system for visually impaired people based on the matrix of electrode and a mobile Kinect. This system consists of two main components: environment information acquisition and analysis and information representation. The first component aims at capturing the environment by using a mobile Kinect and analyzing it in order to detect the predefined obstacles for visually impaired people, while the second component tries to represent obstacle’s information under the form of electrode matrix.}
}

@Article{10.1007/s12193-015-0191-6,
    author = {Sövény, Bálint and Kovács, Gábor and Kardkovács, Zsolt T.},
    title = {Blind guide},
    journal = {Journal on Multimodal User Interfaces},
    volume = {9.0},
    issue = {4},
    doi = {10.1007/s12193-015-0191-6},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s12193-015-0191-6},
    abstract = {In this paper, we present a design of a wearable equipment that helps with the perception of the environment for visually impaired people in both indoor and outdoor mobility and navigation. Our prototype can detect and identify traffic situations such as street crossings, traffic lamps, cars, cyclists, other people and obstacles hanging down from above or placed on the ground. The detection takes place in real time based on input data of sensors and cameras, the mobility of the user is aided with audio signals.}
}

@Chapter{10.1007/978-3-319-08596-8_55,
    author = {Jafri, Rabia and Ali, Syed Abid},
    title = {A Multimodal Tablet–Based Application for the Visually Impaired for Detecting and Recognizing Objects in a Home Environment},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-319-08596-8\_55},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-08596-8\_55},
    abstract = {Object recognition solutions for the visually impaired based on a single modality cannot provide optimal performance under all circumstances, since each modality is best suited for particular usage scenarios. An object recognition application for the visually impaired, meant for a RFID-enabled tablet, which combines three approaches – RFID-based, visual-tag computer vision based and non-visual tag computer vision based – into a single piece of software is, therefore, presented in this paper. This solution has the benefits of being portable, accessible, low cost (the user needs only an RFID-enabled tablet, some inexpensive passive RFID tags and some visual tags (which can be printed out for free)) and more robust in a wider range of conditions than the approaches it is comprised of. The application will be adapted to other mobile platforms and devices (e.g., RFID-enabled smartphones) in the future.}
}

@Article{10.1186/s13673-018-0134-9,
    author = {Jafri, Rabia and Khan, Marwa Mahmoud},
    title = {User-centered design of a depth data based obstacle detection and avoidance system for the visually impaired},
    journal = {Human-centric Computing and Information Sciences},
    volume = {8.0},
    issue = {1},
    doi = {10.1186/s13673-018-0134-9},
    year = {2018},
    url = {http://link.springer.com/article/10.1186/s13673-018-0134-9},
    abstract = {The development of a novel depth-data based real-time obstacle detection and avoidance application for visually impaired (VI) individuals to assist them in navigating independently in indoors environments is presented in this paper. The application utilizes a mainstream, computationally efficient mobile device as the development platform in order to create a solution which not only is aesthetically appealing, cost-effective, lightweight and portable but also provides real-time performance and freedom from network connectivity constraints. To alleviate usability problems, a user-centered design approach has been adopted wherein semi-structured interviews with VI individuals in the local context were conducted to understand their micro-navigation practices, challenges and needs. The invaluable insights gained from these interviews have not only informed the design of our system but would also benefit other researchers developing similar applications. The resulting system design along with a detailed description of its obstacle detection and unique multimodal feedback generation modules has been provided. We plan to iteratively develop and test the initial prototype of the system with the end users to resolve any usability issues and better adapt it to their needs.}
}

@Chapter{10.1007/978-3-642-31534-3_85,
    author = {Tang, Hao and Zhu, Zhigang},
    title = {A Segmentation-Based Stereovision Approach for Assisting Visually Impaired People},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-642-31534-3\_85},
    year = {2012},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-31534-3\_85},
    abstract = {An accurate 3D map, automatically generated in real-time from a camera-based stereovision system, is able to assist blind or visually impaired people to obtain correct perception and recognition of the surrounding objects and environment so that they can move safely. In this paper, a segmentation-based stereovision approach is proposed to rapidly obtain accurate 3D estimations of man-made scenes, both indoor and outdoor, with largely textureless areas and sharp depth changes. The new approach takes advantage of the fact that many man-made objects in an urban environment consist of planar surfaces. The final outcome of the system is not just an array of individual 3D points. Instead, the 3D model is built in a geometric representation of plane parameters, with geometric relations among different planar surfaces. Based on this 3D model, algorithms can be developed for traversable path planning, obstacle detection and object recognition for assisting the blind in urban navigation.}
}

@Article{10.1007/s11042-015-3204-2,
    author = {Nguyen, Quoc-Hung and Vu, Hai and Tran, Thanh-Hai and Nguyen, Quang-Hoan},
    title = {Developing a way-finding system on mobile robot assisting visually impaired people in an indoor environment},
    journal = {Multimedia Tools and Applications},
    volume = {76.0},
    issue = {2},
    doi = {10.1007/s11042-015-3204-2},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11042-015-3204-2},
    abstract = {A way-finding system in an indoor environment consists of several components: localization, representation, path planning, and interaction. For each component, numerous relevant techniques have been proposed. However, deploying feasible techniques, particularly in real scenarios, remains challenging. In this paper, we describe a functional way-finding system deployed on a mobile robot to assist visual impairments (VI). The proposed system deploys state-of-the-art techniques that are adapted to the practical issues at hand. First, we adapt an outdoor visual odometry technique to indoor use by covering manual markers or stickers on ground-planes. The main purpose is to build reliable travel routes in the environment. Second, we propose a procedure to define and optimize the landmark/representative scenes of the environment. This technique handles the repetitive and ambiguous structures of the environment. In order to interact with VI people, we deploy a convenient interface on a smart phone. Three different indoor scenarios and thirteen subjects are conducted in our evaluations. Our experimental results show that VI people, particularly VI pupils, can find the right way to requested targets.}
}

@Chapter{10.1007/978-3-319-12568-8_70,
    author = {Domínguez, A. Rojas and Lara-Alvarez, Carlos and Bayro, Eduardo},
    title = {Automated Banknote Identification Method for the Visually Impaired},
    journal = {Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
    doi = {10.1007/978-3-319-12568-8\_70},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-12568-8\_70},
    abstract = {A novel method for automated identification of banknotes’ denominations based on image processing is presented. The method is part of a wearable aiding system for the visually impaired, and it uses a standard video camera as the image collecting device. The method first extracts points of interest from the denomination region of a banknote and then performs an analysis of the geometrical patterns so defined, which allows the identification of the banknote denomination. Experiments were performed with a test-subject in order to simulate real-world operating conditions. A high average identification rate was achieved.}
}

@Chapter{10.1007/978-3-642-31534-3_86,
    author = {Khan, Atif and Moideen, Febin and Lopez, Juan and Khoo, Wai L. and Zhu, Zhigang},
    title = {KinDectect: Kinect Detecting Objects},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-642-31534-3\_86},
    year = {2012},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-31534-3\_86},
    abstract = {Detecting humans and objects in images has been a very challenging problem due to variation in illumination, pose, clothing, background and other complexities. Depth information is an important cue when humans recognize objects and other humans. In this work we utilize the depth information that a Kinect sensor - Xtion Pro Live provides to detect humans and obstacles in real time for a blind or visually impaired user. The system runs in two modes. For the first mode, we focus on how to track and/or detect multiple humans and moving objects and transduce the information to the user. For the second mode, we present a novel approach on how to avoid obstacles for safe navigation for a blind or visually-impaired user in an indoor environment. In addition, we present a user study with some blind-folded users to measure the efficiency and robustness of our algorithms and approaches.}
}

@Chapter{10.1007/978-3-319-07854-0_67,
    author = {Al-Quwayfili, Norah I. and Al-Khalifa, Hend S.},
    title = {AraMedReader: An Arabic Medicine Identifier Using Barcodes},
    journal = {HCI International 2014 - Posters’ Extended Abstracts},
    doi = {10.1007/978-3-319-07854-0\_67},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-07854-0\_67},
    abstract = {AraMedScanner is a prototype application that mainly helps the visually impaired to identify medicines by scanning their barcode and retrieving their information from a medical database. This paper presents an overview of AraMedScanner’s features and shows preliminary evaluations conducted with blind people. The results of the evaluations revealed the application limitations and leaded to new future improvements.}
}

@Chapter{10.1007/978-3-642-19173-2_9,
    author = {Mata, Felix and Jaramillo, Andres and Claramunt, Christophe},
    title = {A Mobile Navigation and Orientation System for Blind Users in a Metrobus Environment},
    journal = {Web and Wireless Geographical Information Systems},
    doi = {10.1007/978-3-642-19173-2\_9},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-19173-2\_9},
    abstract = {The research presented in this paper introduces a mobile assistant to spatially locate and orient passengers of a Metrobus system in the city of Mexico. The system assists blind passengers or passengers with limited eyesight. While most of the navigation systems developed so far for blind people employ a complex conjunction of positioning systems, video cameras, location-based and image processing algorithms, we developed an affordable and low-cost combination of mobile phone, GPS and compass device to provide the same range of functionalities. The mobile application is developed on top of a 3rd generation cell phone extended by GPS and digital compass devices. Interaction among these devices is achieved by using Bluetooth communications. Audible user-oriented interfaces indicate to a given user her/his location within the Metrobus system, i.e., appropriate instructions of orientation that lead to the boarding gates in that station, thanks to an orientation algorithm developed for this aim. The experiments have been performed on a Nokia N73 cell phone in a Metrobus line of the city of Mexico and validated by a panel of blind users.}
}

@Chapter{10.1007/978-3-319-92049-8_25,
    author = {Oliveira, Juliana Damasio and Borges, Olimar Teixeira and Paixão-Cortes, Vanessa Stangherlin Machado and de Borba Campos, Marcia and Damasceno, Rafael Mendes},
    title = {LêRótulos: A Mobile Application Based on Text Recognition in Images to Assist Visually Impaired People},
    journal = {Universal Access in Human-Computer Interaction. Methods, Technologies, and Users},
    doi = {10.1007/978-3-319-92049-8\_25},
    year = {2018},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-92049-8\_25},
    abstract = {The autonomy of the visual impaired person can be evaluated in day to day activities like recognizing objects, identifying textual information, among others. This paper features the OCR technology-based LêRótulos application, with the objective of helping visually impaired users to identify textual object information that is captured by the camera of an smartphone. The design of the prototype followed guidelines and recommendations for usability and accessibility, aiming for greater user autonomy. There was an evaluation with specialists and end users, in real situations of use. The results indicated that the application has good usability and meets accessibility criteria for blind and low vision users. However, some improvements were indicated. Related work is presented, the LêRótulos design process, the results of usability and accessibility assessments, and lessons learned for the development of assistive technology aimed at visually impaired users.}
}

@Chapter{10.1007/978-981-10-3433-6_106,
    author = {Khade, Sanket and Dandawate, Yogesh H.},
    title = {Hardware Implementation of Obstacle Detection for Assisting Visually Impaired People in an Unfamiliar Environment by Using Raspberry Pi},
    journal = {Smart Trends in Information Technology and Computer Communications},
    doi = {10.1007/978-981-10-3433-6\_106},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-981-10-3433-6\_106},
    abstract = {For assisting blind or visually impaired persons, many computer vision technology has been developed. Some camera based systems were developed to help those people in way finding, navigation and finding daily necessities. The motion of the observer causes all scene object stationary or non-stationary in motion. And hence it is very much important to detect moving object with the moving observer. In this context we have proposed a camera based prototype system for assisting blind person in detection of obstacles by using motion vectors. We have collected dataset of their indoor and outdoor environment and estimated the optical flow to perform object detection. Furthermore we have detected the objects in the region of interest without using costly Depth cameras and sensors. The hardware used in the proposed work is ‘Raspberry Pi 2-B’ and the algorithms used for object detection is performed using MATLAB (for simulation purpose) and Python language.
}
}

@Chapter{10.1007/978-3-642-14100-3_38,
    author = {Ying and Tian, Li and Yi, Chucai and Arditi, Aries},
    title = {Improving Computer Vision-Based Indoor Wayfinding for Blind Persons with Context Information},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-642-14100-3\_38},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-14100-3\_38},
    abstract = {There are more than 161 million visually impaired people in the world today, of which 37 million are blind. Camera-based computer vision systems have the potential to assist blind persons to independently access unfamiliar buildings. Signs with text play a very important role in identification of bathrooms, exits, office doors, and elevators. In this paper, we present an effective and robust method of text extraction and recognition to improve computer vision-based indoor wayfinding. First, we extract regions containing text information from indoor signage with multiple colors and complex background and then identify text characters in the extracted regions by using the features of size, aspect ratio and nested edge boundaries. Based on the consistence of distances between two neighboring characters in a text string, the identified text characters have been normalized before they are recognized by using off-the-shelf optical character recognition (OCR) software products and output as speech for blind users.}
}

@Chapter{10.1007/978-3-319-67687-6_28,
    author = {Ruffieux, Simon and Ruffieux, Nicolas and Caldara, Roberto and Lalanne, Denis},
    title = {iKnowU – Exploring the Potential of Multimodal AR Smart Glasses for the Decoding and Rehabilitation of Face Processing in Clinical Populations},
    journal = {Human-Computer Interaction – INTERACT 2017},
    doi = {10.1007/978-3-319-67687-6\_28},
    year = {2017},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-67687-6\_28},
    abstract = {This article presents an explorative study with a smart glasses application developed to help visually impaired individuals to identify faces and facial expressions of emotion. The paper discusses three experiments in which different patients, suffering from distinct pathologies impairing vision, tested our application. These preliminary studies demonstrate the feasibility and usefulness of visual prostheses for face and emotion identification, and offer novel and interesting directions for future wearable see-through devices.}
}

@Chapter{10.1007/978-3-319-48881-3_25,
    author = {Brenner, Rorry and Priyadarshi, Jay and Itti, Laurent},
    title = {Perfect Accuracy with Human-in-the-Loop Object Detection},
    journal = {Computer Vision – ECCV 2016 Workshops},
    doi = {10.1007/978-3-319-48881-3\_25},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-48881-3\_25},
    abstract = {Modern state-of-the-art computer vision systems still perform imperfectly in many benchmark object recognition tasks. This hinders their application to real-time tasks where even a low but non-zero probability of error in analyzing every frame from a camera quickly accumulates to unacceptable performance for end users. Here we consider a visual aid to guide blind or visually-impaired persons in finding items in grocery stores using a head-mounted camera. The system uses a human-in-the-decision-loop approach to instruct the user how to turn or move when an object is detected with low confidence, to improve the object’s view captured by the camera, until computer vision confidence is higher than the highest mistaken confidence observed during algorithm training. In experiments with 42 blindfolded participants reaching for 25 different objects randomly arranged on shelves 15 times, our system was able to achieve 100 \% accuracy, with all participants selecting the goal object in all trials.}
}

@Article{10.1007/s13218-015-0407-7,
    author = {Schwarze, Tobias and Lauer, Martin and Schwaab, Manuel and Romanovas, Michailas and Böhm, Sandra and Jürgensohn, Thomas},
    title = {A Camera-Based Mobility Aid for Visually Impaired People},
    journal = {KI - Künstliche Intelligenz},
    volume = {30.0},
    issue = {1},
    doi = {10.1007/s13218-015-0407-7},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s13218-015-0407-7},
    abstract = {We present a wearable assistance system for visually impaired persons that perceives the environment with a stereo camera system and communicates obstacles and other objects to the user in form of intuitive acoustic feedback. The system is designed to complement traditional assistance aids. We describe the core techniques of scene understanding, head tracking, and sonification and show in an experimental study that it enables users to walk in unknown urban terrain and to avoid obstacles safely.}
}

@Chapter{10.1007/978-3-319-04114-8_38,
    author = {Wang, Zhe and Liu, Hong and Wang, Xiangdong and Qian, Yueliang},
    title = {Segment and Label Indoor Scene Based on RGB-D for the Visually Impaired},
    journal = {MultiMedia Modeling},
    doi = {10.1007/978-3-319-04114-8\_38},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-04114-8\_38},
    abstract = {The growing study in RGB-D sensor and 3D point cloud have made new progress in obstacle avoidance for the visually impaired. However, it remains a challenging problem due to the difficulty in design a robust and real-time algorithm. In this paper, we focus on scene segmentation and labeling. As man-made indoor scene contains many planar area and structure, plane segmentation and classification is important for further scene analysis. This work propose a multiscale-voxel strategy to reduce the effects of noise and improve plane segmentation. Then the segmentation result is combined with depth data and color data to apply graph-based image segmentation algorithm. After that, a cascaded decision tree is trained to classify different segments into different semantical type. The method is tested on part of the NYU Depth Dataset. Experimental results show that the proposed method combines the advantages of depth data and the geometry characteristics of the scene, and improves scene segmentation and obstacle detection.}
}

@Chapter{10.1007/978-3-319-31744-1_42,
    author = {Salido, Jesus and Deniz, Oscar and Bueno, Gloria},
    title = {Sainet: An Image Processing App for Assistance of Visually Impaired People in Social Interaction Scenarios},
    journal = {Bioinformatics and Biomedical Engineering},
    doi = {10.1007/978-3-319-31744-1\_42},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-31744-1\_42},
    abstract = {This work describes a mobile application (Sainet) for image processing as an assistive technology devoted to visually impaired users. The app is targeted to the Android platform and usually executed in a mobile device equipped with a back camera for image acquisition. Moreover, a wireless bluetooth headphone provides the audio feedback to the user. Sainet has been conceived as an assistance tool to the user in a social interaction scenario. It is capable of providing audible information about the number and position (distance and orientation) of the interlocutors in the user frontal scenario. For validation purposes the app has been tested by a blind user who has provided valuable insights about its strengths and weaknesses.}
}

@Chapter{10.1007/978-3-319-23231-7_43,
    author = {Caldini, Alessandro and Fanfani, Marco and Colombo, Carlo},
    title = {Smartphone-Based Obstacle Detection for the Visually Impaired},
    journal = {Image Analysis and Processing — ICIAP 2015},
    doi = {10.1007/978-3-319-23231-7\_43},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-23231-7\_43},
    abstract = {One of the main problems that visually impaired people have to deal with is moving autonomously in an unknown environment. Currently, the most used autonomous walking aid is still the white can. Though in the last few years more technological devices have been introduced, referred to as electronic travel aids (ETAs). In this paper, we present a novel ETA based on computer vision. Exploiting the hardware and software facilities of a standard smartphone, our system is able to extract a 3D representation of the scene and detect possible obstacles. To achieve such a result, images are captured by the smartphone camera and processed with a modified Structure from Motion algorithm that takes as input also information from the built-in gyroscope. Then the system estimates the ground-plane and labels as obstacles all the structures above it. Results on indoor and outdoor test sequences show the effectiveness of the proposed method.}
}

@Chapter{10.1007/978-981-10-3002-4_9,
    author = {Huang, Dian and Cheng, Hong and Yang, Lu},
    title = {Interactive Banknotes Recognition for the Visual Impaired With Wearable Assistive Devices},
    journal = {Pattern Recognition},
    doi = {10.1007/978-981-10-3002-4\_9},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-981-10-3002-4\_9},
    abstract = {In this paper, we develop a new system, named WVIAS (Wearable Vision Impaired Assistive System), using camera-based computer vision technology to recognize banknote in natural scene aim to help visually impaired people. WVIAS is made up of two mainly parts. In the front, there is a micro camera, set on the glass or mounted on the helmet, to acquire video sequence. In the back, a high performance portable computer is planted to run processing algorithm. To make the system robust to variety conditions including occlusion, rotation, scaling, cluttered background, illumination change, viewpoint variation, and worn or wrinkled banknotes during recognition, we propose a method that using finger pointing as HCI to point out potential targeting district which we call region of interest (ROI), thereafter, we can sharply reduce the processing time by using ROI to replace original image combining with effective ORB feature. The HCI-based framework is effective in collecting more class-specific information and robust in dealing with partial occlusion and viewpoint changes. To authenticate the robustness and generalizability of the proposed approach, we have collected a large dataset of banknotes from natural scene. The proposed algorithm improved the mean average precision from 20.3 \% to 61.6 \%. The experiments result has shown the effectiveness of our proposal both on the natural scene static dataset and the dynamic video sequence.}
}

@Chapter{10.1007/978-3-642-12239-2_38,
    author = {Tamimi, Hashem and Sharabati, Anas},
    title = {Markerless Localization for Blind Users Using Computer Vision and Particle Swarm Optimization},
    journal = {Applications of Evolutionary Computation},
    doi = {10.1007/978-3-642-12239-2\_38},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-12239-2\_38},
    abstract = {In this paper, we propose a novel approach, which aims to solve the localization and target-finding problem for blind and partially sighted people. A guidance system, solely implemented on a mobile phone with a camera, is employed. A computer vision approach integrated with Particle Swarm Optimization (PSO) is proposed for tracking the location. Using PSO leads to many advantages: first, it is possible to obtain robust localization results by combining the current and historical information about the location of the blind person. Second, it helps the system to resolve from ambiguous situations caused by facing similar images at different locations. Third, it can detect and recover from cases where the calculated location is wrong. Experimental results show that the proposed method works efficiently because of the simplicity of the approach, which makes it suitable for mobile applications.}
}

@Chapter{10.1007/978-3-319-07491-7_37,
    author = {Farzana, J. and Muhammad, Aslam and Martinez-Enriquez, A. M. and Afraz, Z. S. and Talha, W.},
    title = {Speech Based Shopping Assistance for the Blind},
    journal = {Pattern Recognition},
    doi = {10.1007/978-3-319-07491-7\_37},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-07491-7\_37},
    abstract = {Vision loss is one of ultimate obstacle in the lives of blind that prevent them to perform tasks on their own and self-reliantly. The blind are trusting on others for the selection of trendy and eye-catching accessories because self –buying effort lead them in such collection that is mismatch with their personalities and society style. That is why they are bound to depend upon on their family for shopping assistance, who often may not afford quality time due to busy routine. The thought of dependency rises lack of self-confidence in blinds, absorbs their ability to negotiate, decision making power, and social activities. Via uninterrupted speech communication, our proposed talking accessories selector assistant for the blind provides quick decision support in picking the routinely wearable accessories like dress, shoes, cosmetics, according to the society drifts and events. The foremost determination of this assistance is to make the blind liberated and more assertive.}
}

@Article{10.1007/s10055-012-0213-6,
    author = {Katz, Brian F. G. and Kammoun, Slim and Parseihian, Gaëtan and Gutierrez, Olivier and Brilhault, Adrien and Auvray, Malika and Truillet, Philippe and Denis, Michel and Thorpe, Simon and Jouffrais, Christophe},
    title = {NAVIG: augmented reality guidance system for the visually impaired},
    journal = {Virtual Reality},
    volume = {16.0},
    issue = {4},
    doi = {10.1007/s10055-012-0213-6},
    year = {2012},
    url = {http://link.springer.com/article/10.1007/s10055-012-0213-6},
    abstract = {Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired.}
}

@Chapter{10.1007/978-3-319-75786-5_36,
    author = {Bogdan, Oleksandr and Yurchenko, Oleg and Bailo, Oleksandr and Rameau, Francois and Yoo, Donggeun and Kweon, In So},
    title = {Intelligent Assistant for People with Low Vision Abilities},
    journal = {Image and Video Technology},
    doi = {10.1007/978-3-319-75786-5\_36},
    year = {2018},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-75786-5\_36},
    abstract = {This paper proposes a wearable system for visually impaired people that can be utilized to obtain an extensive feedback about their surrounding environment. Our system consists of a stereo camera and smartglasses, communicating with a smartphone that is used as an intermediary computational device. Furthermore, the system is connected to a server where all the expensive computations are executed. The whole setup is capable of detecting obstacles in the nearest surrounding, recognizing faces and facial expressions, reading texts, providing a generic description and question answering of a particular input image. In addition, we propose a novel depth question answering system to estimate object size as well as objects relative position in an unconstrained environment in near real-time and in a fully automatic way requiring only stereo image pair and voice request as an input. We have conducted a series of experiments to evaluate the feasibility and practicality of the proposed system which shows promising results to assist visually impaired people.}
}

@Chapter{10.1007/978-3-319-46681-1_66,
    author = {Alshehri, Manal Abdulaziz and Jarraya, Salma Kammoun and Ben-Abdallah, Hanene},
    title = {A Mobile-Based Obstacle Detection Method: Application to the Assistance of Visually Impaired People},
    journal = {Neural Information Processing},
    doi = {10.1007/978-3-319-46681-1\_66},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-46681-1\_66},
    abstract = {Visual impairments suffer many difficulties when they navigate from one place to another in their daily life. The biggest problem is obstacle detection. In this work, we propose a new smartphone-based method for obstacle detection. We aim to detect static and dynamic obstacles in unknown environments while offering maximum flexibility to the user and using the least expensive equipment possible. Detecting obstacles is based on the analysis of different regions of video frames and using a new decision algorithm. The analysis uses prediction model for each region that generated by a supervised learning process. The user is notified about the existing of an obstacle by alert message. The efficiency of the work is measured by many experiments studies on different complex scenes. It records low false alarm rate in the range of [0.2 \% to 11 \%], and high accuracy in the range of [86 \% to 94 \%].}
}

@Chapter{10.1007/978-3-319-07509-9_18,
    author = {Froissard, Benoît and Konik, Hubert and Trémeau, Alain and Dinet, Éric},
    title = {Contribution of Augmented Reality Solutions to Assist Visually Impaired People in Their Mobility},
    journal = {Universal Access in Human-Computer Interaction. Design for All and Accessibility Practice},
    doi = {10.1007/978-3-319-07509-9\_18},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-07509-9\_18},
    abstract = {The study is dedicated to analyze opportunities of augmented reality eyewear solutions for visually impaired people in a context of mobility. In order to perfectly understand the needs of low vision individuals, their expectation towards visual aids, and to clearly define crucial requirements, an experimental study has been carried out in a re-adaptation clinic. 58 patients with different visual pathologies have been carefully selected by vision-care professionals. During experiments and interviews, professional techniques developed for teaching patients to efficiently use their visual residual capabilities have been analyzed. One of the main objectives was to show the usefulness and the importance to put in the loop all actors to be able to derive relevant knowledge essential to success in the design and in the development of new visual aids dedicated to facilitate mobility of low vision people. The first results are encouraging and they tend to demonstrate the interest to use embedded augmented reality systems in order to propose helpful solutions easily adaptable to the specificities of the different visual troubles affecting mobility.}
}

@Chapter{10.1007/978-3-319-51064-4_32,
    author = {Majeed, Asim and Baadel, Said},
    title = {Facial Recognition Cane for the Visually Impaired},
    journal = {Global Security, Safety and Sustainability - The Security Challenges of the Connected World},
    doi = {10.1007/978-3-319-51064-4\_32},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-51064-4\_32},
    abstract = {The modern era is accompanied by various traditional mobility aids which help visually impaired to stay independent and enabling them detecting the objects and scanning surroundings. The use of haptic touch, as well as ultrasound, is embedded in today’s smart canes which detect obstacles up to 3 m distance, GPS navigation, informs the user through Bluetooth and earpiece, and guide the visually impaired to direct from one location to another. The evolution of this technology has motivated the integration of inexpensive camera technology within the cane for facial recognition purposes. The concept of developing this intelligent smart cane which would detect obstacles from up to 10 m as well as recognises friends and family faces, was envisioned by students at Birmingham City University. The developments in this product and adopted technologies guide a visually impaired user to detect obstacles and to find an alternative route while at the same time try to recognize any family or friends within the vicinity. These have been reflected in this research paper along with the limitations and wider issues which may come up when adopting the high-tech advances.}
}

@Chapter{10.1007/978-3-319-16199-0_58,
    author = {Rituerto, Alejandro and Murillo, Ana C. and Guerrero, José J.},
    title = {3D Layout Propagation to Improve Object Recognition in Egocentric Videos},
    journal = {Computer Vision - ECCV 2014 Workshops},
    doi = {10.1007/978-3-319-16199-0\_58},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-16199-0\_58},
    abstract = {Intelligent systems need complex and detailed models of their environment to achieve more sophisticated tasks, such as assistance to the user. Vision sensors provide rich information and are broadly used to obtain these models, for example, indoor scene modeling from monocular images has been widely studied. A common initial step in those settings is the estimation of the \(3\)D layout of the scene. While most of the previous approaches obtain the scene layout from a single image, this work presents a novel approach to estimate the initial layout and addresses the problem of how to propagate it on a video. We propose to use a particle filter framework for this propagation process and describe how to generate and sample new layout hypotheses for the scene on each of the following frames. We present different ways to evaluate and rank these hypotheses. The experimental validation is run on two recent and publicly available datasets and shows promising results on the estimation of a basic \(3\)D layout. Our experiments demonstrate how this layout information can be used to improve detection tasks useful for a human user, in particular sign detection, by easily rejecting false positives.}
}

@Chapter{10.1007/978-3-319-07854-0_41,
    author = {Jafri, Rabia and Ali, Syed Abid},
    title = {A GPS-Based Personalized Pedestrian Route Recording Smartphone Application for the Blind},
    journal = {HCI International 2014 - Posters’ Extended Abstracts},
    doi = {10.1007/978-3-319-07854-0\_41},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-07854-0\_41},
    abstract = {A GPS-based smartphone application for blind users is proposed which will allow them to record pedestrian routes to frequently visited destinations (e.g., supermarket, neighborhood mosque, etc.) and to retrieve them later for autonomous navigation. Unlike similar systems, which simply provide a route to a specified destination based on existing GPS maps which may not contain detailed information especially about pedestrian paths and alleys, our software is unique in that it will allow users to record a customized path to a particular destination based on personal considerations such as whether the area surrounding the route is well-lit and well-populated, the unevenness of the terrain and the absence of hazards (such as traffic intersections). A distress call option and auditory cues about user-specified obstacles will also be provided. The objective is to develop a low-cost, portable solution based on easily accessible technology to assist blind users in their daily outdoor mobility tasks.}
}

@Chapter{10.1007/978-3-319-04406-4_8,
    author = {Kyriazanos, Dimitris M. and Vastianos, George E. and Segou, Olga E. and Thomopoulos, Stelios C. A.},
    title = {Object Tracking AAL Application and Behaviour Modelling for the Elderly and Visually Impaired},
    journal = {Evolving Ambient Intelligence},
    doi = {10.1007/978-3-319-04406-4\_8},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-04406-4\_8},
    abstract = {Different degrees and types of visual impairment have become a common condition among the elderly, as aging inevitably affects the health and lifestyle of individuals. Partial or complete lack of sight is often accompanied by other ailments and conditions which further hinder the individual’s activities. In this work, a novel Ambient Assisted Living (AAL) platform is proposed, aiming to support the functional capabilities of the elderly and visually impaired, thus ameliorating their lifestyle. This platform is based on indoor tracking of commonly used objects, such as medication packages. Accompanied by a proposed behavioural modeling methodology, the application also offers valuable observations that may indicate developing ill-health conditions in an early stage. The proposed platform was tested and evaluated by end-users in Spain, Greece and Finland.}
}

@Chapter{10.1007/978-3-319-08599-9_20,
    author = {Martinez, Manuel and Constantinescu, Angela and Schauerte, Boris and Koester, Daniel and Stiefelhagen, Rainer},
    title = {Cognitive Evaluation of Haptic and Audio Feedback in Short Range Navigation Tasks},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-319-08599-9\_20},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-08599-9\_20},
    abstract = {Assistive navigation systems for the blind commonly use speech to convey directions to their users. However, this is problematic for short range navigation systems that need to provide fine but diligent guidance in order to avoid obstacles. For this task, we have compared haptic and audio feedback systems under the NASA-TLX protocol to analyze the additional cognitive load that they place on users. Both systems are able to guide the users through a test obstacle course. However, for white cane users, auditory feedback results in a 22 times higher cognitive load than haptic feedback. This discrepancy in cognitive load was not found on blindfolded users, thus we argue against evaluating navigation systems solely with blindfolded users.}
}

@Chapter{10.1007/978-3-642-31534-3_3,
    author = {Wang, Shuihua and Tian, Yingli},
    title = {Camera-Based Signage Detection and Recognition for Blind Persons},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-642-31534-3\_3},
    year = {2012},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-31534-3\_3},
    abstract = {Signage plays an important role for wayfinding and navigation to assist blind people accessing unfamiliar environments. In this paper, we present a novel camera-based approach to automatically detect and recognize restroom signage from surrounding environments. Our method first extracts the attended areas which may content signage based on shape detection. Then, Scale-Invariant Feature Transform (SIFT) is applied to extract local features in the detected attended areas. Finally, signage is detected and recognized as the regions with the SIFT matching scores larger than a threshold. The proposed method can handle multiple signage detection. Experimental results on our collected restroom signage dataset demonstrate the effectiveness and efficiency of our proposed method.}
}

@Chapter{10.1007/978-3-319-16199-0_43,
    author = {Stearns, Lee and Du, Ruofei and Oh, Uran and Wang, Yumeng and Findlater, Leah and Chellappa, Rama and Froehlich, Jon E.},
    title = {The Design and Preliminary Evaluation of a Finger-Mounted Camera and Feedback System to Enable Reading of Printed Text for the Blind},
    journal = {Computer Vision - ECCV 2014 Workshops},
    doi = {10.1007/978-3-319-16199-0\_43},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-16199-0\_43},
    abstract = {We introduce the preliminary design of a novel vision-augmented touch system called HandSight intended to support activities of daily living (ADLs) by sensing and feeding back non-tactile information about the physical world as it is touched. Though we are interested in supporting a range of ADL applications, here we focus specifically on reading printed text. We discuss our vision for HandSight, describe its current implementation and results from an initial performance analysis of finger-based text scanning. We then present a user study with four visually impaired participants (three blind) exploring how to continuously guide a user’s finger across text using three feedback conditions (haptic, audio, and both). Though preliminary, our results show that participants valued the ability to access printed material, and that, in contrast to previous findings, audio finger guidance may result in the best reading performance.}
}

@Chapter{10.1007/978-3-319-07491-7_16,
    author = {Terven, Juan R. and Salas, Joaquin and Raducanu, Bogdan},
    title = {Robust Head Gestures Recognition for Assistive Technology},
    journal = {Pattern Recognition},
    doi = {10.1007/978-3-319-07491-7\_16},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-07491-7\_16},
    abstract = {This paper presents a system capable of recognizing six head gestures: nodding, shaking, turning right, turning left, looking up, and looking down. The main difference of our system compared to other methods is that the Hidden Markov Models presented in this paper, are fully connected and consider all possible states in any given order, providing the following advantages to the system: (1) allows unconstrained movement of the head and (2) it can be easily integrated into a wearable device (e.g. glasses, neck-hung devices), in which case it can robustly recognize gestures in the presence of ego-motion. Experimental results show that this approach outperforms common methods that use restricted HMMs for each gesture.}
}

@Chapter{10.1007/978-3-319-40250-5_11,
    author = {Wentzel, Jobke and Velleman, Eric and van der Geest, Thea},
    title = {Developing Accessibility Design Guidelines for Wearables: Accessibility Standards for Multimodal Wearable Devices},
    journal = {Universal Access in Human-Computer Interaction. Methods, Techniques, and Best Practices},
    doi = {10.1007/978-3-319-40250-5\_11},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-40250-5\_11},
    abstract = {Smart wearable devices are integrated our everyday lives. Such wearable technology is worn on or near the body, while leaving both hands free. This enables users to receive and send information in a non-obtrusive way. Because of the ability to continuously assist and support activities, wearables could be of great value to persons with a disability. Persons with a disability can only benefit from the potential of wearables if they are accessible. Like other devices, platforms, and applications, developers of wearables need to take accessibility into account during early development, for example by including multimodal interfaces in the design. Even though some accessibility guidelines and standards exist for websites and mobile phones, more support for the development of accessible wearables is needed. The aim of our project is to develop a set of guidelines for accessible wearables. Three approaches are combined to develop the guidelines. A scan of the literature was done to identify publications addressing the accessibility of wearables and/or development guidelines. Semi-structured interviews were held with developers of accessible wearable technology. Based on these first activities, a draft set of guidelines is created. This draft is evaluated with developers and researchers in the field of universal design, accessibility, and wearables. Further, the draft is evaluated with visually impaired people (VIP) in interviews. Based on these results, a final set of guidelines will be created. This set is evaluated against an actual project in which apps are developed for VIP. This study is in progress; first results are presented (literature study, semi-structured interviews, first draft of guidelines) and a call for participation in the Delphi study is issued.
}
}

@Chapter{10.1007/978-3-642-14100-3_39,
    author = {Tian, Yingli and Yang, Xiaodong and Arditi, Aries},
    title = {Computer Vision-Based Door Detection for Accessibility of Unfamiliar Environments to Blind Persons},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-642-14100-3\_39},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-14100-3\_39},
    abstract = {Doors are significant landmarks for indoor wayfinding and navigation to assist blind people accessing unfamiliar environments. Most camera-based door detection algorithms are limited to familiar environments where doors demonstrate known and similar appearance features. In this paper, we present a robust image-based door detection algorithm based on doors’ general and stable features (edges and corners) instead of appearance features (color, texture, etc). A generic geometric door model is built to detect doors by combining edges and corners. Furthermore, additional geometric information is employed to distinguish doors from other objects with similar size and shape (e.g. bookshelf, cabinet, etc). The robustness and generalizability of the proposed detection algorithm are evaluated against a challenging database of doors collected from a variety of environments over a wide range of colors, textures, occlusions, illuminations, scale, and views.}
}

@Chapter{10.1007/978-3-319-16199-0_56,
    author = {Furnari, Antonino and Farinella, Giovanni Maria and Battiato, Sebastiano},
    title = {An Experimental Analysis of Saliency Detection with Respect to Three Saliency Levels},
    journal = {Computer Vision - ECCV 2014 Workshops},
    doi = {10.1007/978-3-319-16199-0\_56},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-16199-0\_56},
    abstract = {Saliency detection is a useful tool for video-based, real-time Computer Vision applications. It allows to select which locations of the scene are the most relevant and has been used in a number of related assistive technologies such as life-logging, memory augmentation and object detection for the visually impaired, as well as to study autism and the Parkinson’s disease. Many works focusing on different aspects of saliency have been proposed in the literature, defining saliency in different ways depending on the task. In this paper we perform an experimental analysis focusing on three levels where saliency is defined in different ways, namely visual attention modelling, salient object detection and salient object segmentation. We review the main evaluation datasets specifying the level of saliency which they best describe. Through the experiments we show that the performances of the saliency algorithms depend on the level with respect to which they are evaluated and on the nature of the stimuli used for the benchmark. Moreover, we show that the eye fixation maps can be effectively used to perform salient object detection and segmentation, which suggests that pre-attentive bottom-up information can be still exploited to improve high level tasks such as salient object detection. Finally, we show that benchmarking a saliency detection algorithm with respect to a single dataset/saliency level, can lead to erroneous results and conclude that many datasets/saliency levels should be considered in the evaluations.}
}

@Chapter{10.1007/978-3-319-92198-3_15,
    author = {Gomez-Garay, Alejandro and Raducanu, Bogdan and Salas, Joaquín},
    title = {Dense Captioning of Natural Scenes in Spanish},
    journal = {Pattern Recognition},
    doi = {10.1007/978-3-319-92198-3\_15},
    year = {2018},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-92198-3\_15},
    abstract = {The inclusion of visually impaired people to daily life is a challenging and active area of research. This work studies how to bring information about the surroundings to people delivered as verbal descriptions in Spanish using wearable devices. We use a neural network (DenseCap) for both identifying objects and generating phrases about them. DenseCap is running on a server to describe an image fed from a smartphone application, and its output is the text which a smartphone verbalizes. Our implementation achieves a mean Average Precision (mAP) of 5.0 in object recognition and quality of captions and takes an average of 7.5 s from the moment one grabs a picture until one receives the verbalization in Spanish.}
}

@Chapter{10.1007/978-3-642-29364-1_2,
    author = {Yi, Chucai and Tian, Yingli},
    title = {Assistive Text Reading from Complex Background for Blind Persons},
    journal = {Camera-Based Document Analysis and Recognition},
    doi = {10.1007/978-3-642-29364-1\_2},
    year = {2012},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-29364-1\_2},
    abstract = {In the paper, we propose a camera-based assistive system for visually impaired or blind persons to read text from signage and objects that are held in the hand. The system is able to read text from complex backgrounds and then communicate this information aurally. To localize text regions in images with complex backgrounds, we design a novel text localization algorithm by learning gradient features of stroke orientations and distributions of edge pixels in an Adaboost model. Text characters in the localized regions are recognized by off-the-shelf optical character recognition (OCR) software and transformed into speech outputs. The performance of the proposed system is evaluated on ICDAR 2003 Robust Reading Dataset. Experimental results demonstrate that our algorithm outperforms previous algorithms on some measures. Our prototype system was further evaluated on a dataset collected by 10 blind persons, with the system effectively reading text from complex backgrounds.}
}

@Chapter{10.1007/978-1-4614-0064-6_28,
    author = {Ong, S. K. and Shen, Y. and Zhang, J. and Nee, A. Y. C.},
    title = {Augmented Reality in Assistive Technology and Rehabilitation Engineering},
    journal = {Handbook of Augmented Reality},
    doi = {10.1007/978-1-4614-0064-6\_28},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-1-4614-0064-6\_28},
    abstract = {With increasing life expectancy and declining fertility, the ageing population continues to increase worldwide while the population of younger people decreases [1]. The change in the demographic structure would bring greater pressure on long-term health services with increasing population of people who need healthcare and decreasing population of people who can be part of the work force. It is expected that the proportion of people aged above 60 years old would reach 19\% by 2050 [2], and there would be a growing proportion of individuals who suffer from disabilities due to illnesses related to ageing. Therefore, there is increasing demand for Assistive Technology (AT) devices and Rehabilitation Engineering (RE) applications and researches in AT and RE have received more attention.}
}

@Chapter{10.1007/978-3-642-38577-3_67,
    author = {Tsai, Chun-Ming},
    title = {Local Skew Estimation in Moving Business Cards},
    journal = {Recent Trends in Applied Artificial Intelligence},
    doi = {10.1007/978-3-642-38577-3\_67},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-38577-3\_67},
    abstract = {Current methods to help visually impaired persons read brief text like menus, business cards, and book covers are problematic because they assume both the user and the captured scene are stationary and they do not tell the visually impaired user if the target is captured by the camera. Further, these methods cannot estimate whether the text is locally skewed. This paper presents an intelligent system to estimate movement, thumbs, motion blur, text, and local skew in moving business card targets. Experimental results show that the proposed method can reduce time complexity, obtain high text detection rates, and achieve high local skew estimation rates.}
}

@Chapter{10.1007/978-1-4471-6708-2_6,
    author = {Bahram, Sina and Chakraborty, Arpan and Ravindran, Srinath and Amant, Robert St.},
    title = {Intelligent Interaction in Accessible Applications},
    journal = {A Multimodal End-2-End Approach to Accessible Computing},
    doi = {10.1007/978-1-4471-6708-2\_6},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-1-4471-6708-2\_6},
    abstract = {Advances in artificial intelligence over the past decade, combined with increasingly affordable computing power, have made new approaches to accessibility possible. In this chapter we describe three ongoing projects in the Department of Computer Science at North Carolina State University. CAVIAR, a Computer-vision Assisted Vibrotactile Interface for Accessible Reaching, is a wearable system that aids people with vision impairment (PWVI) in locating, identifying, and acquiring objects within reach; a mobile phone worn on the chest processes video input and guides the user’s hand to objects via a wristband with vibrating actuators. TIKISI (Touch It, Key It, Speak It), running on a tablet, gives PWVI the ability to explore maps and other forms of graphical information. AccessGrade combines crowd-sourcing with machine learning techniques to predict the accessibility of Web pages.}
}

@Chapter{10.1007/978-1-4471-5082-4_5,
    author = {Bahram, Sina and Chakraborty, Arpan and Ravindran, Srinath and Amant, Robert St.},
    title = {Intelligent Interaction in Accessible Applications},
    journal = {A Multimodal End-2-End Approach to Accessible Computing},
    doi = {10.1007/978-1-4471-5082-4\_5},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-1-4471-5082-4\_5},
    abstract = {Advances in artificial intelligence over the past decade, combined with increasingly affordable computing power, have made new approaches to accessibility possible. In this chapter we describe three ongoing projects in the Department of Computer Science at North Carolina State University. CAVIAR, a Computer-vision Assisted Vibrotactile Interface for Accessible Reaching, is a wearable system that aids people with vision impairment (PWVI) in locating, identifying, and acquiring objects within reach; a mobile phone worn on the chest processes video input and guides the user’s hand to objects via a wristband with vibrating actuators. TIKISI (Touch It, Key It, Speak It), running on a tablet, gives PWVI the ability to explore maps and other forms of graphical information. AccessGrade combines crowd-sourcing with machine learning techniques to predict the accessibility of Web pages.}
}

@Chapter{10.1007/978-3-319-61949-1_16,
    author = {Marco, Marco De and Fenu, Gianfranco and Medvet, Eric and Pellegrino, Felice Andrea},
    title = {Computer Vision for the Blind: A Comparison of Face Detectors in a Relevant Scenario},
    journal = {Smart Objects and Technologies for Social Good},
    doi = {10.1007/978-3-319-61949-1\_16},
    year = {2017},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-61949-1\_16},
    abstract = {Motivated by the aim of developing a vision-based system to assist the social interaction of blind persons, the performance of some face detectors are evaluated. The detectors are applied to manually annotated video sequences acquired by blind persons with a glass-mounted camera and a necklace-mounted one. The sequences are relevant to the specific application and demonstrate to be challenging for all the considered detectors. A further analysis is performed to reveal how the performance is affected by some features such as occlusion, rotations, size and position of the face within the frame.}
}

@Chapter{10.1007/978-3-319-16220-1_3,
    author = {Rivera-Rubio, Jose and Alexiou, Ioannis and Bharath, Anil A.},
    title = {Associating Locations Between Indoor Journeys from Wearable Cameras},
    journal = {Computer Vision - ECCV 2014 Workshops},
    doi = {10.1007/978-3-319-16220-1\_3},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-16220-1\_3},
    abstract = {The main question we address is whether it is possible to crowdsource navigational data in the form of video sequences captured from wearable cameras. Without using geometric inference techniques (such as SLAM), we test video data for its location-discrimination content. Tracking algorithms do not form part of this assessment, because our goal is to compare different visual descriptors for the purpose of location inference in highly ambiguous indoor environments. The testing of these descriptors, and different encoding methods, is performed by measuring the positional error inferred during one journey with respect to other journeys along the same approximate path.There are three main contributions described in this paper. First, we compare different techniques for visual feature extraction with the aim of associating locations between different journeys along roughly the same physical route. Secondly, we suggest measuring the quality of position inference relative to multiple passes through the same route by introducing a positional estimate of ground truth that is determined with modified surveying instrumentation. Finally, we contribute a database of nearly 100,000 frames with this positional ground-truth. More than 3 km worth of indoor journeys with a hand-held device (Nexus 4) and a wearable device (Google Glass) are included in this dataset.}
}

@Article{10.1007/s13721-013-0025-y,
    author = {Hasanuzzaman, Faiz M. and Yang, Xiaodong and Ying and Tian, Li and Liu, Qingshan and Capezuti, Elizabeth},
    title = {Monitoring activity of taking medicine by incorporating RFID and video analysis},
    journal = {Network Modeling Analysis in Health Informatics and Bioinformatics},
    volume = {2.0},
    issue = {2},
    doi = {10.1007/s13721-013-0025-y},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s13721-013-0025-y},
    abstract = {In this paper, we present a new framework to monitor medication intake for elderly individuals by incorporating a video camera and radio frequency identification (RFID) sensors. The proposed framework can provide a key function for monitoring activities of daily living (ADLs) of elderly people at their own home. In an assistive environment, RFID tags are applied on medicine bottles located in a medicine cabinet so that each medicine bottle will have a unique ID. The description of the medicine data for each tag is manually input to a database. RFID readers will detect if any of these bottles are taken away from the medicine cabinet and identify the tag attached on the medicine bottle. A video camera is installed to continue monitoring the activity of taking medicine by integrating face detection and tracking, mouth detection, background subtraction, and activity detection. The preliminary results demonstrate that 100 \% detection accuracy for identifying medicine bottles and promising results for monitoring activity of taking medicine.}
}

@Chapter{10.1007/978-3-319-60164-9_3,
    author = {Shahrestani, Seyed},
    title = {Digital Senses and Cognitive Assistance},
    journal = {Internet of Things and Smart Environments},
    doi = {10.1007/978-3-319-60164-9\_3},
    year = {2017},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-60164-9\_3},
    abstract = {Institute of Electrical and Electronic Engineers (IEEE) and one of its Future Directions team have initiated work on “digital senses” based on technologies that relate to various human sensory systems. One of the major objectives of this initiative is to promote alliances in the three focus areas of virtual reality, augmented reality, and human augmentation. Taking similar views, this part aims to show how combining the understandings about human senses with the advances in sensor technologies and the permeating nature of the IoT, can be of great value for aged care, cognitive assistance, and provision of disability services. The primary aim of these assistive devices and technologies is to enhance the functioning, independent participation, and overall quality of life for individuals. Some samples of such technologies, along with examples of the devices and systems that can be used in these contexts, are also discussed in this chapter. The discussions start with the traditional AT, then cover cognition assistance, and improvements of these two AT types through sensing to reach context-aware individualized ambient assisted living.}
}

@Chapter{10.1007/978-3-642-39194-1_10,
    author = {Paredes, Hugo and Fernandes, Hugo and Martins, Paulo and Barroso, João},
    title = {Gathering the Users’ Needs in the Development of Assistive Technology: A Blind Navigation System Use Case},
    journal = {Universal Access in Human-Computer Interaction. Applications and Services for Quality of Life},
    doi = {10.1007/978-3-642-39194-1\_10},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-39194-1\_10},
    abstract = {Assistive technology enables people to achieve independence in the accomplishment of their daily tasks and enhance their quality of life. However, the development os assistive technology does not always follow user needs and expectations, comprising their usability and effectiveness. This paper discusses the design and evaluation strategies for assistive technologies applied to a blind navigation system case study. The research carried out focused on the gathering of user requirements for ensuring enhanced autonomy of blind people in their daily life. The picked requirements were used in an architecture that unifies the benefits of a redundant blind navigation system with a set of services that are provided by daily used information and communication technologies. The system combines guidance, navigation and information gathering, extending traditional aids with realtime knowledge of the surrounding environment to provide an enhanced assistive tool for autonomy of the blinds. Moreover, the developed solution fills the gap of existing solutions that require the users to carry a wide range of devices and, mostly, do not include mechanisms to ensure the autonomy of users in the event of system failure.}
}

@Chapter{10.1007/978-3-642-14100-3_48,
    author = {Tian, Yingli and Yuan, Shuai},
    title = {Clothes Matching for Blind and Color Blind People},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-642-14100-3\_48},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-14100-3\_48},
    abstract = {Matching clothes is a challenging task for blind people. In this paper, we propose a new computer vision-based technology of clothes matching to help blind or color blind people by using a pair of images from two different clothes captured by a camera. A mini-laptop or a PDA can be used to perform the texture and color matching process. The proposed method can handle clothes in uniform color without any texture, as well as clothes with multiple colors and complex textures patterns. Furthermore, our method is robust to variations of illumination, clothes rotation, and clothes wrinkles. The proposed method is evaluated on a challenging database of clothes. The matching results are displayed as audio outputs (sound or speech) to the users for “match (for both color and texture)”, “color match, texture not match”, “texture match, color not match”, or “not match (for both color and texture)”.}
}

@Chapter{10.1007/978-3-319-48881-3_26,
    author = {Mocanu, Bogdan and Tapu, Ruxandra and Zaharia, Titus},
    title = {Using Computer Vision to See},
    journal = {Computer Vision – ECCV 2016 Workshops},
    doi = {10.1007/978-3-319-48881-3\_26},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-48881-3\_26},
    abstract = {In this paper we propose a navigation assistant for visually impaired people, which uses computer vision techniques and is integrated on a wearable device. The system makes it possible to detect and recognize, in real-time, both static and dynamic objects existent in outdoor urban scenes without any a priori knowledge about the obstruction type or location. The detection system is based on relevant interest point extraction and tracking, background/camera motion estimation and foreground object identification through motion vectors clustering. The classification method receives as input image patches extracted by the detection module, performs global image representation using binary VLAD and prediction based on SVM. The feedback of our system is transmitted to visually impaired users through bone-conduction headphones as a set of audio warning messages. The entire system is fully integrated on a regular smartphone. The experimental evaluation performed on a set of 20 videos acquired with the help of VI users, demonstrates the pertinence of the proposed methodology.}
}

@Chapter{10.1007/978-3-642-12349-8_2,
    author = {Liu, Xu and Doermann, David and Li, Huiping},
    title = {Mobile Visual Aid Tools for Users with Visual Impairments},
    journal = {Mobile Multimedia Processing},
    doi = {10.1007/978-3-642-12349-8\_2},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-12349-8\_2},
    abstract = {In this chapter we describe “MobileEye”, a software suite which converts a camera enabled mobile device into a multi-function vision tool that can assist the visually impaired in their daily activities. MobileEye consists of four subsystems, each customized for a specific type of visual disabilities: A color channel mapper which can tell the visually impaired different colors; a software based magnifier which provides image magnification as well as enhancement; a pattern recognizer which can read currencies; and a document retriever which allows access to printed materials. We developed cutting edge computer vision and image processing technologies, and tackled the challenges of implementing them on mobile devices with limited computational resources and low image quality. The system minimizes keyboard operation for the usability of users with visual impairments. Currently the software suite runs on Symbian and Windows Mobile handsets. In this chapter we provides a high level overview of the system, and then discuss the pattern recognizer in detail. The challenge is how to build a real-time recognition system on mobile devices and we present our detailed solutions.}
}

@Article{10.1007/s00138-015-0706-x,
    author = {Baştan, Muhammet},
    title = {Multi-view object detection in dual-energy X-ray images},
    journal = {Machine Vision and Applications},
    volume = {26.0},
    issue = {7 - 8},
    doi = {10.1007/s00138-015-0706-x},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s00138-015-0706-x},
    abstract = {Automatic inspection of X-ray scans at security checkpoints can improve the public security. X-ray images are different from photographic images. They are transparent. They contain much less texture. They may be highly cluttered. Objects may undergo in- and out-of-plane rotations. On the other hand, scale and illumination change is less of an issue. More importantly, X-ray imaging provides extra information which are usually not available in regular images: dual-energy imaging, which provides material information about the objects; and multi-view imaging, which provides multiple images of objects from different viewing angles. Such peculiarities of X-ray images should be leveraged for high-performance object recognition systems to be deployed on X-ray scanners. To this end, we first present an extensive evaluation of standard local features for object detection on a large X-ray image dataset in a structured learning framework. Then, we propose two dense sampling methods as keypoint detector for textureless objects and extend the SPIN color descriptor to utilize the material information. Finally, we propose a multi-view branch-and-bound search algorithm for multi-view object detection. Through extensive experiments on three object categories, we show that object detection performance on X-ray images improves substantially with the help of extended features and multiple views.}
}

@Chapter{10.1007/978-3-319-05458-2_32,
    author = {Tsai, Chun-Ming and Yeh, Zong-Mu},
    title = {Detection of Bus Routes Number in Bus Panel via Learning Approach},
    journal = {Intelligent Information and Database Systems},
    doi = {10.1007/978-3-319-05458-2\_32},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-05458-2\_32},
    abstract = {Detection of bus route number is a very important issue for assisting visually impaired people to take the bus. This paper proposes an intelligent approach to detect bus route numbers to help visually impaired people to “see” the bus route number of a moving bus. Current e-bus stations in Taipei, Taiwan, are not very robust in bad weather. For visually impaired people, it is very difficult know which bus route number is approaching the bus station. Some past research used RFID, GIS, sound, and image-based methods to help visually impaired people to “see” bus route number. However, the development of bus route number reading devices for visually impaired people in real time is still at an early stage. In this paper, we propose an efficient and effective learning-based method to detect the bus route number in displayed on the bus façade panel of the moving bus. Experimental results show that the proposed method can reduce time complexity and achieve high bus route number detection rates.}
}

@Chapter{10.1007/978-3-642-41190-8_3,
    author = {Takahisa, Kishino and Sun, Zhe and Micheletto, Ruggero},
    title = {A Fast and Precise HOG-Adaboost Based Visual Support System Capable to Recognize Pedestrian and Estimate Their Distance},
    journal = {New Trends in Image Analysis and Processing – ICIAP 2013},
    doi = {10.1007/978-3-642-41190-8\_3},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-41190-8\_3},
    abstract = {In this paper,we present a visual support system the visually impaired. Our detection algorithm is based on the well known Histograms of Oriented Gradients (HOG) method, due to its high detection rate and versatility [5]. However, the accuracy of object recognition rate is reduced because of high false detection rate. In order to solve that, multiple parts model and triple phase detection have been implemented. These additional filtering stages were conducted by separate action on different area of the sample, considering deformations and translations. We demonstrated that this approach has raised the accuracy and speed of calculation. Through an evaluation experiment based on a large dataset, we found that false detection has been improved by 18.9\% in respect to standard HOG detectors. Experimental tests have also shown the system ability to estimate the distance of the pedestrian by the use of a simple perspective model. The system has been tested on several photographic datasets and have shown excellent performances also in ambiguous cases.}
}

@Chapter{10.1007/978-3-642-40849-6_8,
    author = {Glowa, Christoph and Schlegl, Thomas},
    title = {Ultrasound Based Object Detection for Human-Robot Collaboration},
    journal = {Intelligent Robotics and Applications},
    doi = {10.1007/978-3-642-40849-6\_8},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-40849-6\_8},
    abstract = {For safe human-robot collaboration within a defined working area, a technologically diverse and redundant sensor system is developed, which comprises ultrasound sensors and two monocular cameras. The ultrasound sensor system, as well as the developed algorithms for the sensor system are proposed, which allow a distinction between objects. Detected objects within the working area are classified in static and dynamic objects. The sensor system is able to distinguish between these object types. Due to the safe detection of dynamic objects the robot system is enabled to react with an adaption of its trajectories to avoid undesired collisions. To ensure, that the manipulator only reacts on dynamic objects, distances caused by static objects are eliminated.}
}

@Chapter{10.1007/978-3-319-20681-3_45,
    author = {Paredes, Hugo and Fernandes, Hugo and Sousa, André and Fortes, Renata and Koch, Fernando and Filipe, Vitor and Barroso, João},
    title = {CanIHelp: A Platform for Inclusive Collaboration},
    journal = {Universal Access in Human-Computer Interaction. Access to Interaction},
    doi = {10.1007/978-3-319-20681-3\_45},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-20681-3\_45},
    abstract = {Technology plays a key role in daily life of people with special needs, being a mean of integration or even communication with society. By built up experience, we find that support tools play a crucial part in empowerment of persons with special needs and small advances may represent shifts and opportunities. The diversity of solutions and the need for dedicated hardware to each feature represents a barrier to its use, compromising the success of the solutions against, among others, problems of usability and scale. This paper aims to explore the concept of inclusive collaboration to enhance the mutual interaction and assistance. The proposed approach combines and generalizes the usage of human computation in a collaborative environment with assistive technologies creating redundancy and complementarity in the solutions provided, contributing to enhance the quality of life of people with special needs and the elderly. The CanIHelp platform is an embodiment of the concept as a result from an orchestrated model using mechanisms of collective intelligence through social inclusion initiatives. The platform features up for integrating assistive technologies, collaborative tools and multiple multimedia communication channels, accessible through multimodal interfaces for universal access. A discussion of the impacts of fostering collaboration and broadening from the research concepts to the societal impacts is presented. As final remarks a set of future research challenges and guidelines are identified.}
}

@Chapter{10.1007/978-1-4471-5520-1_2,
    author = {Ebert, Sandra and Schiele, Bernt},
    title = {Where Next in Object Recognition and how much Supervision Do We Need?},
    journal = {Advanced Topics in Computer Vision},
    doi = {10.1007/978-1-4471-5520-1\_2},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-1-4471-5520-1\_2},
    abstract = {Object class recognition is an active topic in computer vision still presenting many challenges. In most approaches, this task is addressed by supervised learning algorithms that need a large quantity of labels to perform well. This leads either to small datasets (\&lt;10,000 images) that capture only a subset of the real-world class distribution (but with a controlled and verified labeling procedure), or to large datasets that are more representative but also add more label noise. Therefore, semi-supervised learning has been established as a promising direction to address object recognition. It requires only few labels while simultaneously making use of the vast amount of images available today. In this chapter, we outline the main challenges of semi-supervised object recognition, we review existing approaches, and we emphasize open issues that should be addressed next to advance this research topic.}
}

@Chapter{10.1007/978-3-319-07455-9_34,
    author = {Cheng, Ching-Ching and Tsai, Chun-Ming},
    title = {Using Red-Otsu Thresholding to Detect the Bus Routes Number for Helping Blinds to Take Bus},
    journal = {Modern Advances in Applied Intelligence},
    doi = {10.1007/978-3-319-07455-9\_34},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-07455-9\_34},
    abstract = {Current methods to help blind persons read text like menus, business cards, and book covers are inadequate because they assumed both the user and the captured scene are stationary. Furthermore, these methods cannot overcome the illumination problem. This paper presents an intelligent system to threshold the bus route number on a moving bus and to identify the bus route number from the binary images. Experimental results show that the proposed method uses less time complexity and achieves a higher detection rate than the gray level Otsu thresholding method.}
}

@Chapter{10.1007/978-3-319-16199-0_25,
    author = {Schauerte, Boris and Koester, Daniel and Martinez, Manel and Stiefelhagen, Rainer},
    title = {Way to Go! Detecting Open Areas Ahead of a Walking Person},
    journal = {Computer Vision - ECCV 2014 Workshops},
    doi = {10.1007/978-3-319-16199-0\_25},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-16199-0\_25},
    abstract = {We determine the region in front of a walking person that is not blocked by obstacles. This is an important task when trying to assist visually impaired people or navigate autonomous robots in urban environments. We use conditional random fields to learn how to interpret texture and depth information for their accessibility. We demonstrate the effectiveness of the proposed approach on a novel dataset, which consists of urban outdoor and indoor scenes that were recorded with a handheld stereo camera.}
}

@Chapter{10.1007/978-3-642-38622-0_18,
    author = {Collazos, Antonio and Fernández-López, David and Montemayor, Antonio S. and Pantrigo, Juan José and Delgado, María Luisa},
    title = {Abandoned Object Detection on Controlled Scenes Using Kinect},
    journal = {Natural and Artificial Computation in Engineering and Medical Applications},
    doi = {10.1007/978-3-642-38622-0\_18},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-38622-0\_18},
    abstract = {This paper presents a new approach for the detection of abandoned objects in partially controlled environments using images from a low cost depth sensor. To reach this goal, we propose a systems which involves: (i) a two phase object segmentation based on 3D points clustering, (ii) an object selection based on permanence and object dimensions and, finally, (iii) a state machine monitoring capable to deal with occlusions. The proposed system exclusively considers depth images, which makes it independent of lighting. In order to obtain evidences of the system performance in real conditions, we have conducted an experimental study on video sequences captured into a public bus, obtaining successful results.}
}

@Article{10.1007/s41315-017-0023-9,
    author = {Kim, Pileun and Chen, Jingdao and Cho, Yong K.},
    title = {Robotic sensing and object recognition from thermal-mapped point clouds},
    journal = {International Journal of Intelligent Robotics and Applications},
    volume = {1.0},
    issue = {3},
    doi = {10.1007/s41315-017-0023-9},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s41315-017-0023-9},
    abstract = {Many of the civil structures are more than half way through or nearing their intended service life; frequently assessing and maintaining structural integrity is a top maintenance priority. Robotic inspection technologies using ground and aerial robots with 3D scanning and imaging capabilities have the potential to improve safety and efficiency of infrastructure management. To provide more valuable information to inspectors and agency decision makers, automatic environment sensing and semantic information extraction are fundamental issues in this field. This paper introduces an innovative method for generating thermal-mapped point clouds of a robot’s work environment and performing automatic object recognition with the aid of thermal data fused to 3D point clouds. The laser scanned point cloud and thermal data were collected using a custom-designed mobile robot. The multimodal data was combined with a data fusion process based on texture mapping. The automatic object recognition was performed by two processes: segmentation with thermal data and classification with scanned geometric features. The proposed method was validated with the scan data collected in an entire building floor. Experimental results show that the thermal integrated object recognition approach achieved better performance than a geometry only-based approach, with an average recognition accuracy of 93\%, precision of 83\%, and recall rate of 86\% for objects in the tested environment including humans, display monitors and light fixtures.
}
}

@Chapter{10.1007/978-4-431-56535-2_7,
    author = {Kise, Koichi and Omachi, Shinichiro and Uchida, Seiichi and Iwamura, Masakazu and Inami, Masahiko and Kunze, Kai},
    title = {Reading-Life Log as a New Paradigm of Utilizing Character and Document Media},
    journal = {Human-Harmonized Information Technology, Volume 2},
    doi = {10.1007/978-4-431-56535-2\_7},
    year = {2017},
    url = {http://link.springer.com/chapter/10.1007/978-4-431-56535-2\_7},
    abstract = {“You are what you read.” As this sentence implies, reading is important for building our minds. We are investing a huge amount of time for reading to input information. However the activity of “reading” is done only by each individual in an analog way and nothing is digitally recorded and reused. In order to solve this problem, we record reading activities as digital data and analyze them for various goals. We call this research “reading-life log.” In this chapter, we describe our achievements of the reading-life log. A target of the reading-life log is to analyze reading activities quantitatively and qualitatively: when, how much, what you read, and how you read in terms of your interests and understanding. Body-worn sensors including intelligent eyewear are employed for this purpose. Another target is to analyze the contents of documents based on the users’ reading activities: for example, which are the parts most people feel difficult/interesting. Materials to be read are not limited to books and documents. Scene texts are also important materials which guide human activities.}
}

@Article{10.1007/s00146-017-0766-8,
    author = {Chacin, Aisen C. and Iwata, Hiroo and Vesna, Victoria},
    title = {Assistive Device Art: aiding audio spatial location through the Echolocation Headphones},
    journal = {AI \\& SOCIETY},
    doi = {10.1007/s00146-017-0766-8},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s00146-017-0766-8},
    abstract = {Assistive Device Art derives from the integration of Assistive Technology and Art, involving the mediation of sensorimotor functions and perception from both, psychophysical methods and conceptual mechanics of sensory embodiment. This paper describes the concept of ADA and its origins by observing the phenomena that surround the aesthetics of prosthesis-related art. It also analyzes one case study, the Echolocation Headphones, relating its provenience and performance to this new conceptual and psychophysical approach of tool design. This ADA tool is designed to aid human echolocation. They facilitate the experience of sonic vision, as a way of reflecting and learning about the construct of our spatial perception. Echolocation Headphones are a pair of opaque goggles which disable the participant’s vision. This device emits a focused sound beam which activates the space with directional acoustic reflection, giving the user the ability to navigate and perceive space through audition. The directional properties of parametric sound provide the participant a focal echo, similar to the focal point of vision. This study analyzes the effectiveness of this wearable sensory extension for aiding auditory spatial location in three experiments; optimal sound type and distance for object location, perceptual resolution by just noticeable difference, and goal-directed spatial navigation for open pathway detection, all conducted at the Virtual Reality Lab of the University of Tsukuba, Japan. The Echolocation Headphones have been designed for a diverse participant base. They have both the potential to aid auditory spatial perception for the visually impaired and to train sighted individuals in gaining human echolocation abilities. Furthermore, this Assistive Device artwork instigates participants to contemplate on the plasticity of their sensorimotor architecture.}
}

@Chapter{10.1007/978-1-4614-0064-6_1,
    author = {Carmigniani, Julie and Furht, Borko},
    title = {Augmented Reality: An Overview},
    journal = {Handbook of Augmented Reality},
    doi = {10.1007/978-1-4614-0064-6\_1},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-1-4614-0064-6\_1},
    abstract = {We define Augmented Reality (AR) as a real-time direct or indirect view of a physical real-world environment that has been enhanced/augmented by adding virtual computer-generated information to it [1]. AR is both interactive and registered in 3D as well as combines real and virtual objects. Milgram’s Reality-Virtuality Continuum is defined by Paul Milgram and Fumio Kishino as a continuum that spans between the real environment and the virtual environment comprise Augmented Reality and Augmented Virtuality (AV) in between, where AR is closer to the real world and AV is closer to a pure virtual environment, as seen in Fig. 1.1 [2].}
}

@Article{10.1007/s10209-013-0338-8,
    author = {Serrão, M. and Shahrabadi, S. and Moreno, M. and José, J. T. and Rodrigues, J. I. and Rodrigues, J. M. F. and du Buf, J. M. H.},
    title = {Computer vision and GIS for the navigation of blind persons in buildings},
    journal = {Universal Access in the Information Society},
    volume = {14.0},
    issue = {1},
    doi = {10.1007/s10209-013-0338-8},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s10209-013-0338-8},
    abstract = {This paper presents a system which integrates a geographic information system of a building with computer vision. It uses only one camera, for example, the one of a mobile phone. Visual landmarks, such as frontal and lateral doors, stairs, signs, and fire extinguishers, are employed for localizing the user in the building and for tracing and validating a route for the user’s navigation. The developed system clearly improves the autonomy of persons with a very low vision during indoor navigation.}
}

@Chapter{10.1007/978-3-319-46672-9_63,
    author = {Chaudhry, Shonal and Chandra, Rohitash},
    title = {Unconstrained Face Detection from a Mobile Source Using Convolutional Neural Networks},
    journal = {Neural Information Processing},
    doi = {10.1007/978-3-319-46672-9\_63},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-46672-9\_63},
    abstract = {We present unconstrained mobile face detection using convolutional neural networks which have potential application for guidance systems for visually impaired persons. We develop a dataset of videos captured from a mobile source that features motion blur and noise from camera shakes. This makes the application a very challenging aspect of unconstrained face detection. The performance of the convolutional neural network is compared with a cascade classifier. The results show promising performance in daylight and artificial lighting conditions while the challenges lie for moonlight conditions with the need for reduction of false positives in order to develop a robust system.}
}

@Chapter{10.1007/978-1-4419-0312-9_21,
    author = {Yu, Donggang and Jin, Jesse Sheng and Luo, Suhuai and Lai, Wei and Huang, Qingming},
    title = {A Useful Visualization Technique: A Literature Review for Augmented Reality and its Application, limitation \\& future direction},
    journal = {Visual Information Communication},
    doi = {10.1007/978-1-4419-0312-9\_21},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-1-4419-0312-9\_21},
    abstract = {Augmented reality (AR), a useful visualization technique, is reviewed based literatures. The AR research methods and applications are surveyed since AR was first developed over forty years ago. Recent and future AR researches are proposed which could help researchers of decide which topics should be developed when they are beginning their own researches in the field.}
}

@Article{10.1007/s10209-009-0171-2,
    author = {Sauer, Graig and Holman, Jonathan and Lazar, Jonathan and Hochheiser, Harry and Feng, Jinjuan},
    title = {Accessible privacy and security: a universally usable human-interaction proof tool},
    journal = {Universal Access in the Information Society},
    volume = {9.0},
    issue = {3},
    doi = {10.1007/s10209-009-0171-2},
    year = {2010},
    url = {http://link.springer.com/article/10.1007/s10209-009-0171-2},
    abstract = {Despite growing interest in designing usable systems for managing privacy and security, recent efforts have generally failed to address the needs of users with disabilities. As security and privacy tools often rely upon subtle visual cues or other potentially inaccessible indicators, users with perceptual limitations might find such tools particularly challenging. To understand the needs of an important group of users with disabilities, a focus group was conducted with blind users to determine their perceptions of security-related challenges. Human-interaction proof (HIP) tools, commonly known as CAPTCHAs, are used by web pages to defeat robots and were identified in the focus group as a major concern. Therefore, a usability test was conducted to see how well blind users were able to use audio equivalents of these graphical tools. Finally, an accessible HIP tool was developed which combines audio and matching images, supporting both visual and audio output. Encouraging results from a small usability evaluation of the prototype with five sighted users and five blind users show that this new form of HIP is preferred by both blind and visual users to previous forms of text-based HIPs. Future directions for research are also discussed.}
}

@Chapter{10.1007/978-3-642-31534-3_6,
    author = {Molina, Edgardo and Zhu, Zhigang and Tian, Yingli},
    title = {Visual Nouns for Indoor/Outdoor Navigation},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-642-31534-3\_6},
    year = {2012},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-31534-3\_6},
    abstract = {We propose a local orientation and navigation framework based on visual features that provide location recognition, context augmentation, and viewer localization information to a human user. Mosaics are used to map local areas to ease user navigation through streets and hallways, by providing a wider field of view (FOV) and the inclusion of more decisive features. Within the mosaics, we extract “visual noun” features. We consider 3 types of visual noun features: signage, visual-text, and visual-icons that we propose as a low-cost method for augmenting environments.}
}

@Chapter{10.1007/978-3-319-04651-8_12,
    author = {Greenwood, Christopher and Nirjon, Shahriar and Stankovic, John and Yoon, Hee Jung and Ra, Ho-Kyeong and Son, Sang and Park, Taejoon},
    title = {KinSpace: Passive Obstacle Detection via Kinect},
    journal = {Wireless Sensor Networks},
    doi = {10.1007/978-3-319-04651-8\_12},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-04651-8\_12},
    abstract = {Falls are a significant problem for the elderly living independently in the home. Many falls occur due to household objects left in open spaces. We present KinSpace, a passive obstacle detection system for the home. KinSpace employs the use of a Kinect sensor to learn the open space of an environment through observation of resident walking patterns. It then monitors the open space for obstacles that are potential tripping hazards and notifies the residents accordingly. KinSpace uses real-time depth data and human-in-the-loop feedback to adjust its understanding of the open space of an environment. We present a 5,000-frame deployment dataset spanning multiple homes and classes of objects. We present results showing the effectiveness of our underlying technical solutions in identifying open spaces and obstacles. The results for both lab testing and a deployment in an actual home show roughly 80\% accuracy for both open space detection and obstacle detection even in the presence of many real-world issues. Consequently, this new technology shows great potential to reduce the risk of falls in the home due to environmental hazards.}
}

@Chapter{10.1007/978-981-10-8108-8_43,
    author = {Zeng, Yi and Li, Duo and Zhai, Guangtao},
    title = {Indoor Localization System for Individuals with Visual Impairment},
    journal = {Digital TV and Wireless Multimedia Communication},
    doi = {10.1007/978-981-10-8108-8\_43},
    year = {2018},
    url = {http://link.springer.com/chapter/10.1007/978-981-10-8108-8\_43},
    abstract = {There are millions of people with visual diseases around the world, most of whom are suffering from inconvenience in their daily lives. In this paper, we demonstrated an indoor localization system for people with visual impairments to help them finding out the right directions without others’ assistance. The system contains three main parts, the hidden display module, the localization module and direction-giving component. Based on temporal psychovisual modulation, hidden display module enables the system to obtain the information required without disturbing normal people. Localization is based on a pair of wearable smart glasses that captures and analyses the surrounding environment. The direction-giving function works by feeding direction back to the user with haptic signals. We test our system by simulating indoor environment and collecting location data. We find out in the same situation, using our system can make subjects find the right places in shorter time compared with random guess.}
}

@Article{10.1007/s11042-015-2745-8,
    author = {Parra, Lorena and Sendra, Sandra and Jiménez, José Miguel and Lloret, Jaime},
    title = {Multimedia sensors embedded in smartphones for ambient assisted living and e-health},
    journal = {Multimedia Tools and Applications},
    volume = {75.0},
    issue = {21},
    doi = {10.1007/s11042-015-2745-8},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s11042-015-2745-8},
    abstract = {Nowadays, it is widely extended the use of smartphones to make human life more comfortable. Moreover, there is a special interest on Ambient Assisted Living (AAL) and e-Health applications. The sensor technology is growing and amount of embedded sensors in the smartphones can be very useful for AAL and e-Health. While some sensors like the accelerometer, gyroscope or light sensor are very used in applications such as motion detection or light meter, there are other ones, like the microphone and camera which can be used as multimedia sensors. This paper reviews the published papers focused on showing proposals, designs and deployments of that make use of multimedia sensors for AAL and e-health. We have classified them as a function of their main use. They are the sound gathered by the microphone and image recorded by the camera. We also include a comparative table and analyze the gathered information.}
}

@Chapter{10.1007/978-3-642-13778-5_22,
    author = {Vorobieva, Hélène and Soury, Mariette and Hède, Patrick and Leroux, Christophe and Morignot, Philippe},
    title = {Object Recognition and Ontology for Manipulation with an Assistant Robot},
    journal = {Aging Friendly Technology for Health and Independence},
    doi = {10.1007/978-3-642-13778-5\_22},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-13778-5\_22},
    abstract = {This article presents a service robotic system for people loosing their autonomy developed at CEA LIST. In the past on SAM robot, we have developed a method for automatic manipulation and object grasping using visual servoing. This method is too stereotyped to correctly grasp objects with complex geometry or to assign particular use to the manipulated object. In this article, we present a new study to adapt the grasping and the usage of an object designed by the user. Our method uses vision object recognition (CBIR) and an ontology for robotic manipulation. This recognition is implemented as a Web Service. It relies on passive vision and does not use a geometric model for grasping. The implementation of this method enables us to automatically search objects in the surrounding areas and to play cognitive and physical stimulation games with the user.}
}

@Article{10.1007/s00138-011-0381-5,
    author = {Metari, S. and Prel, F. and Moszkowicz, T. and Laurendeau, D. and Teasdale, N. and Beauchemin, S. and Simoneau, M.},
    title = {A computer vision framework for the analysis and interpretation of the cephalo-ocular behavior of drivers},
    journal = {Machine Vision and Applications},
    volume = {24.0},
    issue = {1},
    doi = {10.1007/s00138-011-0381-5},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s00138-011-0381-5},
    abstract = {In this paper, we introduce a computer vision system specially designed for the analysis and interpretation of the cephalo-ocular behavior of drivers. The system is composed of both hardware and software components and is described in three steps. The first step is devoted to the description of the driving simulator and the developed software. The second step deals with the identification of the driver’s visual search actions using computer vision. The latter are related to specific driving events such as blind spot checking and rear-view/lateral mirror verification. Based on the simulator’s open module, the third step is concerned with the identification of car/road events (overtaking, crossing an intersection) and the mapping of these events with the driver’s behavior. The proposed system will be used by a kinesiology research group for the evaluation and improvement of driver performances in a safe environment (driving simulator). In addition to the controlled environment, a modified version of the system also deals with real driving contexts (i.e. driving in a real car). Experimental results confirm both the robustness and the effectiveness of the proposed cephalo-ocular analysis framework.}
}

@Article{10.1007/s11042-017-5054-6,
    author = {Abdi, Lotfi and Meddeb, Aref},
    title = {Driver information system: a combination of augmented reality, deep learning and vehicular Ad-hoc networks},
    journal = {Multimedia Tools and Applications},
    volume = {77.0},
    issue = {12},
    doi = {10.1007/s11042-017-5054-6},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-017-5054-6},
    abstract = {Improving traffic safety is one of the important goals of Intelligent Transportation Systems (ITS). In vehicle-based safety systems, it is more desirable to prevent an accident than to reduce severity of injuries. Critical traffic problems such as accidents and traffic congestion require the development of new transportation systems. Research in perceptual and human factors assessment is needed for relevant and correct display of this information for maximal road traffic safety as well as optimal driver comfort. One of the solutions to prevent accidents is to provide information on the surrounding environment of the driver. Augmented Reality Head-Up Display (AR-HUD) can facilitate a new form of dialogue between the vehicle and the driver; and enhance ITS by superimposing surrounding traffic information on the users view and keep drivers view on roads. In this paper, we propose a fast deep-learning-based object detection approaches for identifying and recognizing road obstacles types, as well as interpreting and predicting complex traffic situations. A single convolutional neural network predicts region of interest and class probabilities directly from full images in one evaluation. We also investigated potential costs and benefits of using dynamic conformal AR cues in improving driving safety. A new AR-HUD approach to create real-time interactive traffic animations was introduced in terms of types of obstacle, rules for placement and visibility, and projection of these on an in-vehicle display. The novelty of our approach is that both global and local context information are integrated into a unified framework to distinguish the ambiguous detection outcomes, enhance ITS by superimposing surrounding traffic information on the users view and keep drivers view on roads.}
}

@Article{10.1007/s11042-018-5846-3,
    author = {Talebi, Mehdi and Vafaei, Abbas and Monadjemi, Amirhassan},
    title = {Vision-based entrance detection in outdoor scenes},
    journal = {Multimedia Tools and Applications},
    doi = {10.1007/s11042-018-5846-3},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-5846-3},
    abstract = {Doors are a significant object for the visually impaired and robots to enter and exit buildings. Although the accuracy of door detection is reported high in indoor scenes, it has become a difficult problem in outdoor scenes in computer vision. The reason may lie in the fact that such properties of a simple ordinary door such as handles, corners, and the gap between the door and the ground may not be visible due to the great variety of doors in outdoor environments. In this paper, we present a vision-based method for detecting building entrances in outdoor images. After extracting the lines and deleting the extra ones, regions between the vertical lines are specified and the features including height, width, location, color, texture and the number of lines inside the regions are obtained. Finally, some additional knowledge such as door existence at the bottom of the image, a reasonable height and width of a door, the difference between color and texture of the doors and those of the neighboring regions, and numerous lines on doors is used to decide on door detection. The method was tested on the eTRIMS dataset, door images from the ImageNet dataset, and our own dataset including doors of houses, apartments, and stores leading to acceptable results. The obtained results show that our approach outperforms comparable state-of-the-art approaches.}
}

@Article{10.1007/s00371-016-1250-z,
    author = {Catala, Alejandro and Oliver, Miguel and Molina, Jose Pascual and Gonzalez, Pascual},
    title = {Involving multiple fingers in exploring a haptic surface: an evaluation study},
    journal = {The Visual Computer},
    volume = {32.0},
    issue = {6 - 8},
    doi = {10.1007/s00371-016-1250-z},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s00371-016-1250-z},
    abstract = {In most haptic search tasks, tactile stimuli are usually presented to the fingers to discriminate simulated features and identify patterns. In this paper, we focus on a more complex exploration task in which users have to discriminate different stimuli, move their fingers in a free way to find and locate an object in a wider area of exploration and integrate all the perceived information to determine the position of the object. The study explores how users perform this haptic search task involving one or two fingers on a surface area. In order to carry out this research and overcome limitations of current hardware approaches to multi-point haptic surfaces, we used a setting consisting of a capacitive multi-touch screen and a general-purpose wearable vibrotactile device designed in our laboratory. The results indicate that using one finger in one hand shows to be more effective than using two fingers in either one or two hands in the task under study. Users showed higher confidence, lower exploration times, higher amount of right answers and higher exploration speed. This suggests that great efforts in providing independent multi-point haptic surface hardware could not be a priority for this kind of exploration task.}
}

@Chapter{10.1007/978-3-642-21616-9_19,
    author = {Carrino, Stefano and Mugellini, Elena and Khaled, Omar Abou and Ingold, Rolf},
    title = {ARAMIS: Toward a Hybrid Approach for Human- Environment Interaction},
    journal = {Human-Computer Interaction. Towards Mobile and Intelligent Interaction Environments},
    doi = {10.1007/978-3-642-21616-9\_19},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-21616-9\_19},
    abstract = {In this paper we describe ARAMIS a novel hybrid approach aiming to enhance the human smart-environment interaction. We define this approach as hybrid since it is the combination of three different dichotomies: wearable and pervasive computing paradigms, virtual and real worlds, optical and non-optical sensing technologies. In order to validate the proposed approach we have designed a multimodal framework, in which gestures have been chosen as the main interaction modality. The framework design aims firstly to efficiently manage and merge information from heterogeneous, distributed sensors and secondly to offer a simple tool to connect together such devices. Finally a prototype has been developed in order to test and evaluate the proposed approach.}
}

@Chapter{10.1007/978-3-319-69911-0_3,
    author = {Grega, Michał and Donath, Paweł and Guzik, Piotr and Król, Jakub and Matiolański, Andrzej and Rusek, Krzysztof and Dziech, Andrzej},
    title = {Application of Logistic Regression for Background Substitution},
    journal = {Multimedia Communications, Services and Security},
    doi = {10.1007/978-3-319-69911-0\_3},
    year = {2017},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-69911-0\_3},
    abstract = {The paper presents application of multinomial logistic regression for color segmentation. The common problem in the subject of image understanding is creation of a large enough corpus for algorithm training. Especially when a large set of classes has to be recognized or if using convolutional neural networks the size and diversity of the training set strongly influences the quality of the resulting system. We present a method of automated generation of training samples by combining a well-known green box technique with multinomial logistic regression for background substitution. We show the encountered problems and their solutions. We present numerous examples of algorithm performance in background substitution. We conclude the paper with presentation of other examples of application of logistic regression for image understanding.}
}

@Article{10.1007/s11263-018-1065-7,
    author = {Gurari, Danna and He, Kun and Xiong, Bo and Zhang, Jianming and Sameki, Mehrnoosh and Jain, Suyog Dutt and Sclaroff, Stan and Betke, Margrit and Grauman, Kristen},
    title = {Predicting Foreground Object Ambiguity and Efficiently Crowdsourcing the Segmentation(s)},
    journal = {International Journal of Computer Vision},
    volume = {126.0},
    issue = {7},
    doi = {10.1007/s11263-018-1065-7},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11263-018-1065-7},
    abstract = {We propose the ambiguity problem for the foreground object segmentation task and motivate the importance of estimating and accounting for this ambiguity when designing vision systems. Specifically, we distinguish between images which lead multiple annotators to segment different foreground objects (ambiguous) versus minor inter-annotator differences of the same object. Taking images from eight widely used datasets, we crowdsource labeling the images as “ambiguous” or “not ambiguous” to segment in order to construct a new dataset we call STATIC. Using STATIC, we develop a system that automatically predicts which images are ambiguous. Experiments demonstrate the advantage of our prediction system over existing saliency-based methods on images from vision benchmarks and images taken by blind people who are trying to recognize objects in their environment. Finally, we introduce a crowdsourcing system to achieve cost savings for collecting the diversity of all valid “ground truth” foreground object segmentations by collecting extra segmentations only when ambiguity is expected. Experiments show our system eliminates up to 47\% of human effort compared to existing crowdsourcing methods with no loss in capturing the diversity of ground truths.}
}

@Chapter{10.1007/978-3-319-39952-2_36,
    author = {Schwartz, Michael and Benkert, Denise},
    title = {Navigating with a Visual Impairment: Problems, Tools and Possible Solutions},
    journal = {Foundations of Augmented Cognition: Neuroergonomics and Operational Neuroscience},
    doi = {10.1007/978-3-319-39952-2\_36},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-39952-2\_36},
    abstract = {In this paper we discuss various navigational aids for people who have a visual impairment. Navigational technologies are classified according to the mode of accommodation and the type of sensor utilized to collect environmental information. Notable examples of navigational aids are discussed, along with the advantages and disadvantages of each. Operational and design considerations for navigational aids are suggested. We conclude with a discussion of how multimodal interaction benefits people who use technology as an accommodation and can benefit everyone.}
}

@Chapter{10.1007/978-3-319-58838-4_33,
    author = {Oliveira-Barra, Gabriel and Dimiccoli, Mariella and Radeva, Petia},
    title = {Leveraging Activity Indexing for Egocentric Image Retrieval},
    journal = {Pattern Recognition and Image Analysis},
    doi = {10.1007/978-3-319-58838-4\_33},
    year = {2017},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-58838-4\_33},
    abstract = {Wearable cameras can daily gather large amounts of image data that require powerful image indexing and retrieval techniques in order to find the information of interest. In this work, we address the indexing problem of egocentric data by exploring the relevance of different information sources provided by Convolutional Neural Networks (CNN) combined with image metadata. The proposed method was tested on a public egocentric dataset of 45.000 images and gave encouraging results.}
}

@Chapter{10.1007/978-3-319-67235-9_4,
    author = {Kundu, Rupam and Tummala, Gopi Krishna and Sinha, Prasun},
    title = {Navigation Assistance for Individuals with Visual Impairments in Indoor Environments},
    journal = {Communication Systems and Networks},
    doi = {10.1007/978-3-319-67235-9\_4},
    year = {2017},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-67235-9\_4},
    abstract = {Canes or service dogs in indoor environments are unable to provide spatial information to the Individuals with Visual Impairments (IVIs) to make them independent. An indoor navigation assistance system can provide information on the presence of any obstacles in their vicinity, the distance of separation and their direction of motion (in case of mobile objects) w.r.t the IVIs. In this paper, we attempt to address the above objective by designing a novel time-efficient algorithm where a smart-glass is employed to spot an obstacle (stationary or mobile) in indoor environment using the inbuilt camera and inertial sensors. The system is implemented and tested extensively in indoor settings.}
}

@Article{10.1007/s11042-017-5438-7,
    author = {Abbas, Qaisar and Ibrahim, Mostafa E. A. and Jaffar, M. Arfan},
    title = {Video scene analysis: an overview and challenges on deep learning algorithms},
    journal = {Multimedia Tools and Applications},
    volume = {77.0},
    issue = {16},
    doi = {10.1007/s11042-017-5438-7},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-017-5438-7},
    abstract = {Video scene analysis is a recent research topic due to its vital importance in many applications such as real-time vehicle activity tracking, pedestrian detection, surveillance, and robotics. Despite its popularity, the video scene analysis is still an open challenging task and require more accurate algorithms. However, the advances in deep learning algorithms for video scene analysis have been emerged in last few years for solving the problem of real-time processing. In this paper, a review of the recent developments in deep learning and video scene analysis problems is presented. In addition, this paper also briefly describes the most recent used datasets along with their limitations. Moreover, this review provides a detailed overview of the particular challenges existed in real-time video scene analysis that has been contributed towards activity recognition, scene interpretation, and video description/captioning. Finally, the paper summarizes the future trends and challenges in video scene analysis tasks and our insights are provided to inspire further research efforts.}
}

@Article{10.1007/s10111-017-0442-2,
    author = {Friedland, Heath and Snycerski, Susan and Palmer, Evan M. and Laraway, Sean},
    title = {The effectiveness of glare-reducing glasses on simulated nighttime driving performance in younger and older adults},
    journal = {Cognition, Technology \\& Work},
    volume = {19.0},
    issue = {4},
    doi = {10.1007/s10111-017-0442-2},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s10111-017-0442-2},
    abstract = {Glare from oncoming headlights is a problem for nighttime drivers because it can decrease visual acuity and cause discomfort. This diminished visual ability and discomfort due can increase drivers’ risk for traffic accidents. Older drivers experience more severe detrimental effects from nighttime glare, and these effects may pose a growing roadway hazard as the number of older drivers increases. The increased brightness of popular high-intensity-discharge (HID) headlights may further exacerbate these visibility problems. In a sample of younger (under 40 years of age) and older (40 years of age and older) drivers, we examined the impact of headlight glare from HID and traditional halogen lights on driving performance in a simulator, as well as the effectiveness of novel polarized glare-reducing eyeglasses for mitigating glare-induced performance deficits. The glare-reducing glasses increased visual awareness in the face of oncoming HID headlights compared to halogen headlights in both age groups. Older drivers performed significantly worse than did younger drivers on several measures of driving and visual detection performance. The glare-reducing glasses mitigated performance deficits, with older drivers performing similarly to younger drivers when exposed to HID headlights while wearing the polarized glasses. Due to the introduction of brighter LED-based headlights to the consumer automotive market and an expanding population of older drivers, automotive manufactures should consider glare-mitigation strategies when designing future headlight systems.}
}

@Chapter{10.1007/978-3-319-09387-1_3,
    author = {Nikolić, Zoran},
    title = {Embedded Vision in Advanced Driver Assistance Systems},
    journal = {Advances in Embedded Computer Vision},
    doi = {10.1007/978-3-319-09387-1\_3},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-09387-1\_3},
    abstract = {Throughout history, advances in transportation systems have had large economic and cultural impact. Mobility has changed the way people live and automobiles continue to evolve by becoming smarter and by leveraging cutting-edge technologies. Over the last three decades, we witnessed a tremendous growth of computer vision knowledge through research in academia and industry. More recently, in the last decade, we are finally seeing exciting applications of computer vision. Computer vision plays a fundamental role in the advanced driver assistance systems (ADAS), a field which is of particular interest to the evolution of transportation systems. For example, forward-facing driver assistance functions (such as road sign detection, lane departure warning, and autonomous emergency braking) are heavily relying on information received from a camera. The systems capture video data at high frame rate and process this information in order to warn the driver that the car is moving faster than the posted speed limit or to tell the driver of an unintentional lane drift. The goal of this chapter is to outline key components of ADAS, show how computer vision fits in the system, and describe its contribution to success of ADAS.}
}

@Article{10.1007/s11263-015-0851-8,
    author = {Rohrbach, Marcus and Rohrbach, Anna and Regneri, Michaela and Amin, Sikandar and Andriluka, Mykhaylo and Pinkal, Manfred and Schiele, Bernt},
    title = {Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data},
    journal = {International Journal of Computer Vision},
    volume = {119.0},
    issue = {3},
    doi = {10.1007/s11263-015-0851-8},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s11263-015-0851-8},
    abstract = {Activity recognition has shown impressive progress in recent years. However, the challenges of detecting fine-grained activities and understanding how they are combined into composite activities have been largely overlooked. In this work we approach both tasks and present a dataset which provides detailed annotations to address them. The first challenge is to detect fine-grained activities, which are defined by low inter-class variability and are typically characterized by fine-grained body motions. We explore how human pose and hands can help to approach this challenge by comparing two pose-based and two hand-centric features with state-of-the-art holistic features. To attack the second challenge, recognizing composite activities, we leverage the fact that these activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts. We show the benefits of our hand-centric approach for fine-grained activity classification and detection. For composite activity recognition we find that decomposition into attributes allows sharing information across composites and is essential to attack this hard task. Using script data we can recognize novel composites without having training data for them.}
}

@Chapter{10.1007/978-3-319-16178-5_25,
    author = {Rogez, Grégory and Khademi, Maryam and Montiel, J. S. Supančič IIIJ. M. M. and Ramanan, Deva},
    title = {3D Hand Pose Detection in Egocentric RGB-D Images},
    journal = {Computer Vision - ECCV 2014 Workshops},
    doi = {10.1007/978-3-319-16178-5\_25},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-16178-5\_25},
    abstract = {We focus on the task of hand pose estimation from egocentric viewpoints. For this problem specification, we show that depth sensors are particularly informative for extracting near-field interactions of the camera wearer with his/her environment. Despite the recent advances in full-body pose estimation using Kinect-like sensors, reliable monocular hand pose estimation in RGB-D images is still an unsolved problem. The problem is exacerbated when considering a wearable sensor and a first-person camera viewpoint: the occlusions inherent to the particular camera view and the limitations in terms of field of view make the problem even more difficult. We propose to use task and viewpoint specific synthetic training exemplars in a discriminative detection framework. We also exploit the depth features for a sparser and faster detection. We evaluate our approach on a real-world annotated dataset and propose a novel annotation technique for accurate 3D hand labelling even in case of partial occlusions.}
}

@Chapter{10.1007/978-3-319-16199-0_48,
    author = {Li, Wai Ho},
    title = {A Fast and Flexible Computer Vision System for Implanted Visual Prostheses},
    journal = {Computer Vision - ECCV 2014 Workshops},
    doi = {10.1007/978-3-319-16199-0\_48},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-16199-0\_48},
    abstract = {Implanted visual prostheses generate visual percepts by electrically stimulating the human visual pathway using an array of electrodes. The resulting bionic vision consists of a spatial-temporal pattern of bright dots called phosphenes. This patient-specific phosphene pattern has low resolution, limited dynamic range and is spatially irregular. This paper presents a computer vision system designed to deal with these limitations, especially spatial irregularity. The system uses a new mapping called the Camera Map to decouple the flexible spatial layout of image processing from the inflexible layout of phosphenes experienced by a patient. Detailed simulations of a cortical prosthesis currently in preclinical testing were performed to create phosphene patterns for testing. The system was tested on a wearable prototype of the cortical prosthesis. Despite having limited computational resources, the system operated in real time, taking only a few milliseconds to perform image processing and visualisations of simulated prosthetic vision.}
}

@Article{10.1007/s00371-016-1302-4,
    author = {Wang, Jianhua and Zheng, Chuanxia and Chen, Weihai and Wu, Xingming},
    title = {Learning aggregated features and optimizing model for semantic labeling},
    journal = {The Visual Computer},
    volume = {33.0},
    issue = {12},
    doi = {10.1007/s00371-016-1302-4},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s00371-016-1302-4},
    abstract = {Semantic labeling for indoor scenes has been extensively developed with the wide availability of affordable RGB-D sensors. However, it is still a challenging task for multi-class recognition, especially for “small” objects. In this paper, a novel semantic labeling model based on aggregated features and contextual information is proposed. Given an RGB-D image, the proposed model first creates a hierarchical segmentation using an adapted gPb/UCM algorithm. Then, a support vector machine is trained to predict initial labels using aggregated features, which fuse small-scale appearance features, mid-scale geometric features, and large-scale scene features. Finally, a joint multi-label Conditional random field model that exploits both spatial and attributive contextual relations is constructed to optimize the initial semantic and attributive predicted results. The experimental results on the public NYU v2 dataset demonstrate the proposed model outperforms the existing state-of-the-art methods on the challenging 40 dominant classes task, and the model also achieves a good performance on a recent SUN RGB-D dataset. Especially, the prediction accuracy of “small” classes has been improved significantly.}
}

@Chapter{10.1007/978-3-642-16444-6_38,
    author = {Lee, Changwon and Kim, Minchul and Park, Jinwoo and Oh, Jeonghoon and Eom, Kihwan},
    title = {Development of Wireless RFID Glove for Various Applications},
    journal = {Security-Enriched Urban Computing and Smart Grid},
    doi = {10.1007/978-3-642-16444-6\_38},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-16444-6\_38},
    abstract = {Radio Frequency Identification is increasingly popular technology with many applications. The majority of applications of RFID are supply-chain management. In this paper, we proposed the development of wireless RFID Glove for various applications in real life. Proposed wireless RFID glove is composed of RFID reader of 13.56 MHz and RF wireless module. Proposed Gloves were applied to two applications. First is the interactive leaning and second is Meal aid system for blind people. The experimental results confirmed good performances.}
}

@Chapter{10.1007/978-1-4842-3709-0_4,
    author = {Pathak, Nishith and Bhandari, Anurag},
    title = {Understanding Cognitive APIs},
    journal = {IoT, AI, and Blockchain for .NET},
    doi = {10.1007/978-1-4842-3709-0\_4},
    year = {2018},
    url = {http://link.springer.com/chapter/10.1007/978-1-4842-3709-0\_4},
    abstract = {By now, you are conversant with IoT and have a good understanding of creating a smart application using IoT. Chapter  1 started by introducing your journey to the world of artificial intelligence. As discussed in the previous chapters, we are presently in the first AI revolution, which marks the inception of "AI-as a service". What comes to your mind when you hear the term "artificial intelligence". Scary robots? A topic of sophisticated research? A future of machines that can do complex tasks with a blink of an eye? Normally, developers think of AI implementation as a tough task involving writing complex algorithms and writing hundreds of lines of code. Consider the following conversation:}
}

@Article{10.1007/s11432-012-4649-9,
    author = {De and Li, Ren and Liu, Yong and Xiu and Yuan, Xiao},
    title = {Image-based self-position and orientation method for moving platform},
    journal = {Science China Information Sciences},
    volume = {56.0},
    issue = {4},
    doi = {10.1007/s11432-012-4649-9},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s11432-012-4649-9},
    abstract = {The position and orientation of moving platform mainly depends on global positioning system and inertial navigation system in the field of low-altitude surveying, mapping and remote sensing and land-based mobile mapping system. However, GPS signal is unavailable in the application of deep space exploration and indoor robot control. In such circumstances, image-based methods are very important for self-position and orientation of moving platform. Therefore, this paper firstly introduces state of the art development of the image-based self-position and orientation method (ISPOM) for moving platform from the following aspects: 1) A comparison among major image-based methods (i.e., visual odometry, structure from motion, simultaneous localization and mapping) for position and orientation; 2) types of moving platform; 3) integration schemes of image sensor with other sensors; 4) calculation methodology and quantity of image sensors. Then, the paper proposes a new scheme of ISPOM for mobile robot — depending merely on image sensors. It takes the advantages of both monocular vision and stereo vision, and estimates the relative position and orientation of moving platform with high precision and high frequency. In a word, ISPOM will gradually speed from research to application, as well as play a vital role in deep space exploration and indoor robot control.}
}

@Chapter{10.1007/978-3-642-14100-3_46,
    author = {Murai, Yasuyuki and Kawahara, Masaji and Tatsumi, Hisayuki and Sekita, Iwao and Miyakawa, Masahiro},
    title = {Eye Tracking for Low Vision Aids - Toward Guiding of Gaze},
    journal = {Computers Helping People with Special Needs},
    doi = {10.1007/978-3-642-14100-3\_46},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-14100-3\_46},
    abstract = {Eye tracking technique in the visibility study of low vision was newly introduced in the previous report, where we examined the ease of finding public signs on the streets and in the interior of buildings by low vision people. We got a conclusion that they hardly notice these signs. In this report we continue our research in this direction. We describe details of eye tracking technology applied to low vision. We devise calibration method for low vision. We describe analysis of eye tracking data on the basis of simplified gaze circle model of sight of low vision, leading to a conclusion that it is possible as well for low vision to locate regions of interest (ROI) by applying classical method of scanpath analysis. We also show a preliminary result of public sign recognition in the view by using a fast pattern matching technology called “boosting,” linking to a future system for guiding the gaze of low vision to a missing public sign and zooming into it.}
}

@Article{10.1007/s11042-018-6515-2,
    author = {Obeso, A. Montoya and Benois-Pineau, J. and Vázquez, M. S. García and Acosta, A. A. Ramírez},
    title = {Saliency-based selection of visual content for deep convolutional neural networks},
    journal = {Multimedia Tools and Applications},
    doi = {10.1007/s11042-018-6515-2},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-6515-2},
    abstract = {The automatic description of digital multimedia content was mainly developed for classification tasks, retrieval systems and massive ordering of data. Preservation of cultural heritage is a field of high importance of application of these methods. We address classification problem in cultural heritage such as classification of architectural styles in digital photographs of Mexican cultural heritage. In general, the selection of relevant content in the scene for training classification models makes the models more efficient in terms of accuracy and training time. Here we use a saliency-driven approach to predict visual attention in images and use it to train a Deep Convolutional Neural Network. Also, we present an analysis of the behavior of the models trained under the state-of-the-art image cropping and the saliency maps. To train invariant models to rotations, data augmentation of training set is required, which posses problems of filling normalization of crops, we study were different padding techniques and we find an optimal solution. The results are compared with the state-of-the-art in terms of accuracy and training time. Furthermore, we are studying saliency cropping in training and generalization for another classical task such as weak labeling of massive collections of images containing objects of interest. Here the experiments are conducted on a large subset of ImageNet database. This work is an extension of preliminary research in terms of image padding methods and generalization on large scale generic database.}
}

@Chapter{10.1007/978-0-85729-670-2_2,
    author = {Gong, Shaogang and Xiang, Tao},
    title = {Behaviour in Context},
    journal = {Visual Analysis of Behaviour},
    doi = {10.1007/978-0-85729-670-2\_2},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-0-85729-670-2\_2},
    abstract = {Interpreting behaviour from object action and activity is inherently subject to the context of a visual environment within which action and activity take place. Context embodies not only the spatial and temporal setting, but also the intended functionality of object action and activity. For instance, one recognises, often by inference, whether a hand-held object is a mobile phone or calculator by its relative position to other body parts such as closeness to the ears, even if they are visually similar and partially occluded by the hand. Similarly for behaviour recognition, the arrival of a bus in busy traffic is more likely to be inferred by looking at the passengers’ behaviour at a bus stop. Computer vision research on visual analysis of behaviour embraces a wide range of studies on developing computational models and systems for interpreting behaviour in different contexts. In this chapter, we introduce a range of established topics and emerging trends in visual analysis of behaviour from understanding facial expression, body gesture, action and intent, to the analysis of group activity, crowd and distributed behaviour, and gaining holistic awareness.}
}

@Chapter{10.1007/978-3-319-40244-4_43,
    author = {Partarakis, Nikolaos and Klironomos, Iosif and Antona, Margherita and Margetis, George and Grammenos, Dimitris and Stephanidis, Constantine},
    title = {Accessibility of Cultural Heritage Exhibits},
    journal = {Universal Access in Human-Computer Interaction. Interaction Techniques and Environments},
    doi = {10.1007/978-3-319-40244-4\_43},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-40244-4\_43},
    abstract = {The global impact of the digital revolution in the cultural sector worldwide brings about the need to ensure the accessibility of physical exhibits, interactive digital exhibits, digital media and digital content for disabled people. The paper addresses the accessibility of CH resources, and the need for a new approach to accessible user interaction with CH exhibits.
}
}

@Chapter{10.1007/978-3-319-24702-1_11,
    author = {Agrawal, Harsh and Mathialagan, Clint Solomon and Goyal, Yash and Chavali, Neelima and Banik, Prakriti and Mohapatra, Akrit and Osman, Ahmed and Batra, Dhruv},
    title = {CloudCV: Large-Scale Distributed Computer Vision as a Cloud Service},
    journal = {Mobile Cloud Visual Media Computing},
    doi = {10.1007/978-3-319-24702-1\_11},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-24702-1\_11},
    abstract = {We are witnessing a proliferation of massive visual data. Unfortunately, scaling existing computer vision algorithms to large datasets leaves researchers repeatedly solving the same algorithmic, logistical, and infrastructural problems. Our goal is to democratize computer vision; one should not have to be a computer vision, big data, and distributed computing expert to have access to state-of-the-art distributed computer vision algorithms. We present CloudCV, a comprehensive system to provide access to state-of-the-art distributed computer vision algorithms as a cloud service through a web interface and APIs.}
}

@Article{10.1007/s11263-014-0794-5,
    author = {Lee, Yong Jae and Grauman, Kristen},
    title = {Predicting Important Objects for Egocentric Video Summarization},
    journal = {International Journal of Computer Vision},
    volume = {114.0},
    issue = {1},
    doi = {10.1007/s11263-014-0794-5},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s11263-014-0794-5},
    abstract = {We present a video summarization approach for egocentric or “wearable” camera data. Given hours of video, the proposed method produces a compact storyboard summary of the camera wearer’s day. In contrast to traditional keyframe selection techniques, the resulting summary focuses on the most important objects and people with which the camera wearer interacts. To accomplish this, we develop region cues indicative of high-level saliency in egocentric video—such as the nearness to hands, gaze, and frequency of occurrence—and learn a regressor to predict the relative importance of any new region based on these cues. Using these predictions and a simple form of temporal event detection, our method selects frames for the storyboard that reflect the key object-driven happenings. We adjust the compactness of the final summary given either an importance selection criterion or a length budget; for the latter, we design an efficient dynamic programming solution that accounts for importance, visual uniqueness, and temporal displacement. Critically, the approach is neither camera-wearer-specific nor object-specific; that means the learned importance metric need not be trained for a given user or context, and it can predict the importance of objects and people that have never been seen previously. Our results on two egocentric video datasets show the method’s promise relative to existing techniques for saliency and summarization.}
}

@Chapter{10.1007/978-1-4471-7278-9_2,
    author = {Cai, Yang},
    title = {Default Mode},
    journal = {Instinctive Computing},
    doi = {10.1007/978-1-4471-7278-9\_2},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-1-4471-7278-9\_2},
    abstract = {We often take everyday common sense for granted, frequently acting without thinking. Many subconscious processes are instinctual, but they can also be learned and evolve. When a baby is born, its communication is limited to one mode – crying – which signals to the parents if the baby is feeling a lack of attention, hunger, or pain. When a creature has limited means of survival or is in risky situations that other means are not available, the creature typically reverts to a survival or backup state, so-called default mode, and performs survival actions. It uses simple algorithms to cope with complex and dynamic situations. In this chapter, we will explore how to use default mode algorithms and default knowledge to solve challenging problems in modern control, network, and vision systems such as collision handling in networks, exception-handling in autonomous vehicles, and object recognition.}
}

@Article{10.1186/s40537-015-0031-2,
    author = {Olshannikova, Ekaterina and Ometov, Aleksandr and Koucheryavy, Yevgeni and Olsson, Thomas},
    title = {Visualizing Big Data with augmented and virtual reality: challenges and research agenda},
    journal = {Journal of Big Data},
    volume = {2.0},
    issue = {1},
    doi = {10.1186/s40537-015-0031-2},
    year = {2015},
    url = {http://link.springer.com/article/10.1186/s40537-015-0031-2},
    abstract = {This paper provides a multi-disciplinary overview of the research issues and achievements in the field of Big Data and its visualization techniques and tools. The main aim is to summarize challenges in visualization methods for existing Big Data, as well as to offer novel solutions for issues related to the current state of Big Data Visualization. This paper provides a classification
 of existing data types, analytical methods, visualization techniques and tools, with a particular emphasis placed on surveying the evolution of visualization methodology over the past years. Based on the results, we reveal disadvantages of existing visualization methods. Despite the technological development of the modern world, human involvement (interaction), judgment and logical thinking are necessary while working with Big Data. Therefore, the role of human perceptional limitations involving large amounts of information is evaluated. Based on the results, a non-traditional approach is proposed: we discuss how the capabilities of Augmented Reality and Virtual Reality could be applied to the field of Big Data Visualization. We discuss the promising utility of Mixed Reality technology integration with applications in Big Data Visualization. Placing the most essential data in the central area of the human visual field in Mixed Reality would allow one to obtain the presented information in a short period of time without significant data losses due to human perceptual issues. Furthermore, we discuss the impacts of new technologies, such as Virtual Reality displays and Augmented Reality helmets on the Big Data visualization as well as to the classification of the main challenges of integrating the technology.}
}

@Chapter{10.1007/978-3-319-44550-2_4,
    author = {Olshannikova, Ekaterina and Ometov, Aleksandr and Koucheryavy, Yevgeni and Olsson, Thomas},
    title = {Visualizing Big Data},
    journal = {Big Data Technologies and Applications},
    doi = {10.1007/978-3-319-44550-2\_4},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-44550-2\_4},
    abstract = {This chapter provides a multi-disciplinary overview of the research issues and achievements in the field of Big Data and its visualization techniques and tools. The main aim is to summarize challenges in visualization methods for existing Big Data, as well as to offer novel solutions for issues related to the current state of Big Data Visualization. This paper provides a classification of existing data types, analytical methods, visualization techniques and tools, with a particular emphasis placed on surveying the evolution of visualization methodology over the past years. Based on the results, we reveal disadvantages of existing visualization methods. Despite the technological development of the modern world, human involvement (interaction), judgment and logical thinking are necessary while working with Big Data. Therefore, the role of human perceptional limitations involving large amounts of information is evaluated. Based on the results, a non-traditional approach is proposed: we discuss how the capabilities of Augmented Reality and Virtual Reality could be applied to the field of Big Data Visualization. We discuss the promising utility of Mixed Reality technology integration with applications in Big Data Visualization. Placing the most essential data in the central area of the human visual field in Mixed Reality would allow one to obtain the presented information in a short period of time without significant data losses due to human perceptual issues. Furthermore, we discuss the impacts of new technologies, such as Virtual Reality displays and Augmented Reality helmets on the Big Data visualization as well as to the classification of the main challenges of integrating the technology.}
}

@Chapter{10.1007/978-3-642-23678-5_19,
    author = {Rezaei, Mahdi and Klette, Reinhard},
    title = {3D Cascade of Classifiers for Open and Closed Eye Detection in Driver Distraction Monitoring},
    journal = {Computer Analysis of Images and Patterns},
    doi = {10.1007/978-3-642-23678-5\_19},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-23678-5\_19},
    abstract = {Eye status detection and localization is a fundamental step for driver awareness detection. The efficiency of any learning-based object detection method highly depends on the training dataset as well as learning parameters. The research develops optimum values of Haar-training parameters to create a nested cascade of classifiers for real-time eye status detection. The detectors can detect eye-status of open, closed, or diverted not only from frontal faces but also for rotated or tilted head poses. We discuss the unique features of our robust training database that significantly influenced the detection performance. The system has been practically implemented and tested in real-world and real-time processing with satisfactory results on determining driver’s level of vigilance.}
}

@Chapter{10.1007/978-3-319-25178-3_3,
    author = {Möller, Dietmar P. F.},
    title = {Introduction to Cyber-Physical Systems},
    journal = {Guide to Computing Fundamentals in Cyber-Physical Systems},
    doi = {10.1007/978-3-319-25178-3\_3},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-25178-3\_3},
    abstract = {This chapter begins with an overview of cyber-physical systems in Sect. 3.1 taking into account the introduction to systems in Chap.  1 and embedded computing systems in Chap.  2. Thereafter, Sect. 3.2 concentrates to recommendations with regard to cyber-physical systems design. In Sect. 3.3 cyber-physical system requirements are described to emphasize disciplined approaches to their design. Section 3.4 introduces the opportunities created applying the cyber-physical technology in a wide range of domains, offering numerous opportunities in products and applications. Section 3.5 refers to smart cities and the Internet of Everything. The case study in Sect. 3.6 focuses on cyber-physical vehicle tracking. Section 3.7 contains comprehensive questions from the cyber-physical systems domain, followed by references and suggestions for further reading.}
}

@Chapter{10.1007/978-3-662-45317-9_7,
    author = {Łubkowski, Piotr and Laskowski, Dariusz},
    title = {Selected Issues of Reliable Identification of Object in Transport Systems Using Video Monitoring Services},
    journal = {Telematics - Support for Transport},
    doi = {10.1007/978-3-662-45317-9\_7},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-662-45317-9\_7},
    abstract = {The process of ensuring the security of monitored objects and material goods requires access to the information from sensors located in different points of monitoring and data acquisition systems. Therefore, they are becoming widely used in the transport services where they are needed for the administration of car fleet or the visualization of selected functions of transport processes. In each of these applications, the reliability and quality of the processes of supervision and monitoring is an important issue. Unfortunately, transmission of data from multiple video sensors may lead to degradation of video quality and reliability of identification. The paper presents a study of selected QoS (Quality of Services) supporting mechanisms and components elaborated for provision of reliable video monitoring services. The results of research have been presented as well which show the efficiency of the developed mechanisms in the process of reliable identification of objects in the transport system.}
}

@Chapter{10.1007/978-3-319-27149-1_42,
    author = {Sanz, David and Ahmad, Aamir and Lima, Pedro},
    title = {Onboard Robust Person Detection and Tracking for Domestic Service Robots},
    journal = {Robot 2015: Second Iberian Robotics Conference},
    doi = {10.1007/978-3-319-27149-1\_42},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-27149-1\_42},
    abstract = {Domestic assistance for the elderly and impaired people is one of the biggest upcoming challenges of our society. Consequently, in-home care through domestic service robots is identified as one of the most important application area of robotics research. Assistive tasks may range from visitor reception at the door to catering for owner’s small daily necessities within a house. Since most of these tasks require the robot to interact directly with humans, a predominant robot functionality is to detect and track humans in real time: either the owner of the robot or visitors at home or both. In this article we present a robust method for such a functionality that combines depth-based segmentation and visual detection. The robustness of our method lies in its capability to not only identify partially occluded humans (e.g., with only torso visible) but also to do so in varying lighting conditions. We thoroughly validate our method through extensive experiments on real robot datasets and comparisons with the ground truth. The datasets were collected on a home-like environment set up within the context of RoboCup@Home and RoCKIn@Home competitions.}
}

@Chapter{10.1007/978-1-4471-5134-0_4,
    author = {Ritter, Frank E. and Baxter, Gordon D. and Churchill, Elizabeth F.},
    title = {Behavior: Basic Psychology of the User},
    journal = {Foundations for Designing User-Centered Systems},
    doi = {10.1007/978-1-4471-5134-0\_4},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-1-4471-5134-0\_4},
    abstract = {This chapter examines what are described as user behavioral characteristics. These are characteristics that are related to perception in broad terms. The chapter starts by defining some behavioral terms and concepts that are used in this and subsequent chapters. We then describe in detail several aspects of the two main perceptual systems that are involved in interacting with computer-based systems: vision and hearing. For each of these aspects we consider some of the implications they have for system design. We finish by introducing the topic of motivation to help explain why individual users may behave in a particular way when carrying out a task.
}
}

@Article{10.1007/s00138-013-0525-x,
    author = {Oh, Sangmin and Mc, Scott and Closkey and Kim, Ilseo and Vahdat, Arash and Cannons, Kevin J. and Hajimirsadeghi, Hossein and Mori, Greg and Perera, A. G. Amitha and Pandey, Megha and Corso, Jason J.},
    title = {Multimedia event detection with multimodal feature fusion and temporal concept localization},
    journal = {Machine Vision and Applications},
    volume = {25.0},
    issue = {1},
    doi = {10.1007/s00138-013-0525-x},
    year = {2014},
    url = {http://link.springer.com/article/10.1007/s00138-013-0525-x},
    abstract = {We present a system for multimedia event detection. The developed system characterizes complex multimedia events based on a large array of multimodal features, and classifies unseen videos by effectively fusing diverse responses. We present three major technical innovations. First, we explore novel visual and audio features across multiple semantic granularities, including building, often in an unsupervised manner, mid-level and high-level features upon low-level features to enable semantic understanding. Second, we show a novel Latent SVM model which learns and localizes discriminative high-level concepts in cluttered video sequences. In addition to improving detection accuracy beyond existing approaches, it enables a unique summary for every retrieval by its use of high-level concepts and temporal evidence localization. The resulting summary provides some transparency into why the system classified the video as it did. Finally, we present novel fusion learning algorithms and our methodology to improve fusion learning under limited training data condition. Thorough evaluation on a large TRECVID MED 2011 dataset showcases the benefits of the presented system.}
}

@Chapter{10.1007/978-0-85729-997-0_23,
    author = {Gong, Shaogang and Loy, Chen Change and Xiang, Tao},
    title = {Security and Surveillance},
    journal = {Visual Analysis of Humans},
    doi = {10.1007/978-0-85729-997-0\_23},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-0-85729-997-0\_23},
    abstract = {Human eyes are highly efficient devices for scanning through a large quantity of low-level visual sensory data and delivering selective information to one’s brain for high-level semantic interpretation and gaining situational awareness. Over the last few decades, the computer vision community has endeavoured to bring about similar perceptual capabilities to artificial visual sensors. Substantial efforts have been made towards understanding static images of individual objects and the corresponding processes in the human visual system. This endeavour is intensified further by the need for understanding a massive quantity of video data, with the aim to comprehend multiple entities not only within a single image but also over time across multiple video frames for understanding their spatio-temporal relations. A significant application of video analysis and understanding is intelligent surveillance, which aims to interpret automatically human activity and detect unusual events that could pose a threat to public security and safety.}
}

@Article{10.1007/s11263-017-1001-2,
    author = {Jayaraman, Dinesh and Grauman, Kristen},
    title = {Learning Image Representations Tied to Egomotion from Unlabeled Video},
    journal = {International Journal of Computer Vision},
    volume = {125.0},
    issue = {1 - 3},
    doi = {10.1007/s11263-017-1001-2},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11263-017-1001-2},
    abstract = {Understanding how images of objects and scenes behave in response to specific egomotions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose a new “embodied” visual learning paradigm, exploiting proprioceptive motor signals to train visual representations from egocentric video with no manual supervision. Specifically, we enforce that our learned features exhibit equivariance i.e., they respond predictably to transformations associated with distinct egomotions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.}
}

@Chapter{10.1007/978-3-642-33868-7_3,
    author = {Wang, Zhe and Liu, Hong and Qian, Yueliang and Xu, Tao},
    title = {Real-Time Plane Segmentation and Obstacle Detection of 3D Point Clouds for Indoor Scenes},
    journal = {Computer Vision – ECCV 2012. Workshops and Demonstrations},
    doi = {10.1007/978-3-642-33868-7\_3},
    year = {2012},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-33868-7\_3},
    abstract = {Scene analysis is an important issue in computer vision and extracting structural information is one of the fundamental techniques. Taking advantage of depth camera, we propose a novel fast plane segmentation algorithm and use it to detect obstacles in indoor environment. The proposed algorithm has two steps: the initial segmentation and the refined segmentation. Firstly, depth image is converted into 3D point cloud and divided into voxels, which are less sensitive to noises compared with pixels. Then area-growing algorithm is used to extract the candidate planes according to the normal of each voxel. Secondly, each point that hasn’t been classified to any plane is examined whether it actually belongs to a plane. The two-step strategy has been proven to be a fast segmentation method with high accuracy. The experimental results demonstrate that our method can segment planes and detect obstacles in real-time with high accuracy for indoor scenes.}
}

@Article{10.1007/s00779-014-0769-0,
    author = {Kim, Seokhwan and Takahashi, Shin and Tanaka, Jiro},
    title = {A location-sensitive visual interface on the palm: interacting with common objects in an augmented space},
    journal = {Personal and Ubiquitous Computing},
    volume = {19.0},
    issue = {1},
    doi = {10.1007/s00779-014-0769-0},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s00779-014-0769-0},
    abstract = {We have created a visual interface
 using the human palm that is location sensitive and always available. To accomplish this, we constructed an augmented space in an actual workspace by installing several depth cameras. To manage and connect the multiple depth cameras, we constructed a distributed system based on scalable client––server architecture. By merging depth images from different cameras, the distributed system can track the locations of users within their area of coverage. The system also has a convenient feature that allows users to collect the locations of objects while visualizing the objects via images from the depth cameras. Consequently, the locations of both users and objects are available to the system, thus providing a location-based context for determining which user is close to which object. As a result, the visual interface on the palm becomes location sensitive, which could lead to various applications in daily life. In this paper, we describe the implementation of the aforementioned system and demonstrate its potential applicability.}
}

@Article{10.1007/s11042-018-5730-1,
    author = {Avola, Danilo and Cinque, Luigi and Foresti, Gian Luca and Marini, Marco Raoul and Pannone, Daniele},
    title = {VRheab: a fully immersive motor rehabilitation system based on recurrent neural network},
    journal = {Multimedia Tools and Applications},
    doi = {10.1007/s11042-018-5730-1},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-5730-1},
    abstract = {In this paper, a fully immersive serious game system that combines two Natural User Interfaces (NUIs) and a Head Mounted Display (HMD) to provide an interactive Virtual Environment (VE) for patient rehabilitation is proposed. Patients’ data are acquired in real-time by the NUIs, while by the HMD the VE is shown to them, thus allowing the interaction. A Long Short-Term Memory Recurrent Neural Network (LSTM-RNN), previously trained by healthy subjects (i.e., baseline), processes patients’ movements in real-time during the rehabilitation exercises to provide the degree of their performance. By comparing the functionalities of the proposed system with the ongoing state-of-the-art, it is worth noting that the reported fully immersive serious game system provides a concrete contribute to the current literature in terms of completeness and versatility. The results obtained by three rehabilitation exercises, chosen as reference case studies, performed on real patients affected by Parkinson’s disease have shown the effectiveness of the presented approach. Finally, the analysis of the feedbacks received by the therapists and patients who have used the system have highlighted remarkable results in terms of motivation, acceptance, and usability.}
}

@Chapter{10.1007/978-3-319-13560-1_33,
    author = {Alhwarin, Faraj and Ferrein, Alexander and Scholl, Ingrid},
    title = {IR Stereo Kinect: Improving Depth Images by Combining Structured Light with IR Stereo},
    journal = {PRICAI 2014: Trends in Artificial Intelligence},
    doi = {10.1007/978-3-319-13560-1\_33},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-13560-1\_33},
    abstract = {RGB-D sensors such as the Microsoft Kinect or the Asus Xtion are inexpensive 3D sensors. A depth image is computed by calculating the distortion of a known infrared light (IR) pattern which is projected into the scene. While these sensors are great devices they have some limitations. The distance they can measure is limited and they suffer from reflection problems on transparent, shiny, or very matte and absorbing objects. If more than one RGB-D camera is used the IR patterns interfere with each other. This results in a massive loss of depth information. In this paper, we present a simple and powerful method to overcome these problems. We propose a stereo RGB-D camera system which uses the pros of RGB-D cameras and combine them with the pros of stereo camera systems. The idea is to utilize the IR images of each two sensors as a stereo pair to generate a depth map. The IR patterns emitted by IR projectors are exploited here to enhance the dense stereo matching even if the observed objects or surfaces are texture-less or transparent. The resulting disparity map is then fused with the depth map offered by the RGB-D sensor to fill the regions and the holes that appear because of interference, or due to transparent or reflective objects. Our results show that the density of depth information is increased especially for transparent, shiny or matte objects.}
}

@Article{10.1007/s11390-017-1751-x,
    author = {Ou, Xin-Yu and Li, Ping and Ling, He-Fei and Liu, Si and Wang, Tian-Jiang and Li, Dan},
    title = {Objectness Region Enhancement Networks for Scene Parsing},
    journal = {Journal of Computer Science and Technology},
    volume = {32.0},
    issue = {4},
    doi = {10.1007/s11390-017-1751-x},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11390-017-1751-x},
    abstract = {Semantic segmentation has recently witnessed rapid progress, but existing methods only focus on identifying objects or instances. In this work, we aim to address the task of semantic understanding of scenes with deep learning. Different from many existing methods, our method focuses on putting forward some techniques to improve the existing algorithms, rather than to propose a whole new framework. Objectness enhancement is the first effective technique. It exploits the detection module to produce object region proposals with category probability, and these regions are used to weight the parsing feature map directly. “Extra background” category, as a specific category, is often attached to the category space for improving parsing result in semantic and instance segmentation tasks. In scene parsing tasks, extra background category is still beneficial to improve the model in training. However, some pixels may be assigned into this nonexistent category in inference. Black-hole filling technique is proposed to avoid the incorrect classification. For verifying these two techniques, we integrate them into a parsing framework for generating parsing result. We call this unified framework as Objectness Enhancement Network (OENet). Compared with previous work, our proposed OENet system effectively improves the performance over the original model on SceneParse150 scene parsing dataset, reaching 38.4 mIoU (mean intersectionover-union) and 77.9\% accuracy in the validation set without assembling multiple models. Its effectiveness is also verified on the Cityscapes dataset.}
}

@Article{10.1007/s10044-013-0354-6,
    author = {Filipe, Sílvio and Alexandre, Luís A.},
    title = {Algorithms for invariant long-wave infrared face segmentation: evaluation and comparison},
    journal = {Pattern Analysis and Applications},
    volume = {17.0},
    issue = {4},
    doi = {10.1007/s10044-013-0354-6},
    year = {2014},
    url = {http://link.springer.com/article/10.1007/s10044-013-0354-6},
    abstract = {This paper 
presents two methods for automatic segmentation of images of faces captured in long wavelength infrared, allowing a wide range of face
 rotations, expressions and artifacts (such as glasses and hats). We also present the validation of segmentation results using a recognition method to show the impact of the segmentation accuracy on the recognition. The paper presents two different approaches (one aimed at real-time performance and the other at high accuracy) and compares their performance against three other previously published methods. The proposed approaches are based on statistical modeling of pixel intensities and active contour application, although several other image processing operations are also performed. Experiments were performed on a total of 893 test images from four public available databases. The obtained results improve on previous existing methods up to 29.5 \% for the first measure error (E 1) and up to 34.7 \% for the second measure (E 2), depending on the method and database. Regarding the computational time, our proposals improve up to 63.32 \% when compared with the other proposals. We also present the validation of the various segmentation methods that are presented by applying a face recognition method.}
}

@Chapter{10.1007/978-1-4471-7278-9_5,
    author = {Cai, Yang},
    title = {Primitive Learning},
    journal = {Instinctive Computing},
    doi = {10.1007/978-1-4471-7278-9\_5},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-1-4471-7278-9\_5},
    abstract = {Natural selection has produced instinctive behaviors through the slow and gradual accumulation of numerous slight, yet advantageous variations. The result is automation, a behavior that is second nature. This is called the Baldwin Effect. This principle can be applied to both humans and machines. In order to enable a machine to learn as efficiently as a human or animal, we need to understand primitive learning processes within humans, animals, and even insects. In this chapter, we explore primitive learning behaviors, including perceptual adaptive learning, peak-shift, lateral learning, learn-by-tapping, and indirect learning from virtual experiences, such as playing or gaming.}
}

@Chapter{10.1007/978-3-319-94361-9_4,
    author = {Xia, Weihao and Yang, Chengxi and Yang, Yujiu and Sun, Wenxiu},
    title = {Matching Low-Quality Photo to DSLR-Quality with Deep Convolutional Networks},
    journal = {Artificial Intelligence and Mobile Services – AIMS 2018},
    doi = {10.1007/978-3-319-94361-9\_4},
    year = {2018},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-94361-9\_4},
    abstract = {Off-the-shelf smartphone cameras typical fail to achieve the quality results of Digital Single Lens Reflex (DSLR) cameras due to their physical limitations. In the cases of autonomous driving or surveillance systems where primitive cameras are usually employed, follow-up work may hardly proceed since the low-quality images result in strong obstacles. However, most existing photo quality enhancement methods focus on certain attributes such as super-resolution, generic photo quality enhancement has not been addressed as its entirely. In this work, we formulate this problem as an image quality matching problem under image translation framework and propose an end-to-end learning approach that translates low-quality photos captured by cameras with limited capabilities into DSLR-quailty photos. Unlike most other methods without direction of enhancement, our approach matches low-quality photos to DSLR-quailty counterparts. Qualitative and quantitative comparisons have shown that our method improves the existing state-of-art in terms of structural similarity measure, peak signal-to-noise ratio and by visual appearance, where artifacts and content changes are significantly reduced. Extensive experiments show its potential as a preprocessing module to translate image quality to target domain.}
}

@Article{10.1631/FITEE.1700808,
    author = {Zhang, Quan-shi and Zhu, Song-chun},
    title = {Visual interpretability for deep learning: a survey},
    journal = {Frontiers of Information Technology \\& Electronic Engineering},
    volume = {19.0},
    issue = {1},
    doi = {10.1631/FITEE.1700808},
    year = {2018},
    url = {http://link.springer.com/article/10.1631/FITEE.1700808},
    abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles’ heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.}
}

@Article{10.1007/s11042-016-3394-2,
    author = {Tawadrous, Mina and Rojas, David and Kapralos, Bill and Hogue, Andrew and Dubrowski, Adam},
    title = {The effects of stereoscopic 3D on knowledge retention within a serious gaming environment},
    journal = {Multimedia Tools and Applications},
    volume = {76.0},
    issue = {5},
    doi = {10.1007/s11042-016-3394-2},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11042-016-3394-2},
    abstract = {We present the results of an experiment that investigated the effects of stereoscopic 3D viewing on knowledge retention with respect to a spatial interactive task within a serious game that was designed for fire safety training. Participants were trained to identify the safe distance to remain from a (virtual) fire in both stereoscopic 3D and non-stereoscopic 3D contexts. After a 24 h period, they were then tested to determine whether they retained the information that they were taught. Contrary to prior work that suggests stereoscopic 3D has an impact on knowledge retention, our results indicate no significant difference between knowledge retention in a stereoscopic 3D versus a non-stereoscopic 3D interactive environment. Although greater work remains to be done and no firm conclusions can be made regarding the use of stereoscopic 3D, our results have shown that stereoscopic 3D does not always lead to greater performance. Our results have implications for designers of serious games; the discussion and decision to use stereoscopic 3D should be incorporated early in the design phase and there should be some consideration placed on individualized calibration of stereoscopic 3D settings.}
}

@Chapter{10.1007/978-3-319-19324-3_6,
    author = {Laskowski, Łukasz and Jelonkiewicz, Jerzy and Hayashi, Yoichi},
    title = {Extensions of Hopfield Neural Networks for Solving of Stereo-Matching Problem},
    journal = {Artificial Intelligence and Soft Computing},
    doi = {10.1007/978-3-319-19324-3\_6},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-19324-3\_6},
    abstract = {Paper considers three Hopfield based architectures in the stereo matching problem solving. Together with classical analogue Hopfield structure two novel architectures are examined: Hybrid-Maximum Neural Network and Self Correcting Neural Network.Energy functions that are crucial for the network performance and working algorithm are also presented.All considered structures are tested to compare their performance features. Two of them are particularly important: accuracy and computational time. For the experiment real and simulated stereo images are used. Obtained results lead to the conclusion about feasibility of considered architectures in the stereo matching problem solving for real time applications.}
}

@Chapter{10.1007/978-3-642-39330-3_43,
    author = {Tung, Tony and Gomez, Randy and Kawahara, Tatsuya and Matsuyama, Takashi},
    title = {Multi-party Human-Machine Interaction Using a Smart Multimodal Digital Signage},
    journal = {Human-Computer Interaction. Interaction Modalities and Techniques},
    doi = {10.1007/978-3-642-39330-3\_43},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-39330-3\_43},
    abstract = {In this paper, we present a novel multimodal system designed for smooth multi-party human-machine interaction. HCI for multiple users is challenging because simultaneous actions and reactions have to be consistent. Here, the proposed system consists of a digital signage or large display equipped with multiple sensing devices: a 19-channel microphone array, 6 HD video cameras (3 are placed on top and 3 on the bottom of the display), and two depth sensors. The display can show various contents, similar to a poster presentation, or multiple windows (e.g., web browsers, photos, etc.). On the other hand, multiple users positioned in front of the panel can freely interact using voice or gesture while looking at the displayed contents, without wearing any particular device (such as motion capture sensors or head mounted devices). Acoustic and visual information processing are performed jointly using state-of-the-art techniques to obtain individual speech and gaze direction. Hence displayed contents can be adapted to users’ interests.}
}

@Article{10.1007/s11042-018-6458-7,
    author = {Khoh, Wee How and Pang, Ying Han and Teoh, Andrew Beng Jin},
    title = {In-air hand gesture signature recognition system based on 3-dimensional imagery},
    journal = {Multimedia Tools and Applications},
    doi = {10.1007/s11042-018-6458-7},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-6458-7},
    abstract = {A traditional online handwritten signature recognition system requires direct contact to acquisition device and usually will leave a traceable print on the surface. This made a signature possible and vulnerable to certain attempts of tracking and imitated. Looking into this shortfall, this paper proposes a novel approach to recognise an individual based on his/ her in-air hand motion while signing his/her signature. In this study, a low-cost acquisition device – Microsoft Kinect sensor is adopted to capture an image sequence of hand gesture signature. Palm region is first located and segmented through a predictive palm segmentation algorithm, which are then combined to generate a volume data. The volume data is condensed and reduced into a motion representation image by means of Motion History Image (MHI), which produces rich motion and temporal information. Several features are extracted from the MHI for empirical evaluation. Two classical recognition modes – identification and verification, are testified with an in-house database (HGS database). The proposed system achieves 90.4\% identification accuracy and 3.22\% equal error rate in verification mode. The experimental results substantiated the potential of the proposed system.}
}

@Chapter{10.1007/978-3-642-21326-7_21,
    author = {Olmedo-Payá, Andrés and Martínez-Álvarez, Antonio and Cuenca-Asensi, Sergio and Ferrández, Jose M. and Fernández, Eduardo},
    title = {An Optimized Framework to Model Vertebrate Retinas},
    journal = {New Challenges on Bioinspired Applications},
    doi = {10.1007/978-3-642-21326-7\_21},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-21326-7\_21},
    abstract = {The retina is a very complex neural structure, which contains many different types of neurons interconnected with great precision, enabling sophisticated conditioning and coding of the visual information before it is passed via the optic nerve to higher visual centers. Therefore the retina performs spatial, temporal, and chromatic processing on visual information and converts it into a compact ’digital’ format composed of neural impulses. However, how groups of retinal ganglion cells encode a broad range of visual information is still a challenging and unresolved question. The main objective of this work is to design and develop a new functional tool able to describe, simulate and validate custom retina models. The whole system is optimized for visual neuroprosthesis and can be accelerated by using FPGAs, COTS microprocessors or GP-GPU based systems.}
}

@Chapter{10.1007/978-3-642-23602-0_20,
    author = {Al-Kuwari, Saif and Wolthusen, Stephen D.},
    title = {On the Feasibility of Carrying Out Live Real-Time Forensics for Modern Intelligent Vehicles},
    journal = {Forensics in Telecommunications, Information, and Multimedia},
    doi = {10.1007/978-3-642-23602-0\_20},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-23602-0\_20},
    abstract = {Abstract" id="Abs1" tabindex="-1" lang="en">SummaryModern vehicular systems exhibit a number of networked electronic components ranging from sensors and actuators to dedicated vehicular subsystems. These components/systems, and the fact that they are interconnected, raise questions as to whether they are suitable for digital forensic investigations. We found that this is indeed the case especially when the data produced by such components are properly obtained and fused (such as fusing location with audio/video data). In this paper we therefore investigate the relevant advanced automotive electronic components and their respective network configurations and functions with particular emphasis on the suitability for live (real time) forensic investigations and surveillance based on augmented software and/or hardware configurations related to passenger behaviour analysis. To this end, we describe subsystems from which sensor data can be obtained directly or with suitable modifications; we also discuss different automotive network and bus structures, and then proceed by describing several scenarios for the application of such behavioural analysis.}
}

@Article{10.1007/s10044-018-0741-0,
    author = {Nowosielski, Adam and Forczmański, Paweł},
    title = {Touchless typing with head movements captured in thermal spectrum},
    journal = {Pattern Analysis and Applications},
    doi = {10.1007/s10044-018-0741-0},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s10044-018-0741-0},
    abstract = {Many physically challenged people are unable to operate standard electronic equipment or computer input devices. They need special assistive technologies and one of the options is the head operated interface. Face-oriented algorithms often assume a particular level of lighting with adequate intensity and spatial configuration. In the paper, we propose a thermal-imaging-based algorithm of head operating typing. It does not assume the visible light illumination. We investigated, in context of thermal imagery, several contemporary general purpose object detectors known to be accurate in case of images captured by the visible light camera. Then, a selected face detector is employed in the head operated interface analysing head movements in the thermal spectrum. The attention has been focused on the problem of touchless typing which is performed in the existing solutions either through the camera mouse or through traverse procedure with an addition mechanism (like eye blink or mouth open) needed for clicking events in both cases. Our novel solution for touchless typing with head movements combines the thermal imaging for capturing user action with the hierarchical letter selection procedure. The solution employed allows to reach any alphabet character in just three steps, i.e. with directional head movements, without the need of any additional mechanisms for clicking events.
}
}

@Chapter{10.1007/978-981-10-6171-4_9,
    author = {Liu, Huaping and Sun, Fuchun},
    title = {Visual–Tactile Cross-Modal Matching Using Common Dictionary Learning},
    journal = {Robotic Tactile Perception and Understanding},
    doi = {10.1007/978-981-10-6171-4\_9},
    year = {2018},
    url = {http://link.springer.com/chapter/10.1007/978-981-10-6171-4\_9},
    abstract = {Tactile and visual measurements are two classes of sensing modalities which frequently occur in manufacturing industry and robotics. Their matching problem is highly interesting in many practical scenarios since they provide different properties about objects. This chapter investigates the visual–tactile cross-modal matching problem which is formulated as retrieving the relevant sample in an unlabeled gallery visual dataset in response to the tactile query sample. Such a problem exhibits nontrivial challenges that there does not exist sample-to-sample-pairing relation between tactile and visual modalities, which exhibit significantly different characteristics. To this end, a dictionary learning model is designed, which can simultaneously learn the projection subspace and the latent common dictionary for the visual and tactile measurements. In addition, an optimization algorithm is developed to effectively solve the common dictionary learning problem. Based on the obtained solution, the visual–tactile cross-modal matching algorithm can be easily developed. Finally, experimental validations are performed on the PHAC-2 datasets to show the effectiveness of the proposed visual–tactile cross-modal matching framework and method.}
}

@Chapter{10.1007/978-3-662-44193-0_19,
    author = {Ramos, Adrian and Prattichizzo, Domenico},
    title = {Vibrotactile Stimuli for Distinction of Virtual Constraints and Environment Feedback},
    journal = {Haptics: Neuroscience, Devices, Modeling, and Applications},
    doi = {10.1007/978-3-662-44193-0\_19},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-662-44193-0\_19},
    abstract = {In virtual reality and teleoperation scenarios, active constraints can be used to guide a user, prevent him from entering forbidden regions, and assist him in general. On a haptic interface, this implies superimposing the forces generated by the active constraints on top of the forces that are generated by the environment (real or virtual). This creates the problem of distinguishing from which of these two sources does a stimuli come from while perceiving feedback.In this paper, we present an approach that consists in adding a vibrating component on top of the forces generated by the virtual constraints, while the forces generated by the environment are kept untouched. Blind experiments in which users have to navigate through a scenario containing both active constraints and randomly positioned objects show that they manage to perceive more successfully the presence of the unexpected objects with our approach than with previously existing ones. Moreover, the penetration into the constraints is as good as with the classical approach.}
}

@Chapter{10.1007/978-3-642-37444-9_22,
    author = {Wei, Lan and Tian, Yonghong and Wang, Yaowei and Ebrahimi, Touradj and Huang, Tiejun},
    title = {Automatic Webcam-Based Human Heart Rate Measurements Using Laplacian Eigenmap},
    journal = {Computer Vision – ACCV 2012},
    doi = {10.1007/978-3-642-37444-9\_22},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-37444-9\_22},
    abstract = {Non-contact, long-term monitoring human heart rate is of great importance to home health care. Recent studies show that Photoplethysmography (PPG) can provide a means of heart rate measurement by detecting blood volume pulse (BVP) in human face. However, most of existing methods use linear analysis method to uncover the underlying BVP, which may be not quite adequate for physiological signals. They also lack rigorous mathematical and physiological models for the subsequent heart rate calculation. In this paper, we present a novel webcam-based heart rate measurement method using Laplacian Eigenmap (LE). Usually, the webcam captures the PPG signal mixed with other sources of fluctuations in light. Thus exactly separating the PPG signal from the collected data is crucial for heart rate measurement. In our method, more accurate BVP can be extracted by applying LE to efficiently discover the embedding ties of PPG with the nonlinear mixed data. We also operate effective data filtering on BVP and get heart rate based on the calculation of interbeat intervals (IBIs). Experimental results show that LE obtains higher degrees of agreement with measurements using finger blood oximetry than Independent Component Analysis (ICA), Principal Component Analysis (PCA) and other five alternative methods. Moreover, filtering and processing on IBIs are proved to increase the measuring accuracy in experiments.}
}

@Article{10.1007/s11042-015-2684-4,
    author = {Dziech, Andrzej and Baran, Remigiusz and Leszczuk, Mikołaj},
    title = {Guest Editorial: Intelligent Processing for Citizen Security},
    journal = {Multimedia Tools and Applications},
    volume = {74.0},
    issue = {12},
    doi = {10.1007/s11042-015-2684-4},
    year = {2015},
    url = {http://link.springer.com/article/10.1007/s11042-015-2684-4}
}

@Article{10.1007/s11042-012-0997-0,
    author = {Nam, Yunyoung and Rho, Seungmin and Park, Jong Hyuk},
    title = {Inference topology of distributed camera networks with multiple cameras},
    journal = {Multimedia Tools and Applications},
    volume = {67.0},
    issue = {1},
    doi = {10.1007/s11042-012-0997-0},
    year = {2013},
    url = {http://link.springer.com/article/10.1007/s11042-012-0997-0},
    abstract = {This paper proposes an inference method to construct the topology of a camera network with overlapping and non-overlapping fields of view for a commercial surveillance system equipped with multiple cameras. It provides autonomous object detection, tracking and recognition in indoor or outdoor urban environments. The camera network topology is estimated from object tracking results among and within FOVs. The merge-split method is used for object occlusion in a single camera and an EM-based approach for extracting the accurate object feature to track moving people and establishing object correspondence across multiple cameras. The appearance of moving people and the transition time between entry and exit zones is measured to track moving people across blind regions of multiple cameras with non-overlapping FOVs. Our proposed method graphically represents the camera network topology, as an undirected weighted graph using the transition probabilities and 8-directional chain code. The training phase and the test were run with eight cameras to evaluate the performance of our method. The temporal probability distribution and the undirected weighted graph are shown in the experiments.}
}

@Chapter{10.1007/978-3-319-08780-1_4,
    author = {Sucaet, Yves and Waelput, Wim},
    title = {Image Analysis},
    journal = {Digital Pathology},
    doi = {10.1007/978-3-319-08780-1\_4},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-08780-1\_4},
    abstract = {In the field of digital pathology, image analysis refers to the computer-aided diagnostic assessment of whole slide images (WSIs). While image analysis is clearly another application of WSI, we feel that the subject has become vast enough to warrant its own chapter. The potential of digital pathology has taken another giant step with the emergence of computer-assisted WSI analysis. To overcome challenges related to optimizing speed and accuracy, numerous statistical manipulations and algorithms have been generated adapted, and adopted to enhance the detection, quantification, and characterization of pathology. In this chapter, both the histories and current state of digital pathology and WSI analysis are reviewed, as well as the challenges that remain to optimize their use. It is clear that the potential of digital pathology is almost boundless, but that much work remains to be done.}
}

@Article{10.1007/s00371-011-0642-3,
    author = {Shen, Jianbing and Zhao, Ying and He, Ying},
    title = {Detail-preserving exposure fusion using subband architecture},
    journal = {The Visual Computer},
    volume = {28.0},
    issue = {5},
    doi = {10.1007/s00371-011-0642-3},
    year = {2012},
    url = {http://link.springer.com/article/10.1007/s00371-011-0642-3},
    abstract = {In this paper, we present a novel detail-preserving fusion approach from multiple exposure images using subband architecture. Given a sequence of different exposures, the Quadrature Mirror Filter (QMF) based subband architecture is first employed to decompose the original sequence into different frequency subbands. After that, we compute the importance weight maps according to the image appearance measurements, such as exposure, contrast, and saturation. In order to preserve the details of the subband signals, we compute the gain control maps and improve these subbands. Finally, the coefficients of subbands are blended into a high-quality detail-preserving fusion image. Experimental results demonstrate that the proposed approach successfully creates a visually pleasing exposure fusion image.}
}

@Article{10.1007/s11042-018-5837-4,
    author = {Banitalebi-Dehkordi, Amin and Nasiopoulos, Panos},
    title = {Saliency inspired quality assessment of stereoscopic 3D video},
    journal = {Multimedia Tools and Applications},
    doi = {10.1007/s11042-018-5837-4},
    year = {2018},
    url = {http://link.springer.com/article/10.1007/s11042-018-5837-4},
    abstract = {To study the visual attentional behavior of Human Visual System (HVS) on 3D content, eye tracking experiments are performed and Visual Attention Models (VAMs) are designed. One of the main applications of these VAMs is in quality assessment of 3D video. The usage of 2D VAMs in designing 2D quality metrics is already well explored. This paper investigates the added value of incorporating 3D VAMs into Full-Reference (FR) and No-Reference (NR) quality assessment metrics for stereoscopic 3D video. To this end, state-of-the-art 3D VAMs are integrated to quality assessment pipeline of various existing FR and NR stereoscopic video quality metrics. Performance evaluations using a large scale database of stereoscopic videos with various types of distortions demonstrated that using saliency maps generally improves the performance of the quality assessment task for stereoscopic video. However, depending on the type of distortion, utilized metric, and VAM, the amount of improvement will change.}
}

@Chapter{10.1007/978-1-84800-098-8_1,
    author = {Lahlou, Saadi},
    title = {Augmented Environments and Design},
    journal = {Designing User Friendly Augmented Work Environments},
    doi = {10.1007/978-1-84800-098-8\_1},
    year = {2010},
    url = {http://link.springer.com/chapter/10.1007/978-1-84800-098-8\_1},
    abstract = {What are Augmented Environments (AE)? What are the trends? What are the main challenges? After a quick introduction, we summarize each chapter of this book. We also sum up a few ideas that are present throughout all the chapters and constitute the pillars of design for AE.}
}

@Chapter{10.1007/978-3-319-22698-9_26,
    author = {Väänänen-Vainio-Mattila, Kaisa and Olsson, Thomas and Häkkilä, Jonna},
    title = {Towards Deeper Understanding of User Experience with Ubiquitous Computing Systems: Systematic Literature Review and Design Framework},
    journal = {Human-Computer Interaction – INTERACT 2015},
    doi = {10.1007/978-3-319-22698-9\_26},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-22698-9\_26},
    abstract = {Over the past decades, a plethora of innovative ubiquitous computing (ubicomp) systems have been constructed. The acceptance of the systems, however, depends on how users experience them in real contexts. While many of the ubicomp research projects include some form of user study, there is no overview of how user experience (UX) is approached in ubicomp research. To this end, we conducted a systematic literature review of ubicomp UX studies. Our findings reveal that users‘experiences with ubicomp systems have often been investigated in rather lightweight ways, for example by addressing basic usability issues, collecting ratings by simple, predetermined scales, or producing descriptions of general experiences such as fun and trust. Based on the findings we argue that a deeper and more fine-grained understanding of user experience would help developing more successful ubicomp systems. We propose a ubicomp UX framework that can help design and evaluate ubicomp systems with a desirable set of target experiences.}
}

@Chapter{10.1007/978-3-642-41512-8_5,
    author = {Gulzar, Nikki and Abbasi, Basra and Wu, Eddie and Ozbal, Anil and Wei and Yan, Qi},
    title = {Surveillance Privacy Protection},
    journal = {Intelligent Multimedia Surveillance},
    doi = {10.1007/978-3-642-41512-8\_5},
    year = {2013},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-41512-8\_5},
    abstract = {Surveillance Privacy Protection (SPP) is a realistic issue in the world we are living in today. Due to the massive progress in technologies and systems, surveillance is becoming quite impossible to avoid. More information is being handed out without realizing the risks involved. The objective of this chapter is to evaluate what types of surveillance, privacy and protection measures are being implemented, how information is being used and what rights individuals have over this. In addition, this chapter also emphasizes the importance of tools, data sets and databases that are being developed to enable surveillance privacy.}
}

@ReferenceWorkEntry{10.1007/978-0-85729-859-1_28,
    author = {Uchida, Seiichi},
    title = {Text Localization and Recognition in Images and Video},
    journal = {Handbook of Document Image Processing and Recognition},
    doi = {10.1007/978-0-85729-859-1\\_28},
    year = {2014},
    url = {http://link.springer.com/referenceworkentry/10.1007/978-0-85729-859-1\\_28}
}

@Chapter{10.1007/978-3-319-18720-4_1,
    author = {Wdowiak, Marek and Markiewicz, Tomasz and Osowski, Stanislaw and Swiderska, Zaneta and Patera, Janusz and Kozlowski, Wojciech},
    title = {Hourglass Shapes in Rank Grey-Level Hit-or-miss Transform for Membrane Segmentation in HER2/neu Images},
    journal = {Mathematical Morphology and Its Applications to Signal and Image Processing},
    doi = {10.1007/978-3-319-18720-4\_1},
    year = {2015},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-18720-4\_1},
    abstract = {The paper presents an automatic approach to the analysis of images of breast cancer tissue stained with HER2 antibody. It applies the advanced morphological tools to build the system for recognition of the cell nuclei and the membrane localizations. The final results of image processing is the computerized method of estimation of the membrane staining continuity. The important point in this approach is application of the hourglass shapes in rank grey-level hit-or-miss transform of the image. The experimental results performed on 15 cases have shown high accuracy of the nuclei and membrane localizations. The mean absolute error of continuity estimation of the stained membrane between the expert and our system results was 6.1\% at standard deviation of 3.2\%. These results confirm high efficiency of the proposed solution.}
}

@Chapter{10.1007/978-3-319-39972-0_7,
    author = {Galitsky, Boris and Shpitsberg, Igor},
    title = {Autistic Learning and Cognition},
    journal = {Computational Autism},
    doi = {10.1007/978-3-319-39972-0\_7},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-39972-0\_7},
    abstract = {The focus of this chapter is autistic learning and cognition. We explore the difficullties humans and machines share in learning the external world and focus on how it happens in case of autism. Firstly, the framework of active learning is introduced which is the basis of our model for autistic adaptation. We start with a hypersensitivity of an autistic learning system and explain how it leads to repetitive patterns, stereotypy and ignorance behavior. We then introduce hybrid deductive, inductive and abductive reasoning system Jasmine and reproduce the scenarios of autistic learning.}
}

@Chapter{10.1007/978-1-4471-6524-8_6,
    author = {Sun, Zhenan and Tan, Tieniu},
    title = {Iris Anti-spoofing},
    journal = {Handbook of Biometric Anti-Spoofing},
    doi = {10.1007/978-1-4471-6524-8\_6},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-1-4471-6524-8\_6},
    abstract = {Iris images contain rich texture information for reliable personal identification. However, forged iris patterns may be used to spoof iris recognition systems. This paper proposes an iris anti-spoofing approach based on the texture discrimination between genuine and fake iris images. Four texture analysis methods include gray level co-occurrence matrix, statistical distribution of iris texture primitives, local binary patterns (LBP) and weighted-LBP are used for iris liveness detection. And a fake iris image database is constructed for performance evaluation of iris liveness detection methods. Fake iris images are captured from artificial eyeballs, textured contact lens and iris patterns printed on a paper, or synthesised from textured contact lens patterns. Experimental results demonstrate the effectiveness of the proposed texture analysis methods for iris liveness detection. And the learned statistical texture features based on weighted-LBP can achieve 99accuracy in classification of genuine and fake iris images.}
}

@Chapter{10.1007/978-3-642-33783-3_13,
    author = {Everts, Ivo and van Gemert, Jan C. and Gevers, Theo},
    title = {Per-patch Descriptor Selection Using Surface and Scene Properties},
    journal = {Computer Vision – ECCV 2012},
    doi = {10.1007/978-3-642-33783-3\_13},
    year = {2012},
    url = {http://link.springer.com/chapter/10.1007/978-3-642-33783-3\_13},
    abstract = {Local image descriptors are generally designed for describing all possible image patches. Such patches may be subject to complex variations in appearance due to incidental object, scene and recording conditions. Because of this, a single-best descriptor for accurate image representation under all conditions does not exist. Therefore, we propose to automatically select from a pool of descriptors the one that is best suitable based on object surface and scene properties. These properties are measured on the fly from a single image patch through a set of attributes. Attributes are input to a classifier which selects the best descriptor. Our experiments on a large dataset of colored object patches show that the proposed selection method outperforms the best single descriptor and a-priori combinations of the descriptor pool.}
}

@Article{10.1007/s11432-015-5516-2,
    author = {Tan, Zhongwei and Yang, Chuanchuan and Li, Yuliang and Yan, Yan and He, Changhong and Wang, Xinyue and Wang, Ziyu},
    title = {A low-complexity sensor fusion algorithm based on a fiber-optic gyroscope aided camera pose estimation system},
    journal = {Science China Information Sciences},
    volume = {59.0},
    issue = {4},
    doi = {10.1007/s11432-015-5516-2},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s11432-015-5516-2},
    abstract = {Visual tracking, as a popular computer vision technique, has a wide range of applications, such as camera pose estimation. Conventional methods for it are mostly based on vision only, which are complex for image processing due to the use of only one sensor. This paper proposes a novel sensor fusion algorithm fusing the data from the camera and the fiber-optic gyroscope. In this system, the camera acquires images and detects the object directly at the beginning of each tracking stage; while the relative motion between the camera and the object measured by the fiber-optic gyroscope can track the object coordinate so that it can improve the effectiveness of visual tracking. Therefore, the sensor fusion algorithm presented based on the tracking system can overcome the drawbacks of the two sensors and take advantage of the sensor fusion to track the object accurately. In addition, the computational complexity of our proposed algorithm is obviously lower compared with the existing approaches (86\% reducing for a 0.5 min visual tracking). Experiment results show that this visual tracking system reduces the tracking error by 6.15\% comparing with the conventional vision-only tracking scheme (edge detection), and our proposed sensor fusion algorithm can achieve a long-term tracking with the help of bias drift suppression calibration.}
}

@Chapter{10.1007/978-3-319-60492-3_24,
    author = {Gomes, Cristina Caramelo and Preto, Sandra},
    title = {Should an Artificial Window Substitute a Natural One?},
    journal = {Advances in Usability and User Experience},
    doi = {10.1007/978-3-319-60492-3\_24},
    year = {2018},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-60492-3\_24},
    abstract = {
Nowadays, most of us spend approximately 90\% of our lifetime in an indoor environment. Today a workplace can be set anywhere, at a coffee shop, at home or even in a place without windows. However, a real question is superimposed: it is really so? Can we work anywhere in any environment conditions? Can a virtual image, or video, substitute the real thing? In other words, can a virtual environment be a substitute of a real and natural one, without jeopardizing our physiological and psychological systems? This paper aims to study the “do’s and don’ts” of a lighting design in a workplace. To achieve such goals the research was built upon literature review.
}
}

@Article{10.1007/s12193-015-0204-5,
    author = {Liu, Mengyi and Wang, Ruiping and Li, Shaoxin and Huang, Zhiwu and Shan, Shiguang and Chen, Xilin},
    title = {Video modeling and learning on Riemannian manifold for emotion recognition in the wild},
    journal = {Journal on Multimodal User Interfaces},
    volume = {10.0},
    issue = {2},
    doi = {10.1007/s12193-015-0204-5},
    year = {2016},
    url = {http://link.springer.com/article/10.1007/s12193-015-0204-5},
    abstract = {In this paper, we present the method for our submission to the emotion recognition in the wild challenge (EmotiW). The challenge is to automatically classify the emotions acted by human subjects in video clips under real-world environment. In our method, each video clip can be represented by three types of image set models (i.e. linear subspace, covariance matrix, and Gaussian distribution) respectively, which can all be viewed as points residing on some Riemannian manifolds. Then different Riemannian kernels are employed on these set models correspondingly for similarity/distance measurement. For classification, three types of classifiers, i.e. kernel SVM, logistic regression, and partial least squares, are investigated for comparisons. Finally, an optimal fusion of classifiers learned from different kernels and different modalities (video and audio) is conducted at the decision level for further boosting the performance. We perform extensive evaluations on the EmotiW 2014 challenge data (including validation set and blind test set), and evaluate the effects of different components in our pipeline. It is observed that our method has achieved the best performance reported so far. To further evaluate the generalization ability, we also perform experiments on the EmotiW 2013 data and two well-known lab-controlled databases: CK+ and MMI. The results show that the proposed framework significantly outperforms the state-of-the-art methods.}
}

@Chapter{10.1007/978-3-319-14364-4_56,
    author = {Arigela, Saibabu and Asari, Vijayan K},
    title = {Enhancement of Hazy Color Images Using a Self-Tunable Transformation Function},
    journal = {Advances in Visual Computing},
    doi = {10.1007/978-3-319-14364-4\_56},
    year = {2014},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-14364-4\_56},
    abstract = {Vision based outdoor mobile systems are very sensitive to infelicitous weather circumstances like hazy and foggy conditions. The acquisition of image frames in such an environment deteriorates the scene contrast and biases the color information. In order to recover the scene details, we propose a new method which takes a nonlinear approach, where the haze pixel intensity is manipulated effectively with a specially designed sine nonlinear function. This function is integrated with the optics based haze model to approximate the enhanced inverse transmission of the scene. The transformation function is composed with a variable parameter, which tunes automatically, to produce desired nonlinear mapping for each pixel while maintaining the local contrast. Unlike other state-of art haze removal techniques, which operates on local regions, proposed method operates on each pixel to eliminate the blocking artifacts and minimizes the processing complexity. Our experimental results with quantitative measures demonstrate that the proposed technique yields state-of-the-art performance on hazy images and is suitable to process a dynamic video scenes captured in adverse weather conditions.}
}

@Chapter{10.1007/978-3-319-48746-5_13,
    author = {Teruel, Miguel A. and Navarro, Elena and González, Pascual},
    title = {Towards an Awareness Interpretation for Physical and Cognitive Rehabilitation Systems},
    journal = {Ubiquitous Computing and Ambient Intelligence},
    doi = {10.1007/978-3-319-48746-5\_13},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-3-319-48746-5\_13},
    abstract = {When collaborating remotely, being aware of other participants (their actions, locations, status, etc.) is paramount to achieve a proper collaboration. This issue is magnified when talking about rehabilitation systems, whose users may require additional specific awareness information, due to their cognitive or physical disabilities. Moreover, because of these disabilities, this awareness may be provided by using specific feedback stimuli. This constituted the main motivation of this work: the development of an awareness interpretation for collaborative cognitive and physical therapies. With this aim, an awareness interpretation already applied to the collaborative games field has been modified and extended to make it suitable for these systems. Furthermore, in order to put this interpretation into practice, a case study based on an association image-writing rehabilitation pattern is presented illustrating how this cognitive rehabilitation task has been extended with collaborative features and enriched with awareness information.}
}

@Chapter{10.1007/978-0-85729-932-1_2,
    author = {Shakhnarovich, Gregory and Moghaddam, Baback},
    title = {Face Recognition in Subspaces},
    journal = {Handbook of Face Recognition},
    doi = {10.1007/978-0-85729-932-1\_2},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-0-85729-932-1\_2},
    abstract = {Images of faces, represented as high-dimensional pixel arrays, often belong to a manifold of intrinsically low dimension. Face recognition, and computer vision research in general, has witnessed a growing interest in techniques that capitalize on this observation and apply algebraic and statistical tools for extraction and analysis of the underlying manifold. In this chapter, we describe in roughly chronologic order techniques that identify, parameterize, and analyze linear and nonlinear subspaces, from the original Eigenfaces technique to the recently introduced Bayesian method for probabilistic similarity analysis. We also discuss comparative experimental evaluation of some of these techniques as well as practical issues related to the application of subspace methods for varying pose, illumination, and expression.}
}

@Chapter{10.1007/978-1-84882-935-0_3,
    author = {Szeliski, Dr. Richard},
    title = {Image processing},
    journal = {Computer Vision},
    doi = {10.1007/978-1-84882-935-0\_3},
    year = {2011},
    url = {http://link.springer.com/chapter/10.1007/978-1-84882-935-0\_3},
    abstract = {Now that we have seen how images are formed through the interaction of 3D scene elements, lighting, and camera optics and sensors, let us look at the first stage in most computer vision applications, namely the use of image processing to preprocess the image and convert it into a form suitable for further analysis. Examples of such operations include exposure correction and color balancing, the reduction of image noise, increasing sharpness, or straightening the image by rotating it (Figure 3.1). While some may consider image processing to be outside the purview of computer vision, most computer vision applications, such as computational photography and even recognition, require care in designing the image processing stages in order to achieve acceptable results.}
}

@Chapter{10.1007/978-1-4471-6684-9_1,
    author = {Burger, Wilhelm and Burge, Mark J.},
    title = {Digital Images},
    journal = {Digital Image Processing},
    doi = {10.1007/978-1-4471-6684-9\_1},
    year = {2016},
    url = {http://link.springer.com/chapter/10.1007/978-1-4471-6684-9\_1},
    abstract = {For a long time, using a computer to manipulate a digital image (i.e., digital image processing) was something performed by only a relatively small group of specialists who had access to expensive equipment. Usually this combination of specialists and equipment was only to be found in research labs, and so the field of digital image processing has its roots in the academic realm. Now, however, the combination of a powerful computer on every desktop and the fact that nearly everyone has some type of device for digital image acquisition, be it their cell phone camera, digital camera, or scanner, has resulted in a plethora of digital images and, with that, for many digital image processing has become as common as word processing. It was not that many years ago that digitizing a photo and saving it to a file on a computer was a time-consuming task. This is perhaps difficult to imagine given today’s powerful hardware and operating system level support for all types of digital media, but it is always sobering to remember that “personal” computers in the early 1990s were not powerful enough to even load into main memory a single image from a typical digital camera of today. Now powerful hardware and software packages have made it possible for amateurs to manipulate digital images and videos just as easily as professionals.}
}

@Article{10.1007/s11042-015-3125-0,
    author = {Ioannidou, Anastasia and Apostolidis, Evlampios and Collyda, Chrysa and Mezaris, Vasileios},
    title = {A web-based tool for fast instance-level labeling of videos and the creation of spatiotemporal media fragments},
    journal = {Multimedia Tools and Applications},
    volume = {76.0},
    issue = {2},
    doi = {10.1007/s11042-015-3125-0},
    year = {2017},
    url = {http://link.springer.com/article/10.1007/s11042-015-3125-0},
    abstract = {This paper presents a web-based interactive tool for time-efficient instance-level spatiotemporal labeling of videos, based on the re-detection of manually selected objects of interest that appear in them. The developed tool allows the user to select a number of instances of the object that will be used for annotating the video via detecting and spatially demarcating it in the video frames, and provide a short description about the selected object. These instances are given as input to the object re-detection module of the tool, which detects and spatially demarcates re-occurrences of the object in the video frames. The video segments that contain detected instances of the given object can be then considered as object-related media fragments, being annotated with the user-provided information about the object. A key component for building such a tool is the development of an algorithm that performs the re-detection of the object throughout the video frames. For this, the first part of this work presents our study on different approaches for object re-detection and the finally developed one, which combines the recently proposed BRISK descriptors with a descriptor matching strategy that relies on the LSH algorithm. Following, the second part of this work is dedicated to the description of the implemented tool, introducing the supported functionalities and demonstrating its use for object-specific labeling of videos. A set of experiments and a user study regarding the efficiency of the introduced object re-detection method and the performance of the developed tool indicate that the proposed framework can be used for accurate and time-efficient instance-based annotation of videos, and the creation of object-related spatiotemporal media fragments.}
}
